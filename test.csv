Submission Title,Submission Url,Plain Description,Video,Website,File Url,Desired Prizes,Select A Route (Only 1) To Be Considered For Route Prize,Submitter Screen Name,Team Member 1 Screen Name,...
Food For Though,http://pennapps-xiv.devpost.com/submissions/56529-food-for-though,"Inspiration: Save Food, Give Food

According to the Natural Resources Defense Council, “Forty percent of food in the United States is never eaten, amounting to $165 billion a year in waste, taking a toll on the country's water resources and significantly increasing greenhouse gas emissions”. Food, labor, space and other resources goes wasted at an alarming rate in America and there has to be a mindset change. Restaurants and supermarkets usually have to get rid of their food by the end of the day which adds to the waste every day. In a perfect world, the food that is being wasted would go directly to the people who need it the most. Straight from the restaurants and supermarkets that have perfectly good food that has to be thrown away. 
    However, we understand that restaurants and supermarkets are not nonprofits and many even fear donating the food to local food pantries and shelters because of legal action against them. That being said, the fear is unfounded. According to the Food Donation Connection, “The Federal Bill Emerson Good Samaritan Food Donation Act protects the donor and the recipient agency against liability, excepting only gross negligence and/or intentional misconduct. In addition, each state has passed Good Samaritan Laws that provide liability protection to good faith donors. Each of the Harvest Programs we coordinate have established procedures to ensure that safe food handling and storage is built into their donation program”. However, that still leaves the responsibility of transmission of food to pantries and shelters on the restaurants and supermarkets. We aim to bridge that gap by relocating the resources of food budgets from food pantries, and shelters and instead paying Lyft drivers to deliver the food from a restaurant or supermarket to the food pantries and shelters. If we work together, we could make positive strides towards lowering our food waste in America and helping those who need it the most.

What it does

We connect restaurants and food pantries by use of web app and make it happen via Lyft.

How we built it

Flask and lack of sleep

Accomplishments that we're proud of

Everything

What's next for test

More sophisticated connections between our web app and Lyft and Google APIs.
",,https://github.com/ohlookiseecode/RealFood,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best User Experience, Best Progressive Web App, Best Use of Lyft API","",itsmahon,chenchelsea,kapolo,Mingquan
Politiview,http://pennapps-xiv.devpost.com/submissions/56530-politiview,"Inspiration

Too often, the voters opinion is lost and unheard to their politicians.  Our goal was to give people a voice in political issues and find out what people around them think anonymously while also helping politicians figure out what voters think.

What it does

People enter basic demographics and can post polls and vote within their town, county, legislative district or congressional district.  The user is then registered into a mongodb database.  There, they can view filtered polls based on the level of the post (local, county, national) of their demographic region.  Creating a poll is as simple as inputting a question and 2 answers.  People then can click on different polls that allow them to choose the topics they want

How we built it

Client application is an android app using okhttp to network.  Backend is mongodb database hosted on a linode cloud server and accessed via nodejs.

Challenges we ran into

Networking without using AsyncTasks due to not being able to make synchronous network calls on the main thread and overall the networking architecture for the app

Accomplishments that we're proud of

Setting up a linode server, interacting with mongodb, learning how to use okhttp asynchronously and creating a service to solve a problem

What we learned

How to work with linode, mongodb, and learning how to create a client-server application architecture

What's next for Politiview

Adding more production features (maybe)
",,,,"Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services, Best Use of MongoDB",social + civic hacking,atf1999,SargyBoy
LiveKan,http://pennapps-xiv.devpost.com/submissions/56532-livekan,"Development teams everywhere have tonnes of written data, on meeting boards and tonnes left unsaid. Traditionally, development teams would use very unwieldy software solutions that have large overhead to run and purchase. For teams large and small, physical kanban boards provide the physical satisfaction of accomplishing your tasks but lack in the department for tracking. But what if we can merge the benefits of both systems?

One camera, one board, one website

LiveKan syncs your physical board with your online board. Providing you with the insights of apps, with all the fun of sticky notes. When you move a user story on the board, the web app updates. Reality, more than augmented. It's technology ingrained...

On the website, we expose insights into possible employee metrics that will be useful for small teams. Now you don't have to be a large software foundry to have top league Project Management solutions. Even if you are solo, a camera setup in your home or meeting room will allow you to sync your user stories.

The Technical Solution

A seemingly simple solution, a complex implementation. It all starts with a 8 MP webcam, linked up to advanced OpenCV software running 4 layers of various image enhancements. Processing at 24 frames a second, it goes into a ML data server hosted locally to parse shapes and user story task. 

Piped into a smart backend server storing all our events in Mongo. We take all that data, and pair it with our clean React frontend. Analytics suites standing by, we can store team data and plug it into their associated tasks.

All the while, multiple microservices ensure a smooth - enterprise grade server deployment. Thanks to Docker and Webpack.

Designing Around Our Problems

Once the team arrived we found out that all the webcam's were taken. Starting a city adventure that would take us to multiple stores in order to search for our hardware.

We also ran into an issue for handwriting recognition. A 12 year problem, may exist for more while hackers like us chip away at it.

This hack also required a melding of opposite programming disciplines, most of the team not having OpenCV experience, we had two people work on low level programming to get crucial data, while exposing that in a human centric experience.

Accomplishments that we're proud of

The lessons we documented we can pass on our lessons into a open source contribution.

What's next for LiveKan

Taking it out of the cathedral and into the bazaar. We can't wait to have it on the market in a variety of forms.
",,https://github.com/livekan/,,"Best Use of Data Visualization, Best Use of VR/AR for Content Discovery, Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services","",ndneighbor,Akivuls,devanshk,edesanto
The Quiet Game,http://pennapps-xiv.devpost.com/submissions/56534-the-quiet-game,"Inspiration

The Quiet Game

What it does

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for The Quiet Game
",,,,Best User Experience,"",davidwang
FaceTheMusic,http://pennapps-xiv.devpost.com/submissions/56539-facethemusic,"ZachAndChloesExcellentHackventure
",,https://github.com/pisci95/ZachAndChloesExcellentHackventure,,"Best Design Using PYNQ-Z1 Boards, Peripherals, and Tools (3)","",Chloesnyder
trade-O-bundle,http://pennapps-xiv.devpost.com/submissions/56546-trade-o-bundle,"Inspiration

US export is a Trillion $ industry, with roughly $800+ Billion market share of Organic Food Exports. 
Surprisingly, the functions of such a large scale industry are still massively manual (pen and paper ordering) between Suppliers, Exporters, Distributors & Retail Stores (mom-n-pop shops).

It is time to not only automate the process but also tackle some other pressing problems like


counter the markups by middle-men/distributors 
reduce turn-around time in fulfilling orders
insights into buying behaviors of customers


What it does

A set of responsive web pages for suppliers, distributors and small mom-n-pop shops have been set up to automate the entire flow of information as export/import orders are processed and fulfilled. 


An intuitive ""pool purchasing"" option allows smaller retail stores to directly place order with international suppliers, totally bypassing the markups of the local distributors.
Finally, analytics on order data provide insights into the purchasing behavior of the end-customers. This plays a critical role in reducing the current time to fulfill orders as suppliers can anticipate demand and pre-stock international ports. 


Challenges we ran into


Understanding the depth of the problem statement and coming up with a shrink-wrapped solution for the same.
Serving the app over HTTPS
Using camera to read barcode in a web browser


Accomplishments that we're proud of


Design Thinking session with a probable customer (they have made an offer to purchase our system post the hackathon).
Setting up a full-stack solution in 36 hours :)


What we learned

The importance of co-innovation with the end-customer.

What's next for trade-O-bundle

Setup the entire platform to scale well and handle the expected data-loads of international trade. 
",,,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best User Experience","",Vidurb,prashant4arya,Alex6614
vox,http://pennapps-xiv.devpost.com/submissions/56563-vox,"Inspiration

We wanted to talk and sound like Morgan Freeman and other public figures. 

What it does

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for vox
",,https://github.com/hsheth2/vox,,"","",hsheth2,milesfertel,nboucher,madhav-datt
Medical.me,http://pennapps-xiv.devpost.com/submissions/56568-medical-me,"Inspiration

Many people can recall visiting the doctor and receiving a fairly accurate diagnosis and effective medicine. However, health care can be more often than not, inefficient. As Time's Money once reported, it costs about $43 to sit around the doctor's waiting room. The effect of such lack of productive visits are even more noticeable from the perspective of time; while the American Journal of Managed Care estimates, on average, 121 minutes spent when a person seeks medical care, a great amount when compared to the paltry 20 minutes one actually spends.

How can people save time and money, while not neglecting their health? And how can people receive diagnoses for symptoms that may be serious?

Medical.me makes accessing you medical information, and getting a diagnoses, as easy as talking out loud.

What it does

Medical.me integrates your medical history with a powerful AI that helps answer questions about your prescriptions, injuries, pains, and your medical record. The convenient software can run on any Amazon Echo, and creates a friendly environment which responds to questions, concerns, and symptoms. 

How we built it

Our software uses two powerful APIs to process data and to help connect us with patients in ways that were never possible before. Cerner provides access to vast medical information, including thousands of prescription medicines, doctor history, and patient information. Amazon Echo helps us bring our app to the common household, making it both convenient and accessible. 

Challenges we ran into

-Understanding and utilizing Alexa Skills Kit; 
-Switching between languages and implementations on both APIs;
-Integrating and choosing what data is the most important 

Accomplishments

-Interacting with Alexa Skills directly with PHP
-Dedicated Server with live AI
-Integrating our databases with Cerner API

What we learned

-How to launch an Alexa Skill into the cloud
-How to build an AI in PHP (and, by result of iteration, Java and Node.js) to use Alexa Skills Kit

What's next for Medical.me

A more personalized and data-driven experience, involving making predictions based on a user's medical history and expanding the prescription drugs in our database.  We would also analyze bigger trends of all patients in order to provide long-term trends and smarter diagnoses.
",,http://medical.me,,"Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best User Experience, Best Progressive Web App",health,ryerrabelli,ldominguez,narayar
Batman Scan,http://pennapps-xiv.devpost.com/submissions/56572-batman-scan,"Inspiration

The power of AI has been integrated with several fields now - health, education, finance, social networks, etc. However, the field of surveillance has remained complacent for a long time. Surveillance cameras are all around us - schools, colleges, hospitals, shops, offices - just to name a few. Yet, security authorities still manually monitor multiple screens to detect if something is fishy. Why does it have to be so tedious?

It is now time to disrupt this. The video surveillance market is currently a $16 billion industry and is expected to exceed $30 billion by 2019. If this hack sees the light of day, it can potentially penetrate the global video surveillance market. Imagine a world where accidents, natural disasters, gun violence, robbery, terrorism, etc. are detected automatically. With this hack, we are one step closer to that utopian world.

What it does

Batman-Scan is a real time surveillance monitoring system which is constantly on the hunt for suspicious activities. If something abnormal is detected, the concerned authorities are notified immediately. Local police can also monitor the city through a map interface in real time (just like batman sees the sky!). When an unusual behaviour is detected, a distress bat-signal appears on the map. 

How I built it

This hack is neatly divided into three sections : 


The ""brain"" which detects the abnormality in the live video. 
Nexmo's text-to-speech API which is used to deliver the information accurately over a call. The bot would furnish all the details by voice. Eg : ""Suspicious activity detected at University of Pennsylvania, Philadelphia on 10th Sept 2016 at 10:56PM"". If the user fails to pick up the call, a text message is sent which contains all the details. 
A Google maps interface which renders the distress signal in real time. The location displayed is the location of the camera. 


Unlike exploiting traditional machine learning algorithms like neural nets which process the data in batches, I employ a simple statistical outlier classifier which makes real time decisions. The following figure helps you visualize this. The beauty of this approach is that you DO NOT need training data at all. If a pattern occurs frequently enough, it is regarded as a ""normal"" behaviour. 



Challenges I ran into

Let's acknowledge the fact that working with videos is painful. When I started out, each frame had about 6 million features! (and considering that videos are recorded in 30 fps, each second of the video would have been represented by about 180 million features!) However after numerous attempts, I could bring down the feature size to around 30,000. 

In order to analyze historical data, it is usually stored in some form. But storing all features of a video is expensive and time consuming. To overcome this problem in this hack, the surveillance data is represented in the form of their densities rather than the data itself. 

Dataset used for video surveillance

Accomplishments that I'm proud of


Ensuring that the model makes accurate predictions from the input video stream in real time. (The faster the abnormality is detected, the sooner can the police take an action!)


What's next for Batman Scan


It might be useful if we could localize the abnormality within the frame. (rather than marking the entire frame as abnormal)
The power of GPU can be leveraged to process frames faster.
Minimizing the false positives. 
Rather than a single surveillance camera making independent decisions, it might be more efficient if the cameras learn together as a swarm. (Just like how there exists a ""single brain"" for all of Tesla's autonomous cars)

",https://www.youtube.com/watch?v=ecDj-EbzqkY&feature=youtu.be,https://github.com/animeshramesh/batman-scan,,"Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API",social + civic hacking,animeshramesh
SAFEphilLY,http://pennapps-xiv.devpost.com/submissions/56581-safephilly,"In a nutshell...

This web app allows a user to enter her/his trip's origin and destination and instantly receive a recommendation for the safest route to take. This decision for the 'safest' route is based on historical crime statistics of all the neighborhoods in Philadelphia.

Inspiration

Many people walk around the city - they walk back home from parties and dinners at night, to parks, coffee shops, and grocery stores during different times of the day, and to school and work. Like any big city, Philly has its safe and unsafe spots, and people often do not know if a specific neighborhood is safe to pass by during a certain time of the day. We wanted to address this challenge and designed a lightweight web application that makes the 'best-route' decision for you so that you can reach home SAFEphilLY.

How we built it

We used the Crime Incidents Open Dataset for the city of Philadelphia and ran a support vector machine classifier (with an RBF kernel) to perform supervised learning on the crime statistics. Based on our learning algorithm (which we implement in Python using the scikit-learn toolkit), we came up with a confidence parameter for the safety of a specific neighborhood at a specific hour of the day.

Next, we implement the same learning algorithm for any route and give it a risk level based on previous crime statistics for the regions it goes through. To provide the user the 'safest path', we do this for multiple routes from the origin to the destination given by the user. We pull this set of routes from the Google Maps API and use a few statistic measures to come up with the 'safest' route from this set. Finally, we plot this route on the map for the user to see.

The front-end is built with Polymer, a lightweight framework that can work even on low-internet speeds. All the ML and statistics work is done on the remote server and we ensure our front-end is as lightweight as possible.

Challenges we ran into

While Polymer was easy to work with and get started, we had some trouble working with Google Polymer due to its limited functionality as some of the Google Maps functions we needed to use were not available or difficult to use in the elements in Polymer. We also struggled with building a stable and user-friendly web app due to time constraints.

Accomplishments that we're proud of

We were able to have our frontend and backend components working and giving the expected results. We are proud of our current implementation of the app which suggests the safest route based on crime statistics and time of the day. We also induced a bias in the ML algorithm towards more 'dangerous' crimes - i.e. we trained our algorithm to favor homicide and personal assaults over thefts and minor crimes. This improved the accuracy and authenticity of our results. We studied the dataset very closely to understand trends and the nuances that we needed to account for in our algorithm.

What we learned

We learned to use machine learning algorithms on publicly available data to draw new associations that could be valuable to the public. We also learned to use Google Polymer to make elegant and lightweight apps quickly.

What's next for SAFEphilLY

We would like to improve the user experience with SAFEphilLY and also port it to Android and iOS platforms. We would also like to explore the potential of integrating with cab services such as Uber and Lyft based on the time of the day and the past history of crime in the area.
",,https://github.com/adityavishwanath/SAFEphilLY,,"Best Use of Data Visualization, Best Public Safety or Video Processing App, Best User Experience, Best Progressive Web App",social + civic hacking,adityavishwanath,aismail1997
HearTempo,http://pennapps-xiv.devpost.com/submissions/56582-heartempo,"(""Heart Tempo"", not ""Hear Tempo"", fyi)

Inspiration

David had an internship at the National Institute of Health over the summer, where he researched the effect of auditory stimulus such as music on microcirculation (particularly the myogenic and endothelial bands), using Laser Doppler Flowmetry (LDF) to do so. This experiment all stems from the known fact that the human body often matches its heartrate with the tempo of a song that is playing.

Though that side of things was heavily researched, the opposite wasn't. And for that reason, David developed the idea of making the tempo of the song change relative to the heartrate of the person listening to the song, rather than vice-versa.

What it does

This Android app will connect to your Android Wear device (with a heartbeat sensor) and send this heart rate to a server which modifies the tempo of any song to regulate your heart rate at normal levels.

By regulating your heart rate, it will reduce anxiety and stress, allowing you to relax and not worry about the pressures of life. The best part? You only need a smartwatch and smartphone, no fancy equipment.

(This is where the name comes from, if you haven't figured that out already)

How we built it

We had to figure out how to get the heart rate from an Android Wear watch, which didn't take that long. The hard part was actually figuring out how to send the data from the watch to the phone then to the server. We ended up using the native WearableListenerService to send the data to the phone which then sent the data with OkHttp to our Node.js server. This server will connect with any MIDI-enabled application on your machine to change the BPM.

Challenges we ran into

It took a damn while to figure out how to send data between the watch and the phone, why does Google make this so hard?!

Accomplishments that we're proud of

When we first got the phone to actually send data to the server, we were very happy and wanted to do more with the project.

What we learned

We learned more about Android development, an area we both wanted to get into. It was slightly difficult since we both come from web development backgrounds and the concepts are very different.

What's next for HearTempo

Some sort of logging and data analysis, definitely. We want to prove that this works, so we will perhaps implement a log of your average heart beat over a course of a week or two after you start using it.
",,,,"",health,tjhorner,WhileTrueBreak
WristPass,http://pennapps-xiv.devpost.com/submissions/56584-wristpass,"Inspiration

WristPass was inspired by the fact that NFC is usually only authenticated using fingerprints. If your fingerprint is compromised, there is nothing you can do to change your fingerprint. We wanted to build a similarly intuitive technology that would allow users to change their unique ids at the push of a button. We envisioned it to be simple and not require many extra accessories which is exactly what we created.

What it does

WristPass is a wearable Electro-Biometric transmission device and companion app  for secure and reconfigurable personal identification with our universal receivers. Make purchases with a single touch. Check into events without worrying about forgetting tickets. Unlock doors by simply touching the handle.

How we built it

WristPass was built using several different means of creation due to there being multiple parts to the projects. The WristPass itself was fabricated using various electronic components. The companion app uses Swift to transmit and display data to and from your device. The app also plugs into our back end to grab user data and information. Finally our receiving plates are able to handle the data in any way they want after the correct signal has been decoded. From here we demoed the unlocking of a door, a check in at a concert, and paying for a meal at your local subway shop.

Challenges we ran into

By far the largest challenge we ran into was properly receiving and transcoding the user’s encoded information. We could reliably transmit data from our device using an alternating current, but it became a much larger ordeal when we had to reliably detect these incoming signals and process the information stored within. In the end we were able to both send and receive information. 

Accomplishments that we're proud of


Actually being able to transmit data using an alternating current 
Building a successful coupling capacitor 
The vast application of the product and how it can be expanded to so many different endpoints 


What we learned


We learned how to do capacitive coupling and decode signals transmitted from it. 
We learned how to create a RESTful API using MongoDB, Spring and a Linode Instance. 
We became more familiarized with new APIs including: Nexmo, Lyft, Capital One’s Nessie. 
And a LOT of physics! 


What's next for WristPass


We plan on improving security of the device. 
We plan to integrate Bluetooth in our serial communications to pair it with our companion iOS app. 
Develop for android and create a web UI. 
Partner with various companies to create an electro-biometric device ecosystem. 

",https://youtu.be/Lz2NywK8HkY,,,"Best Use of Rapid Prototyping, sposnored by AddLab, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Use of the Capital One API - Nessie, Best Use of Nexmo API, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Use of Lyft API",hardware,mniebylski,shahvineet98,alexkgodwin,driver733
Newstock,http://pennapps-xiv.devpost.com/submissions/56595-newstock,"Though technology has certainly had an impact in ""leveling the playing field"" between novices and experts in stock trading, there still exist a number of market inefficiencies for the savvy trader to exploit. Figuring that stock prices in the short term tend to some extent to reflect traders' emotional reactions to news articles published that day, we set out to create a machine learning application that could predict the general emotional response to the day's news and issue an informed buy, sell, or hold recommendation for each stock based on that information. 

After entering the ticker symbol of a stock, our application allows the user to easily compare the actual stock price over a period of time against our algorithm's emotional reaction.

We built our web application using the Flask python framework and front-end using React and Bootstrap. To scrape news articles in order to analyze trends, we utilized the google-news API. This allowed us to search for articles pertaining to certain companies, such as Google and Disney. Afterwards, we performed ML and sentiment analysis through the textblob Python API. 

We had some difficulty finding news articles; it was quite a challenge to find a free and accessible API that allowed us to gather our data. In fact, we stumbled upon one API that, without our knowledge, redirected us to a different web page the moment we attempted any sort of data extraction. Additionally, we had some problems trying to optimize our ML algorithm in order to produce as accurate results as possible.

We are proud of the fact that Newsstock is up and running and able to predict certain trends in the stock market with some accuracy. It was cool not only to see how certain companies fared in the stock market, but also to see how positivity or negativity in media influenced how people bought or sold certain stocks. 

First and foremost, we learned how difficult it could be at times to scrape news articles, especially while avoiding any sort of payment or fee. Additionally, we learned that machine learning can be fairly inaccurate. Overall, we had a great experience learning new frameworks and technologies as we built Newsstock.
",,,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services, Best User Experience","",japopelka,subs1,jimtse1,sumitzster
whoot,http://pennapps-xiv.devpost.com/submissions/56600-whoot,"Inspiration

We wanted to make an anonymous chat room, but to make it more fun. Thus, whoot was born. 

What it does

After opening a chatroom, a user is given a random name. Sharing the link to the room allows you to invite your friends to chat. There is a built-in drawing game. (details)

How we built it

At first we were going to use Google Polymer for our front-end, but in the middle of the hackathon we decided to go with Angular.js since we were facing too many difficulties with the framework. The backend uses Express.js and Socket.io to handle chat and gaming.

Challenges we ran into

We wanted to try Google Polymer, but we found it difficult to use because none of us were familiar with its paradigms. After moving to Angular, we found programming the game to be annoying difficult. Not mention the difficulty of using sockets to transmit data

Accomplishments that we're proud of

By 8AM we seemed to have something that worked, which in the end, is all that matters.

What we learned

It's important to choose your frameworks wisely. What we made in polymer in 12 hours was redone in angular.js in only 1. 

What's next for whoot

We want to add more games, and improve overall functionality
",,https://github.com/Andrewjeska/whoot,,"",social + civic hacking,andrewjeska,Nickpesce,robotal,allen12
MOM: Mind Over Matter,http://pennapps-xiv.devpost.com/submissions/56601-mom-mind-over-matter,"Inspiration

Coming up with ideas is hard. That's why we created an Alexa Skill and web application called Mind Over Matter - or what we like to call MOM. Now, you have someone to bounce ideas off of and together, you can come up with an idea that is truly unique.

How it works

Start the MOM skill by saying ""Alexa, Start Mind Over Matter."" Then, state your assignment (report, project, etc) or a subject you are interested in. Alexa will give you readily available topics to research into and help you discover what you really want to know. When you are done, be sure to say ""Review My Brainstorm"" so you can see how far you've come. Once you are done, go to the dashboard website to get more data about your brainstorm session.

Challenges we ran into


Limitations of Alexa cards.
Getting data from the Echo to the personal dashboard.

",https://youtu.be/IQSQeWBj6xc,https://github.com/blondiebytes/mindovermatterPennApps,,"Best Amazon Alexa Hack, Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best User Experience, Best Progressive Web App",education,blondiebytes,hackerbacker
Sophia,http://pennapps-xiv.devpost.com/submissions/56602-sophia,"Inspiration

To be filled later

What it does

How I built it

Challenges I ran into

Accomplishments that I'm proud of

What I learned

What's next for Sophia
",,,,"Best User Experience, Best Progressive Web App",social + civic hacking,edomarc
Buskr,http://pennapps-xiv.devpost.com/submissions/56603-buskr,"Inspiration

We have 2 buskers on the team (a busker is a street musician).  While there are extremely successful street musicians, most are encounter difficulties due to 1) lack of skill or 2) tipping-averse audiences.  While we can't do much for the former, spending propensity in any area is easily modeled by using transaction data.  In order to help our fellow buskers, we built an app that points out the best areas to visit.

What it does

Buskr uses Capital One's Enterprise API to get information on transactions, including amount, date, and geographic location.  Using a simple predictive ML model, Buskr creates a ""spend propensity"" index per person.  Buskr then generates a user-friendly heat map of predicted tip revenue using these spend propensities and directs the user to the nearest ""hot spot.""

How we built it

We queried the data from Capital One's Enterprise API and parsed the jsons using Javascript.  The data frames were then passed onto MongoDB, where they were linearly transformed into the final values used. We created the site using Node under a Bootstrap framework. We used Leaflet to create the maps and used a heat map extension for Leaflet to display the heat map.

Challenges we ran into

Capital One's randomized data had a few identification issues which presented issues when we tried to join tables. 

What we learned

We learned about probability-based marketing models and how they apply to financial decisions.  We also learned some interesting use cases for Javascript and how to use Leaflet with its extensions.

What's next for Buskr

Given more time, we would've introduced an online tipping feature for business to use to incentivize buskers to perform near their area.
",,http://bigbusks.herokuapp.com/,,"Best Use of Data Visualization, Best Use of the Capital One API - Nessie, Best Domain Name Registered in PennApps XIV, Best Use of MongoDB, Best User Experience, Best Progressive Web App",social + civic hacking,jeffreycheng,benjyang,ndduong97,adisrivatsan
IMpulse,http://pennapps-xiv.devpost.com/submissions/56605-impulse,"Inspiration

Survival from out-of-hospital cardiac arrest remains unacceptably low worldwide, and it is the leading cause of death in developed countries. Sudden cardiac arrest takes more lives than HIV and lung and breast cancer combined in the U.S., where survival from cardiac arrest averages about 6% overall, taking the lives of nearly 350,000 annually. To put it in perspective, that is equivalent to three jumbo jet crashes every single day of the year.

For every minute that passes between collapse and defibrillation survival rates decrease 7-10%. 95% of cardiac arrests die before getting to the hospital, and brain death starts 4 to 6 minutes after the arrest. 

Yet survival rates can exceed 50% for victims when immediate and effective cardiopulmonary resuscitation (CPR) is combined with prompt use of a defibrillator. The earlier defibrillation is delivered, the greater chance of survival. Starting CPR immediate doubles your chance of survival. The difference between the current survival rates and what is possible has given rise to the need for this app - IMpulse.

Cardiac arrest can occur anytime and anywhere, so we need a way to monitor heart rate in realtime without imposing undue burden on the average person. Thus, by integrating with Apple Watch, IMpulse makes heart monitoring instantly available to anyone, without requiring a separate device or purchase.

What it does

IMpulse is an app that runs continuously on your Apple Watch. It monitors your heart rate, detecting for warning signs of cardiac distress, such as extremely low or extremely high heart rate. If your pulse crosses a certain threshold, IMpulse captures your current geographical location and makes a call to an emergency number (such as 911) to alert them of the situation and share your location so that you can receive rapid medical attention. It also sends SMS alerts to emergency contacts which users can customize through the app.

How we built it

With newly-available access to Healthkit data, we queried heart sensor data from the Apple Watch in real time. When these data points are above or below certain thresholds, we capture the user's latitude and longitude and make an HTTPRequest to a Node.js server endpoint (currently deployed to heroku at http://cardiacsensor.herokuapp.com) with this information. The server uses the Google Maps API to convert the latitude and longitude values into a precise street address. The server then makes calls to the Nexmo SMS and Call APIs which dispatch the information to emergency services such as 911 and other ICE contacts.

Challenges we ran into


There were many challenges testing the app through the XCode iOS simulators. We couldn't find a way to simulate heart sensor data through our laptops. It was also challenging to generate Location data through the simulator.
No one on the team had developed in iOS before, so learning Swift was a fun challenge.
It was challenging to simulate the circumstances of a cardiac arrest in order to test the app.
Producing accurate and precise geolocation data was a challenge and we experimented with several APIs before using the Google Maps API to turn latitude and longitude into a user-friendly, easy-to-understand street address.


Accomplishments that we're proud of

This was our first PennApps (and for some of us, our first hackathon). We are proud that we finished our project in a ready-to-use, demo-able form. We are also proud that we were able to learn and work with Swift for the first time. We are proud that we produced a hack that has the potential to save lives and improve overall survival rates for cardiac arrest that incorporates so many different components (hardware, data queries, Node.js, Call/SMS APIs).

What's next for IMpulse

Beyond just calling 911, IMpulse hopes to build out an educational component of the app that can instruct bystanders to deliver CPR. Additionally, with the Healthkit data from Apple Watch, IMpulse could expand to interact with a user's pacemaker or implantable cardioverter defibrillator as soon as it detects cardiac distress. Finally, IMpulse could communicate directly with a patient's doctor to deliver realtime heart monitor data.
",,https://cardiacsensor.herokuapp.com/,,"Best Public Safety or Video Processing App, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Use of Nexmo API, Best Use of MongoDB",health,bethanydavis,TMorcott,joeyraso,alidcastano
heur.io,http://pennapps-xiv.devpost.com/submissions/56606-heur-io,"Inspiration

We like the internet, and we like things.

What it does

Heur.io captures data from IOT devices and converts them into readable metrics.

How we built it

A lot of tears and hard work.

Challenges we ran into

The Bloomberg room.

Accomplishments that we're proud of

Heur.io is universal, meaning any IOT device can be set up with it -- it isn't restricted to only one device.

What we learned

How to create/use a Rest API.

What's next for heur.io

There are a lot more features we wanted to push but just did not have the time to. We hope to further develop this app and release it to the public.
",,http://www.heur.io,,"Best Design Using PYNQ-Z1 Boards, Peripherals, and Tools (3), Best Financial Hack Using the Aladdin API, Best Use of Data Visualization, Best Public Safety or Video Processing App, Best Use of VR/AR for Content Discovery, Best Use of Rapid Prototyping, sposnored by AddLab, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Use of the Capital One API - Nessie, Best Domain Name Registered in PennApps XIV, Best Use of Nexmo API, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Progressive Web App, Best Use of Lyft API",health,omarqureshi,nguyenbrian,jg1123,Syed-Niotic
Muffins,http://pennapps-xiv.devpost.com/submissions/56607-muffins,"Inspiration

What it does

How we built it

ChallengTestes we ran into

Accomplishments that we're proud of

What we learned

What's next for Muffins
",,,,"Best Use of Rapid Prototyping, sposnored by AddLab, Lutron IoT Prize, Best User Experience",hardware,Sydriax,WildermuthH
super powerful search,http://pennapps-xiv.devpost.com/submissions/56608-super-powerful-search,"Inspiration

People needs more convenient tool 

What it does

Grab the database from the ELSEVIER medical database and display the search results on a webpage.
In addition, we display the number of relevant papers vs time, showing the trend of the research topics.

How I built it

We use the ELSEVIER api to get access to the medical paper database, organize/analyze the data and display the results using flask to a webpage. 

Challenges I ran into

We have no experiences, need to start everything from scratch.

Accomplishments that I'm proud of

We successfully create a preliminary version of the search/analysis tool.

What I learned

python, javascript, flask, api requests

What's next for super powerful search

optimize the UI;
add in more data analysis results, using machine learning algorithms (clustering, LDA);
make it more powerful
",,https://github.com/benlindsay/med-paper-search,,"",health,ertexi,benlindsay,TJFord,huikuanchao
GotGas,http://pennapps-xiv.devpost.com/submissions/56611-gotgas,"Inspiration

What it does

Find the most fuel efficient and least costly route

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for GotGas
",,https://github.com/nmittu/GotGas,,"Best Use of Data Visualization, Best Domain Name Registered in PennApps XIV, Best Use of Linode Services, Best User Experience, Best Progressive Web App",social + civic hacking,Anjmittu,AliyahShah,nick_mittu,ajaymysore95
The Sticker Exchange,http://pennapps-xiv.devpost.com/submissions/56614-the-sticker-exchange,"Inspiration

We wanted to create a centralized market place for people to buy and sell hackathon stickers.

What it does

Unlike traditional e-commerce market spaces, our web app is a stock exchange where the underlying assets are hackathon stickers. Prices of stickers are driven up and down by bids and offers, all of which are shown in real-time.

How we built it

Our App is hosted on Heroku and Flask. The backend is coded in Python and makes API calls both to our own database on Heroku and Capital One's database on MongoDB in order to reflect price movements and client account balance change. We implemented algorithms to enable order-matching, execution, and market price calculation. We also have bots simulating real-life trading volumes & activities.

Challenges we ran into

On top of having three out of the four people on our team with little to no background in finacne, we also experienced issues when writing our own APIs and making API calls. Furthermore, order-matching algorithms turned out to be a lot more complicated than we originally envisioned. We also faced challenges when designing valuation methods for stickers, as they are not traditional assets and cannot be priced/valuated in the traditional way.

Accomplishments that we're proud of

We are very proud of the extent to which our app mimics actual trading platforms. Three of our hackers don't have any background in finance so it was really impressive how our team picked up the concepts so quickly. 

What we learned

We learned some really cool, out-of-the-box ways of designing databases and algorithms, as well as Trading 101. 

What's next for The Sticker Exchange

Anything and everything has a market, and that's where TSE will be: as long as there is supply and demand for an asset, we will make a market place for it no matter how unconventional it is. 
",,https://github.com/bluedot951/PennApps2016,,"Best Use of the Capital One API - Nessie, Best Progressive Web App","",rosiezou,timotius,joseph-zhong,bluedot951
Beacons,http://pennapps-xiv.devpost.com/submissions/56616-beacons,"Inspiration

Desire to promote civic welfare.through encouraging personal responsibility and communal empathy. Instead of going after individuals, we hope to induce interest in the community as a whole.

What it does

Provides connections to skilled individuals in the area for those who require the completion of a specific task. The functionality of the application is divided into the medical requests, the miscellaneous utility requests, and the social requests. The user places a Beacon for a specific problem they desire completed. All individuals with the application downloaded within a preset radius and fulfilling a basic skill requirement are notified of the Beacon. 
When installing the application, the user inputs their skills. Sensitive skills such as emergency medical abilities are vetted.

How we built it

Significant labor in the user interface to effectively complement a polished appearance with practical functionality. The back end was powered by Firebase. 

Challenges we ran into

Creating a user interface that was easily accessible by individuals of all socioeconomic backgrounds.
Integrating a system that incorporates real-time location analytics to implement in various manners.

Accomplishments that we're proud of

Proud of establishing an overall concept that has extended and essentially infinite possibilities. The potential for expansion is boundless and we hope to continuously integrate more and more features that further the standing of the overall community.
We had our app compatible in both iOS and Android operating systems.

What we learned

We learned how to effectively use firebase to further our capabilities in back end development.

What's next for Beacons

Creating a social leader board system in which individuals that respond to Beacons earn in-app currency. Corporate sponsors can provide compensation based on the amount of in-app currency one earns. This provides incentives for valuing civic responsibility and thus indirectly improves the community.

Drawing data from a smart watch with heart beat sensing capabilities to judge when an individual enters dangerous levels and automatically notifying authorities and nearby professionals.
",,,,"Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best User Experience, Best Use of Lyft API",social + civic hacking,asilam,Alex1511,virajpuri,lsnow2017
Piazza Plz Help Me,http://pennapps-xiv.devpost.com/submissions/56617-piazza-plz-help-me,"Inspiration

This is my fifth semester as a TA for undergrad CS classes. As our department grows, so does the amount of reliance on the popular QA platform Piazza. Piazza was envisioned to be a place where students could ask questions and receive answers from other students and TAs. However, the number of posts is increasing at nearly an exponential rate, which puts a lot of burden on TAs to keep up, and decreases student motivation to read other people's questions. This leads to a lot of duplicate questions.

This tool is intended to both help students identify more easily what other students are struggling with, and help TAs improve assignments from semester to semester. 

What it does

We use the unofficial Piazza API here to get the data from the course. We then build a graph of questions that reference each other. As TAs, we often try to indicate duplicate questions by tagging the duplicate (using an @question_number), so I built this off the idea that this provides dead simple clustering on the data. We visualize it as a normal graph, and you can very easily see what areas of the homework gave a lot of trouble! 

How I built it

Parsed the course data, put it into the Cytoscape graph api. 

What I learned

This immediately makes it clear what areas of the assignment were difficult. 
Also, even as a TA, I learned quite a bit from playing with the UI, and I found that it was a really efficient way to get information at a glance on what students are struggling with.

What's next for Piazza Plz Help Me

A long time ago, I made a Chrome extension that injects additional functionality into Piazza. I'd like to integrate that into this project to allow students and TA's to ""tag"" posts with more meaningful categories. It would be great to identify clusters of questions in a more automated way, so being able to tag posts is one step closer to using true machine learning algorithms. I thought about various models I could use here to better group up questions than just links, and I think that a supervised learning method would perform much better in the long run. Having an extension with easy buttons to tag would definitely not be terrible to implement and have people use. 

There's a lot more data that I am interested in getting out this that can answer interesting questions. For example, are there students who consistently ask duplicate questions? Can we determine the rate at which students move through assignments by associating certain types of questions with parts of the assignment? Is there a correlation between the rate of instructor response and the average number of questions asked on Piazza? These are all things that are answerable using this infrastructure, and questions that have come up before.

Also, this doesn't incorporate follow up posts.
",,http://piazzaplzhelp.me,,"Best Use of Data Visualization, Best Domain Name Registered in PennApps XIV, Best User Experience",education,feefles
CalPal,http://pennapps-xiv.devpost.com/submissions/56618-calpal,"CalPal is a tool that uses computer vision and optical character recognition to detect details from an event poster and autopopulate a calendar event. Users open up app to a snapchat-like camera viewer and can take a picture or upload an existing photo. The Google Vision API helps to parse the image, which the app processes to determine critical information such as the title, location, and time of the event depicted on the poster. This information is used to automatically populate a calendar entry which is added to the user's calendar. We hope this will make it easier than ever to add and keep track of events you are interested in.
",https://www.youtube.com/watch?v=U2B01TTY7mA,https://github.com/chail/CalPal,,Best User Experience,"",chail1111,kellytan,eli8527
Keep Talking Arduino,http://pennapps-xiv.devpost.com/submissions/56619-keep-talking-arduino,"Inspiration

We enjoyed playing the computer party game Keep Talking and Nobody Explodes with our friends and decided that a real-life implementation would be more accessible and interesting. It's software brought to life.

What it does

Each randomly generated ""bomb"" has several modules that must be defused in order to win the game. Here's the catch: only one person can see and interact with the bomb. The other players have the bomb defusal manual to defuse the bomb and must act as ""experts,"" communicating quickly with the bomb defuser. And you only have room for three errors.

Puzzle-solving, communication, and interpretation skills will be put to the test as players race the five-minute clock while communicating effectively. Here are the modules we built:


Information Display Sometimes, information is useful. In this display module, we display the time remaining and the serial number of the bomb. How can you use this information?
Simple Wires Wires are the basis of all hardware hacks. But sometimes, you have to pull them out. A schematic is generated, instructing players to set up a variety of colored wires into six pins. There's only one wire to pull out, but which one? Only the ""experts"" will know, following a series of conditional statements.
The Button One word. One LED. One button. 
Decode this strange combination and figure out if the button saying ""PRESS"" should be pressed, or if you should hold it down and light up another LED.
Password The one time you wouldn't want a correct horse battery. Scroll through letters with buttons on an LCD display, in hopes of stumbling upon an actual word, then submit it.
Simon Says The classic childhood toy and perfect Arduino hack, but much, much crueler. Follow along the flashing LEDs and repeat the pattern - but you must map it to the correct pattern first.


How we built it

We used six Arduino Unos, with one for each module and one for a central processor to link all of the modules together. Each module is independent, except for two digital outputs indicating the number of strikes to the central processor. On breadboards, we used LEDs, LCD displays, and switches to provide a simple user interface.

Challenges we ran into

Reading the switches on the Simon Says module, interfacing all of the Arduinos together

Accomplishments that we're proud of

Building a polished product in a short period of time that made use of our limited resources

What we learned

How to use Arduinos, the C programming language, connecting digital and analog components

What's next for Keep Talking Arduino

More modules, packaging and casing for modules, more options for players
",,https://github.com/Dor-Ron/keepTalkingArduino,,"",hardware,gpranjal,Dor-Ron,spencerng
Blyndfold,http://pennapps-xiv.devpost.com/submissions/56620-blyndfold,"Inspiration

The average adult makes 35,000 conscious decisions per day. Making decisions leads to decision fatigue, which is the deteriorating quality of decisions made by an individual over a period of time. We want to improve lives by removing the need to make a few decisions - mainly those around deciding what to eat. 

What it does

Blyndfold.me is an Amazon Echo skill that makes the decision of where to eat for you - without telling you. Users decide what kind of food they want, how much they want to spend, and how far they want to travel.

Our service takes that data and uses it to find the perfect restaurant. However, instead of telling the user what it is, we call a Lyft to pick them up. This way, they do not know where they will be eating until they arrive! 

This is a fun way to pick what you are doing or where you are going, with friends and family. Perfect for a spontaneous date night or just hanging out with friends.

How I built it

There are two main components to the our app:


Alexa Skill
Backend Service


We used a NodeJS backend and integrated with Lyft, Google Places, and Twilio APIs. 

Challenges I ran into

We ran into some challenges with authenticating to the Lyft API and proper key storage.

Accomplishments that I'm proud of

We are proud of setting up an Alexa skill and integrating with a custom backend.

What I learned

We learned about Alexa skills, Lambda, and using third party services.

What's next for Blyndfold


Integrate with TripAdvisor to find local attractions
Uber support
""Address-only"" mode, if you would like to drive yourself
Integrate with delivery services to have food delivered - as a surprise, of course!
Set pre-defined modes and preferences (e.g. Taco Tuesday mode, which finds you a restaraunt that serves tacos)

",https://www.youtube.com/watch?v=YB1CTZuOOMw,http://blyndfold.me,,"Best Domain Name Registered in PennApps XIV, Best User Experience, Best Use of Lyft API","",jmaslin,JonathanGrant,varninja,MehdiLebdi
DJ Rainbow Pi,http://pennapps-xiv.devpost.com/submissions/56621-dj-rainbow-pi,"What it does

DJ Rainbow Pi is everything you love about music, with none of the emotional baggage. It plays your .mp3 songs using a 3-meter LED strip, and it maps audio frequency values to the color spectrum in real time. It's totes lit.

How we built it

We used a Raspberry Pi 2, an Arduino UNO, Python, and Flask.

Challenges we ran into

Converting a 3.3V PWM signal to a 5V LED strip just wasn't cutting it with the Raspberry Pi. We instead opted to use the Arduino to interact with the LED hardware, and the Pi for raw processing of audio data. We later ran into significant issues installing some of the signal processing libraries we were using on a 32-bit OS, so much of what we wanted to use was incompatible on the Raspberry Pi. Also, reading bytes one-by-one over a serial data line requires quite some patience.
",,,,"Best Use of Data Visualization, Best Use of Rapid Prototyping, sposnored by AddLab",hardware,joemcadams,AngelinaRisi,NicholasLYang
SMS Doc,http://pennapps-xiv.devpost.com/submissions/56623-sms-doc,"Inspiration

Our team identified two intertwined health problems in developing countries: 

1) Lack of easy-to-obtain medical advice due to economic, social and geographic problems, and 

2) Difficulty of public health data collection in rural communities.

This weekend, we built SMS Doc, a single platform to help solve both of these problems at the same time. SMS Doc is an SMS-based healthcare information service for underserved populations around the globe.

Why text messages? Well, cell phones are extremely prevalent worldwide [1], but connection to the internet is not [2]. So, in many ways, SMS is the perfect platform for reaching our audience in the developing world: no data plan or smartphone necessary.

What it does

Our product:

1) Democratizes healthcare information for people without Internet access by providing a guided diagnosis of symptoms the user is experiencing, and

2) Has a web application component for charitable NGOs and health orgs, populated with symptom data combined with time and location data.

That 2nd point in particular is what takes SMS Doc's impact from personal to global: by allowing people in developing countries access to medical diagnoses, we gain self-reported information on their condition. This information is then directly accessible by national health organizations and NGOs to help distribute aid appropriately, and importantly allows for epidemiological study. 

The big picture: we'll have the data and the foresight to stop big epidemics much earlier on, so we'll be less likely to repeat crises like 2014's Ebola outbreak.

Under the hood


Nexmo (Vonage) API allowed us to keep our diagnosis platform exclusively on SMS, simplifying communication with the client on the frontend so we could worry more about data processing on the backend. Sometimes the best UX comes with no UI
Some in-house natural language processing for making sense of user's replies
MongoDB allowed us to easily store and access data about symptoms, conditions, and patient metadata 
Infermedica API for the symptoms and diagnosis pipeline: this API helps us figure out the right follow-up questions to ask the user, as well as the probability that the user has a certain condition.
Google Maps API for locating nearby hospitals and clinics for the user to consider visiting.


All of this hosted on a Digital Ocean cloud droplet. The results are hooked-through to a node.js webapp which can be searched for relevant keywords, symptoms and conditions and then displays heatmaps over the relevant world locations.

What's next for SMS Doc?


Medical reports as output: we can tell the clinic that, for example, a 30-year old male exhibiting certain symptoms was recently diagnosed with a given illness and referred to them. This can allow them to prepare treatment, understand the local health needs, etc.
Epidemiology data can be handed to national health boards as triggers for travel warnings.
Allow medical professionals to communicate with patients through our SMS platform. The diagnosis system can be continually improved in sensitivity and breadth.
More local language support


[1] http://www.statista.com/statistics/274774/forecast-of-mobile-phone-users-worldwide/

[2] http://www.internetlivestats.com/internet-users/
",https://www.youtube.com/watch?v=QGGhnn8mWEc,http://smsdoc.me,,"Best Use of Data Visualization, Best Use of Nexmo API, Best Use of MongoDB, Best User Experience",social + civic hacking,ridoy,jakeaglass,animeshf,SaulAryehKohn
ILOY: In Lieu of You,http://pennapps-xiv.devpost.com/submissions/56624-iloy-in-lieu-of-you,"Inspiration

3 in 5 Americans would try a new brand or company for a better service experience, and yet 78% of American consumers have bailed on a transaction because of a poor service experience. This means that good customer service should be a central part of any business's money-making plan, yet the majority of them succumb to the pitfalls of bad customer service and lose money. In fact, many companies lose more than 600 wage earning hours to inefficient customer service practices every year. 
In order to solve this problem, we attempted to see how to improve the efficiency of customer service while keeping the quality and level of customer appeasement high. Generally speaking, this is a paradoxical problem. People hate to be put on the phone with bots, but human customer service representatives are extremely inefficient. After some thinking, we came to the solution of making bots that are as human as possible.

What it does

ILOY uses multiple different datasets in order to recreate the users voice and tenancies of speaking during a phone call. It uses recordings of the users voice compiled with Festival Speech Synthesis to create a voice that sounds like the user. It then trains a recursive neural network with the users regular daily speech which will allow ILOY to interact with other people in the same way that the original person would in real life. The final step is to plug this into a telephone service which allows ILOY to act and sound like the original user. With all of these things, ILOY will be able to create a convincing simulation of the user in speech and text. 

How We built it

Challenges I ran into

Accomplishments that I'm proud of

What I learned

What's next for In Lieu of You
",,,,Best Use of Linode Services,social + civic hacking,bigmggreer,seyserkoze,wonnor,dasteere
Aladdin,http://pennapps-xiv.devpost.com/submissions/56625-aladdin,"We were inspired by the idea to give a brand new voice based interface to the existing awesome BlackRock Aladdin API from which the BlackRock's end users can benefit. Amazon’s Alexa gives a perfect platform to implement this in a state-of-the-art Speech recognition system using the robust cloud services offered by Amazon. 

Users can perform some of the most useful operations they used to do through the BlackRock web service using this skill deployed on Amazon-Alexa, for example they can get the quotes of the stocks they own or they can get the top performing Funds of the day. They can also ask how the portfolios they own performed during the last year, month or day. So it gives a hands-free and a very convenient interface to interact with the amazing BlackRock APIs using natural language based commands.

We built it using the Aladdin APIs provided by BlackRock to get all the relevant data from BlackRock database. The backend app was developed in NodeJS and was deployed on Lambda server-less compute service with expandable computing server resources. The Alexa skill uses this Lambda function to process the user’s requests and provide with responses in the form of speech. All the configurations for Alexa were done in the Alexa’s Amazon Developer’s console for describing the various intents, slots and utterances.

The biggest challenge that we faced was the unavailability of a real echo device to test our service as the text to speech simulator was not enough to show the real usability of the application. Another challenge that we faced was the limited knowledge that we had of the BlackRock Aladdin’s API and how we could use it to solve the end-user’s problems and the inexperience with AWS Lambda and Node.js

We simplified the user interaction to get financial information from a complex data analysis to a simple question answer based fun exercise. The user just has to ask an abstract question in simple English to Aladdin on Alexa and it will churn the data to give a meaningful response to the user which is really simple to interpret. Also, it opens a whole new avenue to develop speech based apps for financial institutions like BlackRock to serve its end-users. 
Personally, learning to use a financial institution like BlackRock’s API to serve the users and come up with useful information was fun! We also learnt a whole new framework to develop skills for Alexa and picked up some essential skills  like Node.JS and  Amazon Lambda.

What's next? Maybe, Jasmine :P
No :), we have thought of great use cases for Aladdin which can be centered around the end-users, like adding or dropping stocks from the portfolios or suggesting hot stocks for the users based on the market scenarios and also on the user profile. The user can ask for the most recommended suggestions from BlackRock to maximize his/her earnings. 
",https://youtu.be/nrr9kE0iVyw,https://github.com/sagar-sinha/aladdin,,"Best Financial Hack Using the Aladdin API, Best User Experience","",sagar-sinha,hverma
PolitiCall,http://pennapps-xiv.devpost.com/submissions/56627-politicall,"Inspiration

While working on a political campaign this summer, we noticed a lack of distributed system's that allowed for civic engagement.  The campaigns we saw had large numbers of willing volunteers willing to do cold calling to help their favorite candidate win the election, but who lacked the infrastructure to do so.  Even those few who did manage to do so successfully are utilized ineffectively.  Since they have no communication with the campaign, they often end up wasting many calls on people who would vote for their candidate anyway, or in districts where their candidate has an overwhelming majority.  

What it does

Our app allows political campaigns and volunteers to strategize and work together to get their candidate elected.  On the logistical end, in our web dashboard campaign managers can work to target those most open to their candidate and see what people are saying.  They can also input the numbers of people in districts which are most vital to the campaign, and have their constituents target those people.   

How we built it

We took a two prong approach to building our applications since they would have to serve two different people.  Our web app is more analytically focused and closely resembles an enterprise app in it's sophistication and functionality.  It allows campaign staff to clearly see how their volunteers are being utilized and who their calling, and perform advanced analytical operations to enhance volunteer effectiveness.  

This is very different from the approach we took with the consumer app which we wanted to make as easy to use and intuitive as possible.  Our consumer facing app allows users to quickly login with their google accounts, and, with the touch of a button start calling voters who are carefully curated by the campaign staff on their dashboard.  We also added a gasification element by adding a leaderboard and offering the user simple analytics on their performance. 

Challenges we ran into

One challenge we ran into was getting statistically relevant data into our platform.  At first we struggled with creating an easy to use interface for users to convey information about people they called back to the campaign staff without making the process tedious.  We solved this problem by spending a lot of time refining our app's user interface to be as simple as possible.

Accomplishments that we're proud of

We're very proud of the fact that we were able to build what is essentially two closely integrated platforms in one hackathon.  Our iOS app is built natively in swift while our website is built in PHP so very little of the code, besides the api was reusable despite the fact that the two apps were constantly interfacing with each other.

What we learned

That creating effective actionable data is hard, and that it's not being done enough.  We also learned through the process of brainstorming the concept for the app  that for civic movements to be effective in the future, they have to be more strategic with who they target, and how they utilize their volunteers.

What's next for PolitiCall

Analytics are at the core of any modern political campaign, and we believe volunteers calling thousands of people are one of the best ways to gather analytics.  We plan to combine user gathered analytics with proprietary campaign information to offer campaign managers the best possible picture of their campaign, and what they need to focus on.
",,,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best Use of Linode Services, Best User Experience, Best Progressive Web App",social + civic hacking,jakesyl,aep000,sahilambardekar,proman1999
SpyFi,http://pennapps-xiv.devpost.com/submissions/56630-spyfi,"Check it out live! link

Inspiration

From Wikipedia:


In cryptography, a side-channel attack is any attack based on information gained from the physical implementation of a cryptosystem, rather than brute force or theoretical weaknesses in the algorithms (compare cryptanalysis).


As tech products become more prevalent, it is our duty to stay on top of the latest threats to our privacy. Our team stumbled across a potential side-channel attack while playing around with a neat piece of hardware called the bladeRF, a software defined radio.

We discovered that a number of laptops and phones contain network cards that emit a noticeable amount of radio radiation when downloading content from the internet. Apple products in particular displayed the greatest amount of RF activity. We decided to build on this discovery with an interactive IoT data analysis and visualization platform, that we've dubbed SpyFi.

What it does

Our shielded bladeRF listens on a 2.4GHz frequency band for sustained spikes in activity. When holding certain vulnerable phones or laptops close by and downloading internet content, radiation from the device's network card is detected and sent to our Linode server where the data is processed and analyzed, and eventually displayed via a Google Polymer web app, which depicts meaningful graphs and visualizations of the captured radiation.

How we built it

bladeRF

The bladeRF is a software defined radio that can be tuned to pick up a range of frequencies. We were using a 2.4Ghz frequency to measure RF leaks from internet-enabled devices. The blade outputs data in IQ format, which needs further processing and interpretation to make it meaningful.

Linode

Linode is responsible for hosting our web platform, including our Google Polymer app and our MongoDB backend. We run intensive analytical calculations on Linode to avoid overhead on the bladeRF.

Mongo

We used a MongoDB backend, hosted on our Linode server. It receives processed bladeRF IQ data chunks which are then picked up by the frontend for data visualization. Mongo's features and flexibility, namely its capped collections and RESTful API, really tied together our software and hardware.

Challenges we ran into

We had never used a blade before, and getting to know what it was capable of was a dauntingly complicated yet thrilling task.

Accomplishments that we're proud of

We created a polished hack that works perfectly (we hope). Also, we managed to incorporate a huge number of technologies into one project, from capturing signals with the bladeRF to data visualizations with Google Polymer.

What we learned

A healthy bit of math and physics, including transforming IQ data and designing a Faraday cage.
We also gained experience with Polymer, advanced uses for MongoDB, experience with lower level technology and bit manipulation.

What's next for SpyFi

We plan to apply more advanced statistical techniques (such as those seen in side-channel power analysis) to extrapolate more impactful data, applying components of differential power analysis to see if we can extract sensitive information.
",https://youtu.be/iwnOiom2hOA,http://spyfi.me,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Domain Name Registered in PennApps XIV, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Progressive Web App",cybersecurity,petosa,willzma,RiteshMisra,wxia33
Front Desk,http://pennapps-xiv.devpost.com/submissions/56631-front-desk,"Inspiration

Hotels have eliminated lots of friction with booking a room, but services like AirBnB have more unique properties. Meeting a host to enter the property can sometimes be bothersome, so we automate many tasks that the front desk in a hotel would do as a proof of concept for what services like AirBnB could do.

What it does

Check in to short term rental and interface with home automation devices to make transitions seamless.

How we built it

iPhone app, Swift, CloudKit, custom iPad IoT device simulator

Challenges we ran into

Making a web interface for hosts to access client info.

Accomplishments that we're proud of

Fast response times and satisfying click of lock on iPad-home.

What's next for Front Desk

Hopefully our ideas are implemented into services like AirBnB to make booking a place even easier.
",,,,"Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API","",HarrisonWeinerman,patmurray
OHQ: office hours made better,http://pennapps-xiv.devpost.com/submissions/56632-ohq-office-hours-made-better,"OHQ: making office hours better

Office Hours Queue is a web application built by Dorothy Chang, Eric Chiu, and Suzanne Knop during PennApps 2016f. OHQ aims to make office hours a more efficient and data-driven process, and addresses many complaints about how office hours are run, in response to dissatisfaction from both students and TAs. Listed are below are some of OHQ's features and the problems they were made to solve.


Position in queue and estimated wait time: after a student submits a description of their issue to OHQ, they are provided with the number of people in front of them and the estimated amount of time they will have to wait. By making the process more transparent, OHQ allows students to use their time more efficiently: for example, if a student has to wait an hour, they can work on the next problem first.
Student information: TAs can see the list of students in the queue, the type of issue they're having (technical, conceptual, etc), and a short description of the specific problem. One common problem in office hours is students putting their names down before they have actual questions, in hopes that by the time the list gets to them, they will have a question. This prevents that by forcing students to describe their issue. In addition, if a TA sees that two students are having the same problem, he or she can help both of them at the same time, rather than explaining the same concept multiple times.
TA dashboard: TAs can also mark students as being helped or finished. When a TA first helps a student, marking the student as ""being helped"" lets other TAs know to go onto other students. When a student has been successfully helped, the ""finish"" button removes a student from the queue. If a TA cannot figure out a student's particular issue, they can mark them as ""not being helped"", and the student remains at the top of the queue for the next TA to help.
Student form validation: Once students are on the queue, they cannot put their name on the queue again until after they have been helped: this discourages students from signing up several times without specific questions. Note that the student form is easy to integrate with university web logins.
Metrics: Keeping track of how long students wait before they're helped, as well as common categories and problems, will be helpful long-term. For example, if students have to wait 2 hours on Wednesday nights but only 15 minutes on Friday afternoons, more TAs can be made available on Wednesday nights.


Planned functionality: Some categories of problems are easier to solve than others: a compiler error is probably much easier to resolve than five failing test cases. Giving priority to problems that require significantly less time to resolve, such as by putting three compiling problems before one conceptual one, will move the queue along more quickly.

Comments and questions can be directed to Dorothy (dorothyichang@gmail.com), Eric (echiu1997@gmail.com), or Suzanne (sknop8@gmail.com).
",,https://github.com/dorothychang15/ohq,,"Best User Experience, Best Progressive Web App",education,dorothychang15
SmartNotes,http://pennapps-xiv.devpost.com/submissions/56635-smartnotes,"Inspiration

What makes a student exceptional? Of course, it's their work ethic and values which help them succeed. With SmartNotes, students like us can become even more productive - and that's what we're all about. Doing more great stuff, faster.

What it does

SmartNotes is a new way to take notes on an online course, article, or basically any text from the internet. To begin, all you'd have to do is insert your transcript/text data, and start typing. You'll then see sentences that relate to what you are typing and you could instantly insert that text into your notes. From there, you can export your notes as a PDF or to Google Drive.

How we built it

SmartNotes is a web app using a python script that goes through the text given to it and basically identifies keywords that the user can use. The relevant keyword extraction part is implemented in Python using topia.termextract module. This is used to return potentially relevant notes back to the web app. We created our own API to communicate with the module via AJAX requests in JQuery.

Challenges we ran into

At first, we tried using Polymer. However, as we discovered numerous bugs, problems, and incompatibilities with our desired hack, we decided to switch back to what we know best. Pure HTML/CSS/JS

Accomplishments that we're proud of

We're extremely proud of actually making our own API to communicate with our backend. We accomplished personal notes recommendations for individual transcripts for each user.

What we learned

We learned loads of technologies and frameworks -- from troubleshooting with Polymer to figuring out our script and API. We also learned how to connect the front-end and back-end in a seamless way, and since we aren't experts with web apps, we were extremely impressed with our creation. 

What's next for SmartNotes

SmartNotes is an awesome concept. We plan to incorporate OAuth, look into data visualization, and add features like sharing with other students and teachers so that students can really get their work done as soon as possible. 
",https://youtu.be/HUeyczorbKk,https://github.com/ajaxisme/smartnotes,,"Best Use of Data Visualization, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Progressive Web App",education,samayshamdasani,sheilsarda,iChauster,ajaxisme
glTF Emoji Render Plugin,http://pennapps-xiv.devpost.com/submissions/56636-gltf-emoji-render-plugin,"Inspiration

The GL Transmission Format (glTF) is a runtime asset delivery format for GL APIs: WebGL, OpenGL ES, and OpenGL. glTF bridges the gap between 3D content creation tools and modern GL applications by providing an efficient, extensible, interoperable format for the transmission and loading of 3D content. It is announced in GDC 2016 and Siggraph 2016

This self-described model contains geometry, texture, shader, animation, rigging, camera, and scene info. So it's very suitable to be used as Emoji (Meme) through out the web. 

It may be the emoji for the VR 3.0 3D stage, compared to -_-, :-), text expression, in text 1.0 stage, and jpg, gif, png image meme in image 2.0 stage. 

What it does

It's a plugin that renders glTF 3D emoji content in browser. It can be added to forum, blog. The Meme text can be edited. 

We also build a glTF 3D Jackie Chan Meme which is fun!

How we built it

Three.js, glTF, WebGL, Javascript, gulp, blender

Challenges we ran into

glTF format is fresh of shelf format. So there is not that much community support and tools support at this time. Many tools available are not very mature. The model can easily run into some errors. We also find some bugs in three.js glTF loader and we are ready to open pull request to fix it. 

Accomplishments that we're proud of

It's fun! We have a 3D animated Jackie Chan Meme now!

What we learned

Haha a lot

What's next for glTF Emoji Render Plugin

Refactor, clean, make it useful to open source community. contribute to glTF and three.js and WebGL
",,https://github.com/shrekshao/gltf-emoji,,"Best Use of Data Visualization, Best User Experience, Best Progressive Web App",social + civic hacking,shrekshao,ZhiXu,WindyDarian,ChuangLan
PiThon,http://pennapps-xiv.devpost.com/submissions/56637-pithon,"Why the name?

I'm not creative when it comes to names.

Inspiration

Python is my favorite language. In my free time, I dig around through the default cpython interpreter source and every now and then make my own edits to it for fun. This is one of those edits.

What it does

You can store valid python code in an image without actually changing the image, and now run the image as if it were the python code.

Example:
$ python -p myimage.png hello_world.py  # Just prints hello world

# myimage.png can now be run as a python script
$ python myimage.png
Hello world

# Additionally, the image has no visible changes to it and looks the same as it did before

How I built it

Using an image editing library in C, I embed individual bits of the original source code into pixels in the original image. These bits are then extracted from the image when executing it. (Essentially stenanography.) This was implemented in C since the default python interpreter is written in C. The default interpreter was also edited enough to support accepting images as concealing data in those images.

Algorithm Used

The actual algorithm used for concealing data within the images is pretty simple, but can also be changed easily to support other, more complex algorithms. The default one is hiding every two bits in the two least significant bits of the R value in each sequential pixel. For this, the number of bits can be increased or decreased at the cost of altering the image more or not having enough room to store the original code.

Challenges I ran into


C (not C++) has a very limited number of image processing libraries (at least that I know of). As of now, only pngs are supported.
The image that will have the code stored in it must be large enough to actually store the code.
Supporting all the features of python while still being able to run an image


Accomplishments that I'm proud of


Edited the cpython interpreter, the tool that is in charge of evaluating one of the most popular languages in the world.
Did this all by myself
Glad that I know my C as well as my python


What I learned


The source code for cpython is very bulky
There are a limited number of image processing programs in C


What's next for PiThon


The only thing that I have tested for and have not had time to solve yet was being able to use frameworks that exist within a virtual environment in the picture. I am able to import functions/classes from other files given that the image is in the same directory as them or they are also located on the PYTHONPATH.
Implement other algorithms for concealing images

",,https://github.com/PiJoules/image-exe,,"",cybersecurity,lc599
PHELLS,http://pennapps-xiv.devpost.com/submissions/56638-phells,"## Inspiration
Perhaps the biggest factor stopping members of lower class families from getting jobs is illiteracy. Most kids are taught how to read and write in school, but what about those who need extra help. Furthermore, what hope is there for adults who have minimal resources and no teachers to teach them to read. The illiteracy rate among adults in Detroit was recorded to be 48% and we as a team find that to be unacceptable. 

The product

Phonetically helping educational language learning system or PHELLS for short is an electronic based educational tool that matches words to sounds for its users. Targeted towards both adults and kids, PHELLS offers an efficient yet simple way to learn how to read English. Controlled by a Raspberry Pi 3, this python based program works offline and interacts with the user via an LCD display and 4 easily operated buttons. What more? This project requires nothing more than a battery to use.

Complexities

Other than condensing the entire English language into a simple, easy to use program, this program also features an intelligent unit that follows certain linguistic algorithms to enhance the learning experience. When a user takes a reading test, PHELLS comes up with similar looking and sounding wrong answer choices faster and more reliably than a human can. This along with a full interface and an interactive display help make use of the complex background programs installed on the raspberry pi unit.

How Expensive is it?

More like, how inexpensive is it? 1 unit alone would cost at most $50-60 and in mass production this novel product could cost as little as just $30 dollars. That's just under the price of a full tank of gas.   

Challenges we ran into

Any project attempting to mix several skills will inherently breed challenges. Our case was no different. The mixture of a compressed linux OS on a raspberry pi with embedded python running and storing data in mongo db was not initially a harmonious one. The programs reacted poorly to each other and caused a number of setbacks due to mysterious errors with no apparent cause or solution. The pi itself was actually a challenge because, despite many of us having worked with linux and python, the nuances brought into play by the tiny board caused some dramatic delays. Finally, we actually lost an LCD screen, and we were saved by our ability to adapt all of our code to a display of different size withing the last 6 hours of the project.

Accomplishments that we're proud of

The AI unit knows english better than most of our members. Several different, varied technologies, such as embedded python and mongodb, were mixed to achieve a functional and actually usable product.

What we learned

There were so many takeaways from the project. From the message to never trust your hardware too much to the fundamental concept of planning before you execute, there were a ton of things to learn. But, the most important thing that we learned is that there is always more to learn, and that if you don't know something, you can always look it up or find something about it, just like if you don't know how read, you can always use PHELLS

What's next for PHELLS

The impact that PHELLS can make by teaching people English is way too significant to keep away from the rest of the world. We hope to add new languages and voice preferences in the future, as well as to begin simplifying it and packaging it in such a way that charities may begin to take interest
",,,,"Best Use of MongoDB, Best User Experience",social + civic hacking,franke808,azaini
PYNQ Jam,http://pennapps-xiv.devpost.com/submissions/56639-pynq-jam,"We went on relentlessly. Beating back against the tide (that being lack of documentation), we created something truly magnificent out of an underused device. What's next you ask? Well, we'll go on with our lives, but in the back of our minds we will always remember the times we had, and the beauty we effected in this sorry, sullen world.
",,,,"Best Design Using PYNQ-Z1 Boards, Peripherals, and Tools (3)",hardware,didinium
hospital.ity,http://pennapps-xiv.devpost.com/submissions/56641-hospital-ity,"Inspiration

Currently, the process to get medical information from your healthcare provider is intimidating and can be quite a pain for many users.  Hoping to solve this problem, we decided to create an app that would make this process easy and natural while also providing additional comprehensive information on personal health.

What it does

hospital.ity ""makes the hospital hospitable"" – it enables people from all walks of life to retrieve information from their healthcare providers and inform themselves about their personal health with ease.  Users can access everything from blood oxygen levels to prescriptions to medical encounter history.  Furthermore, with its chatbot interface, hospital.ity is extremely user-friendly, allowing people to find the information they want with a simple question.

How we built it

We built the app using Objective-C, queried the Wolfram Alpha and Human API for personal wellness/medical data via REST API calls, and trained the natural language processing (NLP) model using wit.ai. Using image recognition algorithms, the app also provides nutritional information for food in a photograph.

Challenges we ran into

We actually changed our focus halfway through the hackathon from food/nutrition to overall health after discovering the Human API.  However, as we were unfamiliar with the API, which retrieves medical information from users, we initially struggled with authentication in accessing user data.  We also had some difficulty with handling the dozens of possible queries in wit.ai – each potential question required a large number of NLP training cases (each sometimes requiring a large amount of time to learn) for it to work properly in response to conversational speech.  And of course, it was quite the challenge to parse API data and produce the proper response on screen.

Accomplishments that we're proud of

Making the process of finding health information friendly and extremely accessible for the average person.  On the technical side, developing a robust intent identification with NLP and developing algorithms to parse the data for the chatbot.

What we learned

We learned how to train an NLP model via semantic analysis and how to connect it to multiple APIs to create a functioning chatbot.

What's next for hospital.ity

We didn't end up developing for all the queries trained in wit.ai due to time constraints, so there's definitely more to add there.  In addition to implementing more possible queries and strengthening the NLP model, hospital.ity could extend to allowing the user to perform actions such as setting up appointments.  This would ease users' interactions with their healthcare providers even more, furthering our goal of making personal health important and understandable for all.
",,https://wit.ai/carolineh101/hospital.ity,,"Most Entrepreneurial Hack - Blackstone, Best User Experience",health,cho19,nishanthrs
eyeHUD,http://pennapps-xiv.devpost.com/submissions/56642-eyehud,"Inspiration

The blinding sun, rural Canadian driving

What it does

The window heads up display that we have developed can identify real world objects which can be tracked relative to it's user. 

How we built it

The demolition of most of an LCD computer monitor interfaced with two web cams combined with many hours of debugging and a lot of coffee. 

Challenges we ran into:

Developing a calibration procedure that would properly sample the full degrees of freedom of the system. Working with disparate hardware. 

Accomplishments that we are proud of

Getting a fully operational proof of concept. 

What we learned

We gained experience in image recognition software, specifically the openCV library. 

What's next for eyeHUD

Expanding the scope of applicability.


Infrared detection for pedestrians and wildlife in night time conditions
Displaying information on objects of interest 
Police information via license plate recognition
Transition to a fully transparent display and more sophisticated cameras.
General optimization of software. 

",https://www.youtube.com/watch?v=5E8dueHWfjA,https://github.com/Rob-MFn-Fletcher/eyeHUD,,"Best Public Safety or Video Processing App, Most Entrepreneurial Hack - Blackstone",vr/ar,lucasflores,djbrout,robroyfletcher,sebastianhp
Ventus,http://pennapps-xiv.devpost.com/submissions/56643-ventus,"Inspiration

Ventus formed at the unlikeliest of crossroads- four college freshman with little to none coding experience, but aspired to dream big. We noticed that it is difficult to accurately gauge a return on investment in renewable energy, especially wind power, and realized that an gargantuan amount of data is publicly available from the US government through NOAA, or the National Oceanic and Atmospheric Administration. Putting two and two together, we strive to simplify swaths of data to a single output with massive repercussions, both to homeowners and to the renewable energy movement. 

What it does

Ventus draws upon NOAA's database of weather observation sites to parse through thousands of data entries to calculate the average wind speed at that location. Next, Ventus will pass that data through calculations that will determine a more precise estimate of the return on investment. 

How we built it

We used NOAA's v2 API to get data from the database, and then coded a framework as well as a user-friendly interface in Java.

Challenges we ran into

Everything. Everything was a challenge. Especially the NOAA API, because it's probably from the nineties. Oh, and no documentation. Thanks Obama. 

Accomplishments that we're proud of

IT COMPILES AND RUNS AND WORKS. Also, our team didn't ragequit. That would have be bad. 

What we learned

We learned how to use API's and also how to create a user interface in Java. 

What's next for Ventus

We are planning to use existing framework to extract solar radiation data from NOAA, which make adding on a solar option extremely easy. Another step would be to use Google Map's API in order to help search for the nearest stations more effectively as well as perhaps creating a graphics interface that could display regions of the map and help users visualize patterns. 
",,,,"",social + civic hacking,crchong1,reHash,alexdo
eagleEye,http://pennapps-xiv.devpost.com/submissions/56644-eagleeye,"eagleEye

Think Google Analytics... but for the real world



Team


Austin Liu
Benjamin Jiang
Mahima Shah
Namit Juneja


Al

EagleEye is watching out for you. Our program links into already existing CCTV feeds + webcam streams but provides a new wealth of data, simply by pairing the power of machine learning algorithms with the wealth of data already being captured. All of this is then output to a simple-to-use front-end website, where you can not only view updates in real-time but also filter your existing data to derive actionable insights about your customer base. The outputs are incredibly intuitive and no technical knowledge is required to create beautiful dashboards that replicate the effects of complicated SQL queries. All data automatically syncs through Firebase and the site is mobile compatible.

Technologies Used


AngularJS
Charts.js
OpenCV
Python
SciKit
Pandas
FireBase


Technical Writeup

Image processing is done using OpenCV with Python bindings. This data is posted to Firebase which continuously syncs with the local server. The dashboard pulls this information and displays it using Angular. Every several minutes, the front-end pulls the data that has not already been clustered and lumps it together in a Pandas dataframe for quicker calculations. The data is clustered using a modified SciKit library and reinserted into a separate Firebase. The chart with filters pulls from this database because it is not as essential for these queries to operate in real time.

All of the front-end is dynamically generated using AngularJS. All of the data is fetched using API calls to the Firebase. By watching $scope variables, we can have the charts update in real time. In addition, by using charts.js, we also get smoother transitions and a more aesthetic UI. All of the processing that occurs on the filters page is also calculated by services and linked into the controller. All calculations are done on the fly with minimal processing time.
",,https://github.com/aliu139/eagleEye,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best User Experience",social + civic hacking,aliu139,CityofAngels,namitjuneja
EngineeringKit,http://pennapps-xiv.devpost.com/submissions/56645-engineeringkit,"Inspiration

The app idea originated from the expense of testing such basic physical properties as Young's modulus, a standard measure of the stiffness of a material. Instron force testing machines, typically used to measure properties like Young's modulus, are very expensive. In the field of bioengineering, materials tested are often much softer and do not require the full force of the machine. Furthermore, Instron machines are also utilized for labs, yet students don't usually require the full precision of the Instron to complete their work. A portable, cheap way for mechanical and biomedical engineers to measure physical properties is as simple as utilizing the accelerometers in the smartphone. This then evolved into the full EngineeringKit, a user-friendly virtual toolbox with 4 engineering features, all of which usually require more expensive equipment or a greater investiture of time, and brings them together for convenient usage.  

What it does

EngineeringKit has 4 features:

Young's modulus,
Calipers,
Material Properties, &
Mathematics.

How I built it

Built in Android Studio with the Java language; makes use of the phone's accelerometers

Young's Modulus - Uses the phone's accelerometers and known mass and area of the object to calculate stress-strain graph and display it with a graphing library

Calipers utilize listeners for screen touch events and allowing the user to drag lines across the screen to fit against the object needed, then converts the distance between these lines to inches.

Material properties utilizes Google Voice API or text entry to listen for a query, then uses Wolfram Alpha API to search a specific answer, hand-parsing the XML string for the correct data

Mathematics similarly uses Google Voice API or text entry to listen for a query, then uses Wolfram Alpha API to search a specific answer, hand-parsing the XML for a correct image of the needed equation.

Accomplishments that I'm proud of

Implementing the Wolfram Alpha API by querying their website and returning the relevant information for the search.

What's next for EngineeringKit

Calibrating Young's modulus calculator more thoroughly, implementing a Kalman filter to increase its position-sensing accuracy; allowing for stepped force input rather than just constant force which would primarily be useful for only viscoelastic materials; allowing for three-point bending with a 3D-printed attachment to phone case; allowing for calculation of shear modulus
",,https://github.com/carlsonkellie/Engineering-Multitool,,Best User Experience,education,carlsonkellie,jaimiec
HLSettingsView,http://pennapps-xiv.devpost.com/submissions/56646-hlsettingsview,"Inspiration

The current way of handling preferences options on iOS is somewhat frustrating: too many options, too many layers and too much blank spaces. There's gotta be a better to do it. That's why I built this framework.

What it does

It presents users a clear, simple collection view with all the options. It also uses the new UIPreviewInteraction API along with 3D Touch to navigate the layers and toggle switches. (""peek - preview"" delegate callbacks also provided, ""pop - commit"" stage redirects to generic custom settings view controllers)

How I built it

Xcode 8 GM + iOS 10 GM

Challenges I ran into

lack of documentation on 3D Touch and lack of time

Accomplishments that I'm proud of

The outcome looks really good

What's next for HLSettingsView

This is an unfinished project due to the fact that I couldn't spend full time on Pennapps. I will try to complete the framework once iOS 10 is officially released next week.
",,https://github.com/hollisliu/HLSettingsView,,"","",hollis0807
HACK 2G3TH3R (Hack Together),http://pennapps-xiv.devpost.com/submissions/56647-hack-2g3th3r-hack-together,"When we arrived to PENNAPPS we were all ready for a weekend full of programming, learning, as well as enjoyable experiences. However, none of us had a team. Each of us were more than eager to get started and finding people who shared the same interests and scope in programming proved to be challenging. So our problem/project was in front of our noses. We needed to develop a web application that is accessible to organize  teams and prepare for HACKATHONS.

Inspiration

It allow users to provide their own profile, create their own teams, and search for teams they want to join depending on their areas of interest, current skill sets and preference for other group members.

What it does

We built the front-end framework using npm and angular, and the back-end framework using Firebase and JSON. 

How we built it

Challenges we ran into

And we finally made it.

Accomplishments that we're proud of

What we learned

Hopefully in the future, this webapp can be applied to multiple hackathons including PennApps and HackCMU (which will be held next week).

What's next for HACK 2G3TH3R
",,https://github.com/jlautman/HackTeam5-UI,,"Best User Experience, Best Progressive Web App",social + civic hacking,judykong,darodrig,jlautman
Crowd Compute,http://pennapps-xiv.devpost.com/submissions/56649-crowd-compute,"Inspiration

We were on a plane and we were wondering what could we do with these devices in our hands

What it does

.Many people in this world have phones that they are not using at this moment in time. A large subset of these people want money. Our hack crowdsources computational power taking advantage of the increasingly fast growth in smart phone processors. 

Training a machine learning model takes a great deal of power to do and can take a long time when using only one machine. With our platform, the task is parallelized and distributed among multitudes of mobile devices. 

How we built it

We worked together in a cohesive manner keeping team-building concepts in mind in order to create this project-based project. 

Challenges we ran into

Implementing the network for iOS was a difficult task for us

Accomplishments that we're proud of

it works

What we learned

how to make it work

What's next for CrowdCompute

CrowdCompute is set to become an amazing business helping people all around the world, by simultaneously helping people from poor college students to the largest of businesses, allowing for an gr9 thing that mutually helps people.
",https://youtu.be/vTIIMJ9tUc8,https://github.com/kvfrans/Crowd,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Use of the Capital One API - Nessie, Best User Experience, Best Progressive Web App",social + civic hacking,Kevinf120,KevinFrans,liliatangxy,myh1000
Dr. Jarvis,http://pennapps-xiv.devpost.com/submissions/56651-dr-jarvis,"Inspiration

By 2050,16% of the the Global Population will be the elderly. Around 1.5 billlion people will be above the age of 65. Professionals will not be able to cope up with this increased demand for quality healthcare. Many elders don't get in-time treatment, and emergency is always a fear for the children. 
Artificial Intelligence is the solution. 

What it does


Diagnose disease
Offer medicine recommendations
Send daily reports
Create emergency calls to 911
Process injury images


Technology we used


MongoDB
Node.js
Express.js
Python
JavaScript
Twilio
Amazon Echo (hardware)
Camera (hardware)
Machine Learning 
Computer Vision


Challenges we ran into


Integrating the Naive Bayesian and Decision Tree Models for our Limited Test Set data.
Run python file in Node.js


Accomplishments that we're proud of

Integrating and building the backend for Alexa.

What we learned

How to Integrate the Back end with the Cloud services and an Intelligent Speech to Text System

What's next for Dr. Jarvis

Larger Data Set and utilzing Deep learning Convolutional Nerual Networks for multiclass classification.
High Resolution Camera to be integrated with the System for Image to detect visible skin diseases from a persisting trained data set.
",https://youtu.be/AT6ksXyGXYc,https://github.com/Mojiashen1/Dr.Jarvis.git,,"Best Use of Data Visualization, Best Public Safety or Video Processing App, Lutron IoT Prize, Best Use of MongoDB, Best User Experience",health,kartikay,mvivek,Mojiashen1,praneetd
Lantern,http://pennapps-xiv.devpost.com/submissions/56654-lantern,"Inspiration

The inspiration for this project came from the group's passion to build health related apps. While blindness is not necessarily something we can heal, it is something that we can combat with technology.

What it does

This app gives blind individuals the ability to live life with the same ease as any other person. Using beacon software, we are able to provide users with navigational information in heavily populated areas such as subways or or museums. The app uses a simple UI that includes the usage of different numeric swipes or taps to launch certain features of the app. At the opening of the app, the interface is explained in its entirety in a verbal manner. One of the most useful portions of the app is a camera feature that allows users to snap a picture and instantly receive verbal cues depicting what is in their environment. The navigation side of the app is what we primarily focused on, but as a fail safe method the Lyft API was implemented for users to order a car ride out of a worst case scenario.

How we built it

Challenges we ran into

We ran into several challenges during development. One of our challenges was attempting to use the Alexa Voice Services API for Android. We wanted to create a skill to be used within the app; however, there was a lack of documentation at our disposal and minimal time to bring it to fruition.  Rather than eliminating this feature all together, we collaborated to develop a fully functional voice command system that can command their application to call for a Lyft to their location through the phone rather than the Alexa.

Another issue we encountered was in dealing with the beacons.  In a large area like what would be used in a realistic public space and setting, such as a subway station, the beacons would be placed at far enough distances to be individually recognized.  Whereas, in such a confined space, the beacon detection overlapped, causing the user to receive multiple different directions simultaneously.  Rather than using physical beacons, we leveraged a second mobile application that allows us to create beacons around us with an Android Device.

Accomplishments that we're proud of

As always, we are a team of students who strive to learn something new at every hackathon we attend.  We chose to build an ambitious series of applications within a short and concentrated time frame, and the fact that we were successful in making our idea come to life is what we are the most proud of.  Within our application, we worked around as many obstacles that came our way as possible.  When we found out that Amazon Alexa wouldn't be compatible with Android, it served as a minor setback to our plan, but we quickly brainstormed a new idea.

Additionally, we were able to develop a fully functional beacon navigation system with built in voice prompts.  We managed to develop a UI that is almost entirely nonvisual, rather used audio as our only interface.  Given that our target user is blind, we had a lot of difficulty in developing this kind of UI because while we are adapted to visual cues and the luxury of knowing where to tap buttons on our phone screens, the visually impaired aren't. We had to keep this in mind throughout our entire development process, and so voice recognition and tap sequences became a primary focus.  Reaching out of our own comfort zones to develop an app for a unique user was another challenge we successfully overcame.

What's next for Lantern

With a passion for improving health and creating easier accessibility for those with disabilities, we plan to continue working on this project and building off of it.  The first thing we want to recognize is how easily adaptable the beacon system is.  In this project we focused on the navigation of subway systems: knowing how many steps down to the platform, when they've reached the safe distance away from the train, and when the train is approaching.  This idea could easily be brought to malls, museums, dorm rooms, etc.  Anywhere that could provide a concern for the blind could benefit from adapting our beacon system to their location.

The second future project we plan to work on is a smart walking stick that uses sensors and visual recognition to detect and announce what elements are ahead, what could potentially be in the user's way, what their surroundings look like, and provide better feedback to the user to assure they don't get misguided or lose their way.
",,http://www.lightthewayfor.me,,"Best Public Safety or Video Processing App, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best User Experience, Best Use of Lyft API",social + civic hacking,Jovl,Gchorba,caelooney,jrach190
Encrypted Messenger,http://pennapps-xiv.devpost.com/submissions/56655-encrypted-messenger,"PGP Encryption for messenger.com

Pennapps 2016 fall hack.
",,https://github.com/david-cao/encrypt-messenger-extension,,"","",carolinazheng,gmosley,david-cao,bartonb
Emogram,http://pennapps-xiv.devpost.com/submissions/56656-emogram,"Inspiration

We were planning on how to go forward with our project from last Pennapps link (a web app that donates leftover food from restaurants to homeless shelters) and we wanted to know how organizations with similar aims have gone about it in the the past. Moreover, we wanted to know how efficient they were, and how their actions were perceived. What we wanted was an opinion. Then it hit us that we're not the only ones looking for an opinion. So many companies spend billions of dollars on consumer research to find out whether the public loves or hates their products. We forget that Twitter is filled to the brim with the very data we're looking for: opinions on literally everything.

What it does

Emogram is a web app that performs sentiment analysis on recent tweets using Natural Language Processing (NLP) and aggregates this data to put on a map. The map shows the population density of each emotion connected to the topic at hand (be it a company's product or any other issue).

How we built it

We took tweets from twitter using the Twitter Search API and sorted them by location. We used the IBM Watson Alchemy API to analyze the tweets for their emotion. We used d3.js for the data visualization aspect, Linode to host our whole web app.
",,https://github.com/ming-zhang/literate-bassoon,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best Use of Linode Services, Best User Experience, Best Progressive Web App",social + civic hacking,sanjanasarkar,Vichuang,abhaved,mingzhang
Elsa Smart Insulin Pen,http://pennapps-xiv.devpost.com/submissions/56657-elsa-smart-insulin-pen,"Inspiration

Diabetes affects 30 million people in the US (10% of the population), with an estimated 21 million people still undiagnosed. Once diagnosed with diabetes, patients must begin a complete lifestyle shift - they have to exercise on a regular basis, many track the food they eat, and almost all will begin taking insulin since their bodies either don't produce it or can't use it properly.

We believe there are massively impactful ways technology can improve the lives of diabetics, and we decided to focus on insulin management and two problems that diabetics struggle with relating to insulin:

1) Medical adherence is a huge problem, and specifically with diabetes, we see insulin adherence rates at 60% (Type 2). Many diabetics don't correctly follow the insulin injection schedules given to them by their doctors, leading to ineffective treatment and care as well as wasted money (only 60% adherence for Type 2 diabetes)

2) Even once a diabetic has committed to taking insulin, it's often difficult to determine exactly how much insulin they should take. Currently, there's no easy ways for diabetics to take into account their current blood glucose levels and other personal factors in determining how much insulin to inject.

What it does

Elsa is a end-to-end solution that helps diabetics manage their insulin injection schedule - it includes both a smart insulin pen and a companion app. The companion app pulls blood glucose data from from blood glucose meter APIs to maintain a complete picture of the patient's blood glucose levels. With this data, we are able to be predictive about the amount of insulin the patient needs to take. When the patient wants to inject insulin, they can use our recommended amount or change it, and they simply click a button on the smart pen to administer the insulin (no longer do patients have to fiddle with dials on their insulin pens). The smart pen retrieves the amount to inject from the smartphone app and administers the correct amount of insulin.

How we built it

During PennApps, we built all the hardware and software from scratch. We fabricated the casing and all materials for the pen, designed and assembled the electrical components, and designed and programmed the companion app. 

What's next for Elsa Smart Insulin Pen

Our next steps are twofold:


First, we plan to conduct extensive user studies about the habits of diabetics and how we can serve them best
Second, we plan to continue development of the smart pen device and begin submission for FDA approval

",,,,Most Entrepreneurial Hack - Blackstone,health,benhsu,AjayP13,Philiphu
tEXt (working title),http://pennapps-xiv.devpost.com/submissions/56658-text-working-title,"Inspiration

As college students, we find it difficult to make it to our 9 am classes. However, as good students and eager learners, we still strive to accomplish the task of waking up earlier. Despite our valiant efforts, we still find ourselves hitting the snooze button over and over again.

What it does

With tEXt, an alarm clock that texts your ex if you don't prove your full alertness with a minute of vigorous ShakeWeight(TM) activity using your phone, you will never have to worry about being awake to catch that earlier bus in the morning, or making it to your meeting on time. The threat of a message to your ex begging for them to take you back will propel you out of bed in the mornings and catapult you into your day.

How we built it

The alarm was set by using native iOS Date and Time, and Timer functionalities. We built our app solely in Swift in the XCode environment. We imported Alamofire to send requests to the Nexmo API for their SMS service, and parsed the response to make sure the texts were sent.

Challenges we ran into

Lack of certain pieces of hardware throughout the development process. The Nexmo API was also trickier than expected.

Accomplishments that we're proud of

We made a fully functioning, aesthetically pleasing app with all the core functionality we had hoped to implement. 

What we learned

We learned that Apple should really give third party apps the ability to run in the background because the alarm function was alarmingly difficult to implement, but because we succeeded in the end we learned that persistence is key.

What's next for tEXt

We hope to add more functionality such as continuing to assure your state of consciousness by engaging the user in further vigorous activity, both physical, emotional, and mental. After you succeed in completing the first task, we hope to add more benchmarks, such as 5 minutes after completing the first task, having you play a minigame, and if incomplete, sending increasingly desperate messages to the ex, trying to get him or her back.

In addition, we would like to integrate fitness goals into the app by using HealthKit data. If a daily or monthly fitness goal is not met, we are happy to text your ex for you once again. In the future we also want to allow our users to hit the snooze button for a small price.
",,,,"Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API",social + civic hacking,yinting,xwyzhx
N2Patent,http://pennapps-xiv.devpost.com/submissions/56659-n2patent,"Inspiration

After my love of data analysis grew this year, I got to work with a lot of different public datasets and released that the true potential in them is so high and they insights they might offer, excited me greatly.

What it does

This platform is very versatile, in which it takes the USPTO patent application files and does a full analysis on them. So, it initially, takes the huge XML file and uses a XMLsplit file and splits the XML into individual file. Then, it has a SQL stored procedure that creates tables on MSSQL and loads the split patent data XMLs. Then, it develops SQL queries to view data with different dimensions, and uses available data visualization tools to help create visual displays for SQL queries.

How I built it

I used AWS to build it and built it on an EC2. I used 3 freeware; JDK, Netbeans, Microsoft SQL Server. 

Challenges I ran into

I tried connecting to an Amazon RDS database but the data wouldn't load. Making and extracting certain information proved to be a daunting task.

Accomplishments that I'm proud of

I am very proud of making such a versatile software in which I was able to extract all the patent data from the huge XML and its highly complex format. Additionally, I was able to connect it and provide visuals and I find this as just the starting point for a potentially unique business proposition. Additionally, I did not spend a single penny on infrastructure.

What I learned

I learned so many new things about AWS, such as that the free tier is highly limited. But overall, I learned many new things about the EC2 and security loops and etc. Also, I end this hackathon with a much deeper understanding of Microsoft SQL Server, and how to write queries for effective and impactful dimensions/analysis.

What's next for N2Patent

Next, N2Patent is going to fully develop the VC funding page with actual data from Hoovers D&B. Additionally, merging this application data with industry data could prove to be a investment opportunity and a window for new analysis. With new data being incorporated, this could help Intellectual Property Service providers, startups, technology companies, law firms, investors, and venture capitalists.
",,http://www.enterpriseinstance.com,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone",social + civic hacking,nandinpadheriya743
Odin,http://pennapps-xiv.devpost.com/submissions/56660-odin,"Inspiration

Android Development Sucks! Gnarly Gradle errors, sustained setup times, complicated compilation steps. What if there was a better way? We got to thinking we could abstract away all of the ""dirty work"" of android development and leave you to work on functionality, not boilerplate setup.

What it does

Our product allows beginners to easily pick up Android development and execute it remotely with almost zero setup on your part. The user just has to install Nuclide and our small Atom package, and is immediately able to edit, build, and deploy android apps. Instead of having to configure apps over complex tools like ADB, Android Studio, SDK's, and various CLI's, beginners can use our app to directly install their APK onto their phone. In short, we provide a seamless, cloud-based system to: 


Auto-setup a development environment in the cloud with all of the relevant build-tools preconfigured. 
Online hosting of the entire codebase of your app.
Building the app in the cloud. 
Seamlessly push the built app to any android device with our companion app. 


How we built it

The workflow happens in a series of stages:
First the user opens our Atom package (built on top of Nuclide) that remotely logs into our server and lets the user liver edit the code from a cutting edge IDE. When the user wants to build, they hit the button in our toolbar which runs a CoffeeScript to compile the code on the server and start a Cron job to pull the debug logs back to the IDE. 

Meanwhile on the server, a a bash script executes to compile the Gradle project and then exposes a REST endpoint to download the apk. As soon as the user goes to our app and hits the ""Reinstall"" button, the app downloads the APK and installs it. As soon as the installed app starts logging, we push the logs to a Mongo database on our server. Then the Cron job pulls those logs back to the IDE for the user to review.

Accomplishments that we're proud of

The smooth workflow we created around building and deploying apps. The process used to require a lot of setup, but our cloud-based platform abstracts all of that away. 

Our Vision for Odin

The simplicity of web development has brought computer science to the reach of millions of students. Anyone with a browser and notepad can make simple and educational web apps. At Odin, we aim to bring this to freedom to Android. With just a simple application, and students can start building their own games and diving into the world of computer science. We plan to market Odin to school systems nationwide as a way to boost CS education in the country. 

Furthermore, we also realize the potential enterprise applications of having a cloud-based build system and development tool. By harnessing the computational power of the cloud, we can greatly accelerate Gradle build times. This is because our cloud servers can be optimized solely for building APK's. In addition, removing the hassle of local development environment setups is a valuable service for companies small and large. Furthermore, we offer the service of being able to push a complete app or update to any android device with our companion app installed. This greatly simplifies testing and shortens the SDLC. 
",,,,"Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services, Best Use of MongoDB",education,Ysthedood,tchordia,wangalec,raditya
Newstock,http://pennapps-xiv.devpost.com/submissions/56661-newstock,"Though technology has certainly had an impact in ""leveling the playing field"" between novices and experts in stock trading, there still exist a number of market inefficiencies for the savvy trader to exploit. Figuring that stock prices in the short term tend to some extent to reflect traders' emotional reactions to news articles published that day, we set out to create a machine learning application that could predict the general emotional response to the day's news and issue an informed buy, sell, or hold recommendation for each stock based on that information.

After entering the ticker symbol of a stock, our application allows the user to easily compare the actual stock price over a period of time against our algorithm's emotional reaction.

We built our web application using the Flask python framework and front-end using React and Bootstrap. To scrape news articles in order to analyze trends, we utilized the google-news API. This allowed us to search for articles pertaining to certain companies, such as Google and Disney. Afterwards, we performed ML and sentiment analysis through the textblob Python API.

We had some difficulty finding news articles; it was quite a challenge to find a free and accessible API that allowed us to gather our data. In fact, we stumbled upon one API that, without our knowledge, redirected us to a different web page the moment we attempted any sort of data extraction. Additionally, we had some problems trying to optimize our ML algorithm in order to produce as accurate results as possible.

We are proud of the fact that Newsstock is up and running and able to predict certain trends in the stock market with some accuracy. It was cool not only to see how certain companies fared in the stock market, but also to see how positivity or negativity in media influenced how people bought or sold certain stocks.

First and foremost, we learned how difficult it could be at times to scrape news articles, especially while avoiding any sort of payment or fee. Additionally, we learned that machine learning can be fairly inaccurate. Overall, we had a great experience learning new frameworks and technologies as we built Newsstock.
",,http://45.56.96.205:5000/,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services",social + civic hacking,sumitzster,subs1,japopelka,jimtse1
go to rite aid for like an hour or more like 2 and 1/3,http://pennapps-xiv.devpost.com/submissions/56663-go-to-rite-aid-for-like-an-hour-or-more-like-2-and-1-3,"Inspiration

I am sleepy a lot of the time. So are many other people as well. This trend seems to be especially pervasive in places like high school or college. I find it quite disturbing.

What it does

This app tracks your sleeping and your sleep. It tells you how much sleep you are getting.

How we built it

We used Node.js and stuff. It was fun.

Challenges we ran into

Much to my chagrin, I did not sleep much at the beginning of the hackathon, so I became sleepy. Perhaps this occurrence is to be expected.

Accomplishments that we're proud of

I fell asleep so I suppose that's good.

What we learned

It is very difficult to get Mongo to work for you. But Mongo is fun. I spent a lot of time tweaking the data analysis so it worked. I learned how to graph some things using some Node packages.

What's next for go to rite aid for like an hour or more like 2 1/3

Probably make it connect to some sort of IoT button so I can press a button to send data about when I go to sleep or wake up and publish the data automatically.
",https://youtu.be/JAe_PT94OE4,,,"","",justin_yang,tejasmanohar,KevinFrans,Kevinf120
RedSi,http://pennapps-xiv.devpost.com/submissions/56664-redsi,"Background

Millions of students learn and have fun in Minecraft, punching trees, crafting swords, and blowing up their friends' houses with TNT, but Minecraft has another aspect with serious learning potential: Redstone. In Minecraft, you can build circuits using Redstone that behave very similarly to real digital circuits. Although a lot of the complexity of digital circuit design is hidden away, serious experimenters have been able to create amazing circuits, emulating entire computers in Minecraft. If this skill was easy transferrable to the closest industry equivalent, Verilog programming, these students would be on track to have successful careers. RedSi exists to put these students on learning track to take their Minecraft Redstone knowledge and directly compile those circuits into industry standard Verilog which runs on a FPGA.

What it does

The first step of the RedSi process is to build a circuit in Minecraft. Then, the simple /redsi command is run on the server, a typical task that most Minecrafters of all ages are able to complete, and this starts the compilation process. A spigot plugin on the server written in Java collects the block data marked out by the user and sends it to a python program that forms the compiler. The Python program first accounts for most of the complexities of the Minecraft Universe to develop a net list and logic gate list which represents all of the operations performed by the Minecrafter’s circuit. Then, these nets and gates are optimized to remove logic gates that have no function, and after this, a Verilog module is synthesized. This module is then placed in a normal FPGA toolchain, and turned loose on an FPGA development board.

How we built it

RedSi connects to an active Minecraft server through a plugin which can be easily installed. In-game, the user is able to build a circuit using Redstone. Once they complete it, they can activate the plugin through a command-line interface (CLI). The plugin gets all of the selected blocks and uploads their metadata to our Linode server.

The server processes all of the blocks and, following the rules of how Redstone works, convert that into equivalent Verilog code. Then, it generates a full Verilog file and returns that URL to the user to download.

Challenges we ran into

Documentation for creating a Minecraft plugin is of varying quality and not always correct. Several dependencies used the same class names and non-standard method names. Additionally, it was challenging to create a Java build-script that would generate the correct type of jar file for Minecraft to read. 

The Minecraft Redstone system has seen many years of development by a highly skilled development team, and we were required to emulate many of the features of the Minecraft Redstone engine. The system is fundamentally a Redstone to Verilog compiler, and carried with it the technical difficulty compiler design is known for.

Accomplishments that we're proud of

The command-line interface has a number of flags for quick debugging without having to restart the plugin. It made development much easier since we could quickly see information.

We’re very excited that we were able to take the full potential of a children’s learning tool and apply it to a platform very few industry professionals can master. To the extent of our research, no one has done this before. 1020 intense lines of code and five programming languages after we started, we’re able to take most of our Freshmen year digital circuits, make them in Minecraft, and then see them on an FPGA with the power of RedSi.

What we learned

We learned a lot about Java build scripts and the differences between Maven and Ant. We were also exposed to the basic building blocks of compiler design through the design of the Redstone to Verilog Converter. The team also exercised a lot of our computer science knowledge as we worked through various algorithmic problems.

What's next for RedSi

The next step for RedSi is to clean up the code so that it can easily be installed and run on other Minecraft servers. The Verilog compiler we developed can also be refined to work with more complex types of systems.
",https://youtu.be/fwKL3BDwKY4,http://redsi.me,,"Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Domain Name Registered in PennApps XIV, Best Use of Linode Services, Best User Experience",education,fleker,chrisfrederickson,MaxBareiss
MusicShift,http://pennapps-xiv.devpost.com/submissions/56665-musicshift,"Why MusicShift?

When you listen to music, it belongs to you & your friends. We want to make sure you feel that way about every song. Switching aux cords, settling for lackluster playlists, or attempting to plan a playlist in advance doesn't let that happen. Through MusicShift, we make sure that the best playlist is also the most spontaneous.

What is it?

MusicShift is a plug-and-play, ever-evolving collaborative playlist in a box. Just plug in an aux cord, share a QR code with your friends, and let the best music start playing.

MusicShift lets you collaborate on your playlists. You can add songs to your playlist, and even upvote songs that others have added so the more popular songs are played sooner. There is no limit to the songs you can search, and no limit to the number of people who can collaborate on a single playlist through real time multi-user sync.

Playlists can have different purposes too. MusicShift is fun enough to be the music player during a carpool, and sophisticated enough to supply the music in public parks and restaurants. There's no need to worry about how your party's playlist fares when everyone is working together to pick the music.

How it works

MusicShift is made up of three parts: a hardware device, a progressive web app, and a database backend. The hardware device is a Raspberry Pi 2 which polls the backend (MongoDB database of tracks & votes used to generate rankings / play order) for the next Spotify song to play. Using Spotify’s Python bindings & taking advantage of its predictable caching locations, we intercept the downloaded streams and live route them to the aux output. 

Meanwhile, our progressive web app built using Polymer offers a live view into the playlist - what’s playing, what’s next, the ability to upvote/downvote songs to have them play sooner or later, and of course skip functionality (optional, configurable by the playlist creator). It loads instantly on users’ devices and presents itself as a like-native app (addable to the user lockscreen).

What's next?

Here's a look at the future of MusicShift:


User authentication, so you have complete control over your playlists
Playlist uploads through Spotify integration
Establish private and public streams for different settings and venues
NFC or Bluetooth beacon with MusicShift for easier connection

",,https://musicshift-85305.firebaseapp.com/,,"Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Use of MongoDB, Best User Experience, Best Progressive Web App",hardware,masonmeyer,Ralph_Maamari,theopolisme,anantdgoel
Matr,http://pennapps-xiv.devpost.com/submissions/56666-matr,"Inspiration

A big part of why I attended PennApps was to learn about how to host an effective Hackathon since I will be organizing HackKean at Kean University in Spring 2017. I really wanted to make something with Hackathons in mind and this actually helps with any big events in general. It's a loss prevention system and something I definitely will be using in the future!

What it does

matr stands for Material Assistant Tracking Response
It creates codes that can be attached to possessions so if those possessions are lost a simple scan of the code will alert the owner via Email, and text, and if they allow it they can receive a phone call as well.

How I built it

I used Java the Android Platform for the mobile app, applying XML, SMTP protocols, and handling different network provider channels. It also comes with a companion app that is built with JavaFX, CSS. The server I aimed to use was with AWS and MongoDB but that'll be in the future.

Challenges I ran into

An encryption algorithm that was secure and applicable on a standard windows app and Android App, and cloud services. Although they all adhere to AES standards there were very different libraries I would use only to find that some crucial piece was completely missing in another platform.

I also ran into challenges making 

Accomplishments that I'm proud of

Despite feeling tired I managed to help my friend with his project and still even made a companion application. I rolled my own encryption and it works fairly well. The app is very responsive and I had some forethought with security.

What I learned

I learned so much about QRcodes, I have a ton more ideas how to apply them. I also learned about encryption and encryption standards.  I learned a lot about MongoDB although It won't be in my presentation edition.

What's next for Matr

Using the server-base to actually create special one-time codes that could be saved but that a user could assign to different possessions. Possibly hardware-special codes that alert the user when outside a certain proximity.
",,https://github.com/Javaliant/PennAppsXIV,,Best User Experience,social + civic hacking,Legato
PicSpeak,http://pennapps-xiv.devpost.com/submissions/56667-picspeak,"Inspiration

While the world continues to grow more digital, we cannot allow those who depend on traditional resources to fall behind. Although I personally rely on textbooks or notebooks less and less, I realize that many individuals still need paper documents to fulfill their jobs or learn new subjects. However, some people have natural barriers to making that information accessible. For example, what if you have a visual impairment, or lack of familiarity with a language? Therefore, to help make paper documents more understandable, we can expand the senses we use to process that information. What if rather than just reading every document you come across, you could instantly hear it in English, or any language you want?

What it does

PicSpeak, the mobile application, allows people to quickly take a picture of a document and hear it 'speak.' You simply need to snap the image, and wait as the application gives voice to the words in front of you. Also, with the ability to translate in French, German, Korean, and more, you can even take a picture of an English paper and hear it come alive in a foreign language!

How I built it

The mobile application uses React Native and various open-source React Native packages to take advantage of the Camera and FileUploading components of an iPhone or Android device. The backend file conversions take place on the cloud with a Linode Ubuntu server. A node.js and express.js framework team up to make a REST framework that allows the user to post Images and texts to specific endpoints. Finally, Firebase allows the user to have feedback with near instantaneous performance.

Challenges I ran into

Determining how all these varied components can work together in a way that keeps the user on one simple interface proved my greatest challenge. At multiple points I could have decided to add another page to simplify the process, but I really wanted the user experience of anyone using my application to feel intuitive and simple.

Accomplishments that I'm proud of

I feel particularly proud that I managed to convert images to text while providing a quick process of file transferring between my frontend and backend systems. The application works with very detailed documents, which allows it to help those who have trouble reading very fine print.

What I learned

I learned that I love this new form of development called the ‘middle-end,’ a type of coding that takes place when you equally devote time to the backend and frontend of your systems. However, I found out that at many times certain tasks like file conversion don’t necessarily have to take place completely separate from each other. 

What's next for PicSpeak

The addition of more languages to PicSpeak would mean the addition of its capability to impact more people and communities. Also, PicSpeak has the potential to grow into an application that can provide real-time image-speaking. What if you only had to hover over text as you read a document to simultaneously hear its “voice”? Ultimately, as the algorithm for image-to-text conversion only gets more precise with further coding time, the feasibility of this application making a true difference only improves.
",https://www.youtube.com/watch?v=HIXaejgHqR0&feature=youtu.be,https://github.com/15Dkatz/PicSpeak,,"Most Entrepreneurial Hack - Blackstone, Best Use of Linode Services, Best User Experience",education,sfhackerdavid
Yapper,http://pennapps-xiv.devpost.com/submissions/56668-yapper,"Inspiration

We didn't know where to go and nobody wanted the pressure of picking.

What it does

It automatically books a Lyft for you to go to a random nearby restaurant. We also built a dashboard for restaurants to pay for a premium service where they get feedback from their users and have more customers routed from them, with an option to get more from specific locations.
",,,,"Most Entrepreneurial Hack - Rough Draft Ventures, Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best User Experience, Best Use of Lyft API","",a-sy,davidongchoco,saraxiang,jessicaxiang
Showcase,http://pennapps-xiv.devpost.com/submissions/56669-showcase,"Inspiration

The idea came from attending activity fairs the first week of school and not knowing where to find anything or what was there hat interested us. We faced the same problem while trying to find sponsors we wanted to talk to at hackathons and career fairs. So we decided to make Showcase in order to solve that problem and also made it accessible for the visually impaired.

What it does

It is a platform where event hosts can make their events and everyone showcasing something at the event can put their location, website and details for attendees to discover them easily. It has an interactive map which can be created on top of any venue and filled with booths/spots. It can help manage crowding at large events, and allows attendees to go where they want to.

How we built it

We built it as an HTML 5 web app using Angular.js for the front end and Firebase to store data. We also used canvas and fabric.js to make the interactive map and used python, selenium and Google voice API for accessibility.

Challenges we ran into

Neither of us had worked with Firebase, Canvas or Python so the learning curve was high and it was fun learning new technologies while implementing something useful and fun.

Accomplishments that we're proud of

We are really proud of the smooth interface, the custom canvas edited maps, and the voice navigation we implemented.

What's next for Showcase

We are going to use it for the enormous activities fair at UCLA to make discovery of the 500 clubs easy for the 6000 incoming freshmen. The product is live already so once we test it out at UCLA and polish the kinks, we can let everyone in the community use it.
",,https://github.com/rishabhaggarwal2/Showcase,,"Best Public Safety or Video Processing App, Most Entrepreneurial Hack - Blackstone, Best User Experience, Best Progressive Web App",social + civic hacking,rishabhaggarwal2,aksdad,SlashDK
Klack,http://pennapps-xiv.devpost.com/submissions/56670-klack,"Until now, the only way someone was able to determine their typing speed was by utilizing a website that tracked your typing speed as you typed furiously in short one to two minute bursts. However, Klack is the first extension that allows you to track your keyboard metrics over time, providing a more accurate representation of your day-to-day typing speed. In addition to tracking your typing speed over time, Klack also provides the user with a heatmap of total keystrokes, allowing keyboard enthusiasts to determine which keys undergo the most wear and tear and adjust their typing habits for maximum efficiency.

Klack was conceived during a late-night brainstorming session at Pennapps at the University of Pennsylvania.
",,https://github.com/ehsanmasdar/PennappsXIV,,Best Use of Data Visualization,"",ehsanmasdar,notsosmartasian,jennybean36
tongueSpeak,http://pennapps-xiv.devpost.com/submissions/56671-tonguespeak,"Inspiration

Primarily YouTube tutorials and Coursera videos. Some of my relatives wanted to learn latest tools and technologies like programming, machine learning, and psychology. However, the language barrier always stood high and prevented them from accessing the tons of freely available video lectures on the internet. We were surprised to learn that video translation is not supported by even the major learning platforms and decided to explore this area.

What it does

tongueSpeak essentially translates any given video into a video in another language in a highly scalable manner. It uses machine learning, speech recognition, speech generation, text translation, signal processing (eg: chromagram and FFT algorithms) and audio normalization to stitch together a video translation service.

How we built it

We used numpy and pandas for all mathematical calculations. Scikit-learn for machine learning algorithms. Librosa for signal processing. pydub for audio stitching and splitting, and gtts for speech recognition and generation.

Challenges we ran into

One of the biggest challenges was identifying the gender of speakers as it was necessary to identify the tone of voice of the speakers to preserve the charisma of the original video. Since there is no definite mechanism to do this, we used RandomForest ensemble machine learning classification algorithm trained on 5000 input audio files. This gave us an appreciable 75% accuracy in identifying the gender of the person, through which we adjusted the pitch of the output audio to mimic the input audio.

Apart from gender recognition, we also faced challenges in noise filtering, background music detection and pitch resolution. Altogether, these cutting-edge challenges gave us an opportunity to explore the latest frontiers of machine learning and use sophisticated algorithms to solve challenging problems.

Accomplishments that we're proud of

We understood the overall mechanism of the algorithms, got together a working web service, and processed extensive signal inputs, all in less than 36 hours. Since none of us had prior experience in these domains, this opportunity was a fantastic learning experience.

What we learned

Apart from the obvious gain in technical prowess, especially related to signal processing and machine learning, we also learnt essential interpersonal skills task distribution, project planning, collaboration, and effective time management.

What's next for tongueSpeak


Improved background noise filtering
Wider range of languages
Handle multiple overlapping speakers
Deploy as a chrome extension for real-time translation

",,,,Best Public Safety or Video Processing App,education,madhursingal,XenonMolecule,Sampathchanda,kgarg
Parking Pal,http://pennapps-xiv.devpost.com/submissions/56672-parking-pal,"Inspiration

City Officials: Towing Companies ‘Going Rogue,’ Illegally Removing Cars

Alleged Tow Trap Caught on Camera in South Philly

Councilwoman targets illegal activities by Philly towing companies

Are drivers in South Philly getting towed because of a parking scam?

Philadelphia councilwoman wants to crack down on illegal towing

We met a journalist from PhillyVoice who described the problem of rampant illegal towing in Philly. Essentially, towing companies are towing cars parked in perfectly legal places, and disorganiztion at the city level is preventing the police from effectively handling the situation. In the meantime, we've made an application where Philadelphians can view city-sanctioned zones to park, past parking violations, and report first hand if their car was towed at a certain area.

What it does

Displays the places that are common to get a parking violation as well as streets where you can park safely with its hours and pay rates. 

How we built it

We parsed open source data from Open Gov Philly and stored it in a MongoDB schema database, which our front end then used to call through to the google maps API to generate the interactive map.

Challenges we ran into

Government data is horribly organized (latitude and longitude got mixed up 25% of the time)

Accomplishments that we're proud of

We made it in like 8 hours

What we learned

There are lots of problems that we can solve if we look towards the resources given to us from the government. We don't consider a lot of everyday problems and asking non-engineers for inspiration was a great idea. 

What's next for Parking Pal

We only used 10% of the data available - hoping to do some neat machine learning next
",,,,"Best Use of Data Visualization, Best Public Safety or Video Processing App, Best Use of MongoDB",social + civic hacking,cszhu,rniemo
BeThere,http://pennapps-xiv.devpost.com/submissions/56673-bethere,"Inspiration

Intro Psych tells us that the best way to make people do things is to present them with the possibility of losing something.  StickK explains that ""What behavioral science tells us is that we are loss-averse, social animals that make decisions in a time-inconsistent manner. What…? Simply put, we hate losing things and often give into immediate gratification (e.g. eating a donut) at the expense of our long term goals (e.g. losing weight).""

From personal experience, we know that no matter how much we want to build good habits, like getting out of bed to eat breakfast, going to the gym, and attending lecture), when faced with temptations like a cozy bed, extra dessert, or just one more episode of The Office, it's hard to resist.

BeThere uses a combination of loss aversion and accountability to encourage you to reach your full potential. While many services try to encourage you to stop bad habits by forcing you to lose money when you succumb, they rely on the users to keep themselves accountable. It's easy to check off a box even though you didn't do the habit, or simply forget to update your lists. BeThere changes that by checking your location for place-based activities. There's no way to fake it: if you weren't in class, BeThere will know. 

What it does

BeThere motivates you to get to places on time. We pull your events from Google Calendar and make sure you're at the location you specified when you said you’d be there. If you’re not, we’ll pull money from your bank account and donate it to a charity. We’ll also tweet from your Twitter account, publicizing to the whole world that you were late. 

BeThere also keeps track of how many times you've been on time and displays your streaks. You can chose how much repeat offenses will increase the charge, as well as which charity you're donating to. 

How we built it

Flask powers a RESTful API deployed on Heroku, which handles integrations with Google Calendar, Capital One, and Twitter. mLab hosts a MongoDB instance user information

iOS app is built in Swift with SwiftCharts, AlamoFire, and CBCalendar. 

Challenges we ran into

None of us had done any sort of work with push notifications before, so learning to use those was a huge challenge.  BeThere had to keep track of location in the background, while complying with Apple's strict rules on background app usage. 

This was also our first time deploying a webserver not on our local computer. It was difficult figuring out how to configure the ports so that we could run it on Heroku.

In addition, this was our first time building something heavy enough that we had to optimize for performance. We ended up trimming down the number of requests and the data we passed per request, but for a while, were trying to test with an API that took 30 seconds to load.

Accomplishments that we're proud of

Push notifications work! Push notifications in the background work! Location services in the background works! If you're not somewhere on time, BeThere will tell you!

We're proud that we managed to integrate with legitimate APIs (Annie finally understands GET and POST requests). 
push notifications. We're proud of the UI (Ngan is a graphic design genius). 

We (think) we finally learned how to use git to collaborate properly.  While some of the commit messages were a bit suspicious, we figured out how to use and merge branches. 

What we learned

How to Google error codes. The importance of synchronization across multiple threads. How to use and make RESTful APIs. How to communicate and split up work as a team (usually, all four of us were working on different things, with periodic breaks to merge). 

What's next for BeThere

We'd like to include alerts that tell you to when you have to leave your current location to get to your event on time. We also want to leverage the power of social influences more by creating a leaderboard where you can compare your streaks and stats with your friends. 
",,https://github.com/dduan97/BeThere,,"Most Entrepreneurial Hack - Blackstone, Best Use of the Capital One API - Nessie, Best Use of MongoDB, Best User Experience",social + civic hacking,annieechen,dduan97,jaredweinstein
Mongo Bongo,http://pennapps-xiv.devpost.com/submissions/56675-mongo-bongo,"Inspiration

Always having to travel from room to room looking for a decent place to study gets annoying quickly. Whether you are in a library looking for a quiet place to study, or a building you are unfamiliar with, trying to find a quiet place to read, we wanted to come up with a solution that would make it easy to see the real time status of a particular environment.

What it does

This project was built on the premises of combining hardware with software to create a revolutionary idea. Mongo Bongo is a powerful network hidden behind a beautiful website. It gathers and processes raw information from sensors constantly gathering information, and converts it into a simple interface that anyone can understand. Long story short, we base the status of a room by the inputs we get from the sensors. We have four sensors, vibration, sound, light, and temperature. For example, if the sound in a particular classroom is louder than a certain threshold, and the lights are on as well, we can assume that the classroom is either busy or in use, and is not a fit place to study. Additionally, if the vibration sensor has a high reading, we can assume that not only is the room not quiet, but also rather active and not a good place to study. On the other hand, if a room is dark, and there is very little sound, we can assume that the classroom is a better place to study, and that information will be displayed to the end user on a nicely made website.

How we built it

We have a temperature sensor, a vibration sensor, a photoresistor (light sensor), and a microphone. Data is constantly being collected across different classrooms and that data is wirelessly sent from the sensors through the adafruit Wi-Fi shield (managed with Arduino), then transmitted using Python to a server running Mosquitto and MongoDB to catalog the sensor outputs. Then the Mosquitto server publishes the raw data to a Java API that then retrieves the data, parses it, organizes it, and then stores it on the MongoDB server. Then lastly that information is retrieved from the MongoDB server by a web-server in order to publish live statuses of different rooms to a beautifully custom made website that easily illustrates what rooms are available based on an algorithm that relies on predetermined thresholds.

Challenges we ran into

Just about everything. The first night we had issues authenticating the Adafruit Huzzah wifi micro controllers to the wi-fi network in order to wirelessly transmit information. Another huge issue we had was getting them to communicate using the MQTT messaging protocol to our server so that we could ultimately use the data. The issues with information transmission didn't stop with the Adafruit devices. 

When trying to fetch data through MQTT with our Java client, we were unable to create a secure connection to the MQTT house. 

Accomplishments that we're proud of

Manipulating data collected by 5 independent wireless through over 10 platforms with different interfaces to create a beautiful end result presented by our web server was by far the most difficult and also gratifying accomplishment of the weekend, as we ultimately only integrated all of these different systems a few hours before our deadline.

What we learned

A lot. A whole lot. Some of us learned new coding languages. Others learned how to spoof MAC Addresses. It was a long painful journey, but if anything the knowledge was worth it.

What's next for Mongo Bongo

Mongo Bongo as itself was never intended to be a long term project, however when speaking to some Music students from The University of the Arts in Philadelphia, we realized that this system could fill a need of theirs to be able to determine if practice rooms were open in their school buildings, saving them time and energy. We plan to stay in touch with them and hopefully develop a prototype for them to pilot.

In the short term, we hope to apply the skills we've used to build an Internet-of-Things system within our own dorm rooms, apartments, and houses.  We plan to continue running our Mosquitto and MongoDB server will stay up for the time being to facilitate these project. We have only scratched the surface of what is possible with these devices, and they will ultimately have a significantly greater value-add as part of our every day lives.
",,https://pennapps.mootoo.co,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best Domain Name Registered in PennApps XIV, Best Use of MongoDB",hardware,muyiwao221,Micool,samslew,gdascoli
Encrypted Messenger,http://pennapps-xiv.devpost.com/submissions/56677-encrypted-messenger,"Inspiration

In the age of common data leaks, we want our conversations to be secure. We focused on one of the most popular chat platforms in the world, Facebook Messenger.

What it does

Using a chrome extension, users can send and receive encrypted messages using PGP. While Facebook is starting to roll out encryption on the mobile version of messenger, we wanted to bring encryption to everyone using messenger.com right now.

How we built it

Our extension hooks into the React components of the page. We monitor incoming and outgoing messages so we can react (haha) accordingly. We also provide uploads and lookups for user's public keys. We used OpenPGP.js for key creation and encryption/decryption and python flask for our keyserver.

Challenges we ran into

There are wayyy to many to list, but there were two main issues.

Hooking into a page using React is really really hard. The HTML is constantly changing, and injecting is also really really hard since Chrome extensions are sandboxed (double injection is the answer!).

Messenger.com has a set list of urls that can make cross-origin requests, and surprise surprise our website is not on that list. We had to send messages to our extension from messenger.com, which in turn acted as a middle man to communicate with our injected script.

Accomplishments that we're proud of

It works!

What we learned

This was our first encounter with chrome extensions and React.

What's next for Encrypted Messenger

We'd love to make it easier to upload and verify PGP keys.
",,https://github.com/david-cao/encrypt-messenger-extension,,"Best Domain Name Registered in PennApps XIV, Best User Experience",cybersecurity,bartonb,gmosley,carolinazheng,david-cao
Lyft$aver,http://pennapps-xiv.devpost.com/submissions/56678-lyft-aver,"Inspiration

Did you know that you could save money on a Lyft ride by just walking one block? Well, with Lyft$aver, we find the lowest price in a 3 block radius to you and help you save on that Lyft. All you have to do is input where you are and where you want to go and we will do the rest. Start saving money now.

What it does

Lyft$aver reccomends the cheapest pick up location based on your current location and destination.

How we built it

We use Google Polymer to build the front end. Our backend was done with the Lyft API to figure out what is the pickup location that save you the most money.

Challenges we ran into

This is our first time doing a hackathon. There is a learning curve for Google Polymer and connecting the front end to the back end is not as straightforward.

Accomplishments that we're proud of

We made it! We get every part of the web app to work together and we built something that is useful for almost everyone.

What we learned

We learned a lot about Google Polymer which is a great front end library. In addition, we have gained a much deeper understanding of RESTful APIs.

What's next for Lyft$aver

We are looking into scaling up the project. Our potential exit is to be acquired by either Lyft or Uber to improve their service and algorithm. The idea of recommending cheap pick-up location for users creates a win-win situation for the companies, drivers, and passengers. Passengers save more money, drivers save on gas, and the companies can better coordinate their resources to provide better services.
",,https://github.com/snchung/lyftSaver.git,,"Most Entrepreneurial Hack - Blackstone, Best User Experience, Best Progressive Web App, Best Use of Lyft API",social + civic hacking,ZX,bfineran,tstat1996
Black Crystal,http://pennapps-xiv.devpost.com/submissions/56679-black-crystal,"Inspiration

We were inspired to create the Black Crystal because of an interest in creating an aesthetically-pleasing ‘life hack.’ Since our team consists of three members who are both Wharton and Engineering students, we often do not have the time to regularly check the stock market. A relaxing desk accessory, such as the Black Crystal and its light display, would be even more functional for casual investors who could be seamlessly alerted to a changing market throughout the day. We also wanted to interact with the Blackrock Aladdin API, and this project offered an opportunity to integrate the Blackrock API and use the data collected on multiple different platforms.

What it does

This lamp interfaces with the stock market and automatically adjusts the lighting color based on market trends. The user can also remotely control which stocks/indices are monitored from a web portal. 

How I built it

The piece itself was created from laser-cut dark-grey acrylic, bolted together, creating a very elegant and sturdy appearance. The electronics consisted of tri-color LEDs connected to an Arduino for data input. 

The software was designed to be simple, customizable, and swift. To this end, the Aladdin API was used to access the market data. The market data is accessed by a web server, which streams this information to the Arduino inside the Black Crystal. The Arduino analyzes the data and determines which color to light up. On the front end, users are presented with an online interface that allows them to remotely customize which stocks/indices to track. The website was made using Node.JS on the backend and Jade on the frontend. 

Challenges I ran into

It is very difficult to wirelessly communicate with an Arduino. Even with a WiFi board, Arduino's WiFI libraries were buggy, with an inability to acquire SSL certificate, connect to WPA 2 Enterprise networks, and an inability to install up to date firmware. The latter was an especially difficult hurdle because this prevented the Arduino from communicating with directly with the Aladdin API, as originally planned. This forced us to design a backend on the website, which was originally planned to only serve as an interface for the user. The fact that we had to design a backend significantly set us back on our schedule. 
Additionally, the laser engravers used a buggy controller software that created parts that did not match the specified dimensions. 

Accomplishments that I'm proud of

Our biggest accomplishment was creating the software that allowed an Arduino to wirelessly interface with a web backend, user portal, and  the Aladdin API. This took the majority of our time, presenting us with numerous hurdles along the way. We are very proud that we solved these hurdles.

What I learned

In solving these problems, not only did we improve our engineering process, but we also learned more about web development and wireless communication.

For everybody on the team, the laser engraving software was brand new, giving us an opportunity to learn how to use DraftSight.

What's next for Black Crystal

The more important future application of Black Crystal and its technology is its application to real-time financial data collection and integration of that data with an ‘internet of things.’ Negative market activity can have a serious adverse effect on an economy, even if there are no real economic effects immediately (one prominent example is the pound’s collapse after Brexit). By integrating various systems with financial data, it may be possible to apply behavioral science to mitigate the harmful impact of certain responses to market changes.

If Black Crystal can reach a mass-market, more people may be inclined to trade in the stock market, as they see the market fluctuations on the daily basis. This would great boost the economy, but for this to happen, we will need to create a plan to mass-manufacture Black Crystal, as well as create a frontend and backend that can handle a heavy user load, while securely transferring data.

Finally, Black Crystal can be designed to display data from multiple markets at the same time. Now that we have proven the concept with a well-designed prototype, we can grow Black Crystal further.
",https://www.youtube.com/watch?v=xldiaTMGHqo&feature=youtu.be,,,"Best Financial Hack Using the Aladdin API, Best Use of Data Visualization, Best Use of Rapid Prototyping, sposnored by AddLab, Lutron IoT Prize, Best Use of Linode Services",hardware,georgepandya,dex_star
PiBench,http://pennapps-xiv.devpost.com/submissions/56681-pibench,"Inspiration

This project was built with the purpose of convenience and necessity. Between school, work, hobbies, and hackathons, the need for test equipment on the go has become more critical. A majority of the hardware I use on a daily basis is open-source and commercial off-the-shelf (COTS). 

What it does

The PiBench is built around the Digilent Electronics Explorer hardware and Waveforms 2015 software, which means that it supports all the same functions and features that are found in the PC, Mac, and Linux versions.

Quoted from the Digilent website:

""The Digilent Electronics Explorer board (EE board) includes all of the test and measurement equipment needed to design and test analog and digital circuits of all types.

Built around a large solderless breadboard, the EE board includes oscilloscopes, waveform generators, power supplies, voltmeters, reference voltage generators, and thirty-two digital signals that can be configured as a logic analyzer, pattern generator, or any one of several static digital I/O devices. All of these instruments can be connected to circuits built on the solderless breadboards using simple jumper wires. 

The EE board is powered by the free, PC-based WaveForms™ software that makes it easy to acquire, store, analyze, produce and reuse analog and digital signals. WaveForms™ runs well on virtually any notebook PC, including low-cost Netbooks. A high-speed USB2 connection ensures all EE board instruments respond in near real-time. 

WaveForms™ data files are stored using standard formats, making it easy to share data between instruments, and to export data to word processors or graphics editors. This means well documented lab/project reports can be completed using standard office tools, and submitted electronically.""

How I built it

The electronic hardware that is in the Pi Bench includes a Raspberry Pi 3, Pi 7"" touchscreen, Digilent Electronics Explorer board, and dual rail power supply that provides power to both the Pi and Electronics Explorer board. All of the electronics are either mounted and secured with bolts, washers, nuts, Loctite, and hot glue to the waterproof equipment case (similar to a Pelican case)

The software, which is officially supported on Mac and Windows, runs on Linux, but is not guaranteed to work and be stable on all distributions. The Pi bench, with some software modifications under the hood, is able to run the software stably in Raspbian Jessie.

Challenges I ran into

Software does not run on the Raspberry Pi. 
Choosing a power supply 5 volts @ 3 Amps and 12 volts @ 2 Amps.

Accomplishments that I'm proud of

By installing a few extra libraries (dependencies) and changing a few configuration files, I was able to run the software stably. I found a power supply that would meet the power requirements for the Raspberry Pi and Electronics Explorer Board, with some overhead.  

What's next for PiBench

In the future I hope to upgrade to a bigger and higher resolution touchscreen (Full HD 1920x1080, and 10"" - 12"" diagonal). Integrate power supply into the case and add a battery for true portability.
",,,,Lutron IoT Prize,hardware,electronicbyjh
Park and Share,http://pennapps-xiv.devpost.com/submissions/56684-park-and-share,"We needed an idea for PennApps and although the implementation of the idea is not perfect, it is still a valuable product to sell to a company like Airbnb, which specializes in renting out living spaces. In this same way, our application intends to rent out parking spaces to people who need parking for any given amount of time and want to select their ideal preferences for a parking space.
",,,,Most Entrepreneurial Hack - Blackstone,social + civic hacking,ajaiswal,yupengf,clokireddy
recon_rocks,http://pennapps-xiv.devpost.com/submissions/56685-recon_rocks,"Inspiration

We wondered if there could be a way to get crucial information about your vulnerabilities in your website, so that you could limit the chances of being hacked. That idea led us to create recon_rocks.

What it does

recon_rocks lets you enter in the URL of your website, and it will run a series of diagnostic tests on the website you entered. It will display all of the crucial information, and even who to contact in case of a bad vulnerability so that it can be fixed as soon as possible. 

How we built it

We build recon_rocks using Python, Flask, and Recon-Ng to build the backend and carry out all of the tests we run on the website. Then we used Bootstrap for the UI, and customized to give it more a terminal/hacky feel to it. 

Challenges we ran into

We ran into the issues of dispatching multiple sub-processes, asynchronously. To solve that problem we first tried to solve it using the Celery framework, and Celery wasn't allowing us to run the processes like we had hoped. We had to learn a new way to multithread tasks. 

Accomplishments that we're proud of

We're really proud of learning how to multithread tasks using Python, because it allowed us so that we wouldn't have to sacrifice any of the tests that we run on the websites. 

What we learned

We learned a lot about Python, and how best to make use of the Flask framework with Python. We also learned about other security tests and options that matter most to people so that we can be sure we are providing them with the right information that's useful.

What's next for recon_rocks

We really want a tool like recon_rocks, and we envision it to be versatile across many tests; some tests that we did not have a chance to add in the short span of time we had. In order to make recon_rocks versatile, we're going to be open sourcing it on GitHub. That will allow anyone who believes or wants a tool like this to have a certain function, they can add it. We want to make sure we are covering as many web insecurities as possible, and the best way to do that is, for us to allow more people to add tests, and give feedback on what's most meaningful in protecting websites from being attacked.
",,http://www.recon.rocks,,Best Domain Name Registered in PennApps XIV,cybersecurity,AlexSantarelli,ttg,amcdevitt
Take Me Out,http://pennapps-xiv.devpost.com/submissions/56686-take-me-out,"Inspiration

There have been countless days when I have stepped into a car with my friends upon the realization that we had no idea as to what we wanted to do. Thus, my team and I came up with the idea of an app that would give a simple recommendation for a night out and learn from its users' preferences. We are all familiar with the examples of Tinder's swipe left/swipe right technique as well as with the concept of the like button. In Take Me Out, we strived to combine the simplicity of these features with machine learning tactics that would learn to give better and better recommendations to users.

What it does

Take Me Out presents the user with an option for a night out, simply a button click away. From user response through like and dislike buttons, the app collects data from the user and uses it to make more informed decisions from recommendation to recommendation

How we built it

We used the Yelp API to search for entertainment opportunities around our location as well as the Lyft API to get a ride there. We developed the app to be cross platform, with versions in both iOS Swift and Android Java.

Challenges we ran into

Our primary challenges were in properly integrating the API's that we wanted to use and in getting the machine learning techniques that we used to be responsive enough to be useful.

Accomplishments that we're proud of

We were able to develop the app for both iOS and Android, as well as use a variety of API's and other resources in order to get it done.

What we learned

It is incredibly helpful to strip an application down to what it's core components are and not to over embellish it with extraneous menus and settings. Having simple settings and API usage allow us to make the most of our user experience and machine learning tactics.

What's next for Take Me Out

We will most likely add some cloud integration (probably with Firebase) to allow user data to be stored on the cloud. We didn't do this here because of a desire for simplicity and immediate access, but use of cloud storage is probably the next step for this application.
",,https://github.com/DanBarychev/TakeMeOut,,"Best User Experience, Best Use of Lyft API",social + civic hacking,write2danielb,saram1604,guangstick,orudenko
Immersive Music Experience,http://pennapps-xiv.devpost.com/submissions/56689-immersive-music-experience,"Inspiration

Trying to simulate immersive music experiences that would soothe and relax the mind! 

What it does

The mobile virtual reality application brings you into this virtual room with some records and a record player. You choose a record and drop it into the player and enjoy the music with ambiance immersion.

How we built it

Using unity and c# to create a google cardboard application. We used the google vr sdk and the android sdk to make full use of unity tied to its VR capabilities. 

Challenges we ran into

We had some difficulties with designing and creating/finding 3d models for the virtual room. Most of all, the hardest challenge was to finally implement the c# code in unity which we had no previous experience with. 

Accomplishments that we're proud of

Creating and compiling a virtual room that looks decent and can play music at the start, while having no experience in the VR world, Unity and C#. 

What we learned

We learnt a lot about coding VR applications, especially using the Unity program. And we had the chance to learn C#.

What's next for Immersive Music Experience

We want to polish the design of the virtual room, have multiple scenarios (disco, classic, rock, hip-hop, vapor wave...) all with their different designs that pop and make the user feel in another world. We want to possibly incorporate history into the immersive music experience. History in the way, we would present images, videos that correlate to the history of a song, its period that it alludes to. Hence providing a relaxing, educational and immersive experience to the user. 
",,https://github.com/SantiagoOrdonez/PennApps,,Best Use of VR/AR for Content Discovery,vr/ar,distinctmind,SantiagoOrdonez
Barrage Game for Live Streaming Video Platform,http://pennapps-xiv.devpost.com/submissions/56690-barrage-game-for-live-streaming-video-platform,"Inspiration: As watching live streaming video platform becomes more popular, we do find chatting with other audience online is also of fun. As a result, we decide to add more functions to barrage which are generated from others' comments. Moreover,  lots of audience in China buy virtual gifts for their favorited anchors, such as rockets, cars etc., which are equal to a specific amount of real money. Our group members agree on a saying -- ""What is taken from the people is used in the interests of the people."" Therefore, we start to design an online game called Barrage Game which allows audience  to share dividends from gifts.

What it does: We simulate a real live streaming video platform which enables audience to send barrages and communicate with anchors more interactively. Each audience has a rocket which could destroy the barrages sent by others and attack anchor's gift. If the gift's life is reduced to zero, this audience could share part of gift with anchors. The barrages protect gifts from damage and the level of protection is related to gifts' value. Also, websites could gain more money from this schematic. We treat this game a good way to attract audience and earn more money.

How we built it: We used C++ to initialize three main classes: User (rocket & bullets),  Gift, Barrage. We judged whether a bullet met with a barrage by checking the distance between their centers. If the distance is shorter than the sum of the radiuses, the barrage is eliminated. When a gift is hit, its stamina goes down and audience's money increases accordingly. When the gift's stamina reaches zero, the audience get a maximum share of money. Yet once the gift flies out of the screen, the game stops.

Challenges we ran into: 1. When we design the mechanism of our bullets, we do meet a great challenge of bullets' track. If a bullet attacks a barrage, it may leave an obvious track on the screen if this barrage disappears.


We have no authority of getting access of a streaming video platform and we also do not have enough time to make one on our own.


Accomplishments that we're proud of : 1. Great teamwork 2. Successfully simulated the environment of a true streaming video platform.

What we learned: Bugs are everywhere in real world.

What's next for Barrage Game for Live Streaming Video Platform: Maybe we could get contact or coordinate with a live streaming video platform. We are expected to build html versions more easily.
",https://youtu.be/XzOJMXoQ8xs,https://github.com/mzymarcus/Pennapps,,Most Entrepreneurial Hack - Blackstone,social + civic hacking,ziangzh,yibochen,mzymarcus
BrunchOut,http://pennapps-xiv.devpost.com/submissions/56691-brunchout,"Inspiration

Me and my co-workers always have last minute plans to go out to eat, but its always a mess trying to find somewhere to eat without pre-planning. I believe BrunchOut solved this use-case since it allows map location and interactivity with the users using the service.

What it does

BrunchOut lets users post places to recommend to go out to brunch to their co-workers. it allows to search the location of the place and add a quick description on why you and your co-workers should go there to eat.

How I built it

I used the Ionic Framework to build this hybrid app. Some technologies that I used include AngularJS, Google Maps API, MongoDBand Cordova to wrap in a native screen (iOS)

Challenges I ran into

This was my first time building a mongoDB backend, and it was a great learning experience for me since I am mostly a front end person. 

Accomplishments that I'm proud of

Learning Google Maps API, and MongoDB

What I learned

How to incorporate APIs and their services to a product

What's next for BrunchOut

I would like to finish this project so me and my co-workers could use it. Some features that I have in mind are SMS when there are X amount of votes, and emails about upcoming brunches.
",,https://github.com/jonmrod/BrunchOut,,Best Use of MongoDB,"",jrod224
TweetVoteML,http://pennapps-xiv.devpost.com/submissions/56694-tweetvoteml,"Inspiration

Apparently Facebook can tell your political leanings based on if you like Ben and Jerry's Ice Cream. This is the next best thing.

What it does

Predicts who you'll vote for this election using Machine Learning (scikit-learn) and your Twitter data.

How we built it

Pressed buttons on our computers until we stopped getting error messages.

Challenges we ran into

Deploying to Heroku
Sleeping through people's annoyingly loud Bluetooth speakers

Accomplishments that we're proud of

Ubering to the Apple Store to get an ethernet adapter so I could reinstall my OS, which I didn't end up actually doing.

What we learned

It's possible to survive for a weekend off NutriGrain bars and Starbucks Instant Iced Coffee.

What's next for TweetVoteML

Return tweets based on relevancy
""moving React components""
",,http://tweetvote.ml,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best Use of MongoDB, Best User Experience, Best Progressive Web App",social + civic hacking,wibrown,robzajac
Dear Me - your first text message based diary.,http://pennapps-xiv.devpost.com/submissions/56695-dear-me-your-first-text-message-based-diary,"What it does

In short, Dear Me is a fast and easy way for you to keep track of your day to day goals, events, and emotions. Dear Me is a text messaged based diary service that sends users prompts to record what's going on in their life in the form of text messages. 

Every morning, Dear Me will text you and ask you to set a goal for the day, and to talk about how you feel about the day ahead. Over the course of the day, Dear Me will follow up with you to track your progress on your goals as well as your mood and emotions. 

This information is sent back to our server and analyzed, and you are able to view statistics of your life on our website. Some examples would include trends in your mood over the course of the day, week, month, how your sleep schedule affects your emotions, how often you are achieving the goals you set, and when major events in your life have happened.

Inspiration

Dear Me was inspired namely by the decline of diaries/personal journals in the 21st century. In the age of technology, it seems that no one has the time or commitment to keep a paper journal anymore. While social media may seem like an alternative to this - the nature of sharing your content with everyone who's every known you means that your more sensitive topics and thoughts - things you might not necessarily want to share - are never talked about. A private, online diary not 

Dear Me was also inspired by the scrum/agile methodology - especially the concept of the daily scrum in software development. We wanted a way to set and track goals for yourself to improve efficiency, productivity, motivation, similar to what scrum does for companies.

We chose to use an SMS based system over a native mobile because we believe it would more suit our goals. We wanted a fast, easy, and engaging way to keep track of ""diary"" entries - by using an SMS system, we avoided the hassle and complexity of downloading and setting up another mobile app, as well as save the user some space on their phone.

Use Cases

Dear Me not only has use cases in the personal productivity industry, but also in the mental health industry. 

By keeping track of a user's emotion, Dear Me would be able to detect if a user is going through a rough period through their life, if they need any help or resources, or if they're at a major risk for suicide, and point them towards the right help.

Dear Me could also be used as a form of therapy to treat depression. One therapy for treating depression - Behavioral Activation Therapy, involves unlearning depressive behavior, and improving engagement in meaningful activity. Dear Me's daily goal setting prompts and emotion tracking could play a role in these kinds of therapy.

How we built it

We used Nexmo's API to coordinate sending and receiving text messages. Our backend was written in php and mySQL, and our frontend is a single page web app written in HTML/CSS/Javascript. We also worked on integrating our app with text and sentiment analysis APIs such as indicoio and aylien, as well as visualizing the data using google charts API.  

Challenges we ran into

Issues with Nexmo, PHP and mySQL. 

Accomplishments that we're proud of

Using PHP in an app and not destroying everything with it.

What we learned

We learned PHP at this hackathon! 

What's next for Dear Me

Mobile app with offline save option. Finish fully integrating Google Charts API and Text Analysis API. Integrate with other resources (suicide hotlines, suicide prevention resources). Account verification between text app and web app. 
",,http://454bc06c.ngrok.io/dear_me/index.html,,"Best Use of Data Visualization, Best Use of Nexmo API, Best User Experience, Best Progressive Web App",health,broshen,ssc1
Today In History,http://pennapps-xiv.devpost.com/submissions/56696-today-in-history,"Inspiration

We visit many places, we know very less about the historic events or the historic places around us. Today In History notifies you of historic places near you so that you do not miss them.

What it does

Today In History notifies you about important events that took place exactly on the same date as today but a number of years ago in history. It also notifies the historical places that are around you along with the distance and directions. Today In History is also available as an Amazon Alexa skill. You can always ask Alexa, ""Hey Alexa, ask Today In History what's historic around me? What Happened Today? What happened today in India.......

How we built it

We have two data sources: one is Wikipedia -- we are pulling all the events from the wiki for the date and filter them based on users location. We use the data from Philadelphia to fetch the historic places nearest to the user's location and used Mapquest libraries to give directions in real time.

Challenges we ran into

Alexa does not know a person's location except the address it is registered with, but we built a novel backend that acts as a bridge between the web app and Alexa to keep them synchronized with the user's location.
",,http://todayinhistory.me,,"Best Amazon Alexa Hack, Best Use of Data Visualization, Best Domain Name Registered in PennApps XIV, Best User Experience, Best Progressive Web App, Best Use of Lyft API",education,summyfeb12,AniruddhIyengar,anthonytopper
SmartBiddr,http://pennapps-xiv.devpost.com/submissions/56697-smartbiddr,"Inspiration

Frustration at having to browse Ebay to look for the lowest prices and best value for certain products. 

What it does

Uses machine learning to 

How I built it

Challenges I ran into

Accomplishments that I'm proud of

What I learned

What's next for SmartBiddr
",,,,Best Use of MongoDB,social + civic hacking,CulloCampaign,jackfischer11,alexfortis
Runway,http://pennapps-xiv.devpost.com/submissions/56701-runway,"What it does

Runway is a raspberry pi server that allows for people to plug in USBs on the go to share and receive files from their phones.

How we built it

The pi is running a Node http server that exposes an endpoint for retreiving the file structure and individual files, and an endpoint for uploading files to paths within the connected USB. 

Challenges we ran into

There was no network card on the pi, so we had to usb tether our phones to the pi.

Accomplishments that we're proud

First embedded application hack

What's next for Runway


Web and iOS apps
Get an actual wireless card
Get internal power

",,https://github.com/AJLiu/runway,,"Lutron IoT Prize, Best User Experience",hardware,AJLiu1,IvonLiu,alexjordache,lnmangione
Theia,http://pennapps-xiv.devpost.com/submissions/56703-theia,"Inspiration

Providing low cost and eyecare in third world countries can be expensive and require professionals. We want to enable as many people to receive eye care even if they can't afford it. 

What it does

It walks a user through an vision acuity exam and provides an approximate prescription for corrective lenses. 

How we built it

We made it an iOS application. 

Challenges we ran into

Finding out the correct conversions from acuity to a prescribable number.

Accomplishments that we're proud of

We did it.

What we learned

Optometry. Year's worth of Ophthalmology in two days. 

What's next for Theia

VC funding baby!
",,,,"Best Use of Rapid Prototyping, sposnored by AddLab, Best User Experience",health,jalvarado91,takashiw,mcgomez
Tempus,http://pennapps-xiv.devpost.com/submissions/56704-tempus,"Inspiration

Procrastination. Busy and complicated lives. 
Here's the app that makes it super easy to be organized and keep track of what you're doing. 

What it does

Our web app (which is both desktop and mobile friendly) helps you keep track of the time you're spending on certain activities. No longer are you wondering where the day went with your personalized Tempus schedule that can be created simply through adding activities and the time you'll spend on each one. To keep track of the actual number of hours you spend on a task, you just need to click the task you've created to start timing. The anticipated amount of time for each task is compared to the actual number of hours in a graph spent to show you just how productive you are. 

How we built it

For the front end of the project, we used HTML, CSS, JS to make it extra user-friendly. We used NodeJs for backend programming and MongoDB for our database and also implemented the Nexmo's SMS API to send the you notifications just in case procrastination takes over. 

Challenges we ran into

We had some issues implementing the MongoDB API when we were trying to store the user's personalized schedules and tasks. There was a limitation with MongoDB where we cannot update arrays of arrays with one query. Had to work around the issue.

Accomplishments that we're proud of

We're proud of how nicely the web app turned out in terms of its interface and its design, the fact that we created a fully functioning web app and how well we worked together as a team.

What we learned

We learned how to code as part of a team and coordinate our work, despite doing different parts of the app.

What's next for Tempus

We hope to add more features to Tempus like suggestions for the amount of time each activity should take based on the user's history of tasks overtime.
",,https://github.com/Jacob-Gray/PennAppsFall16,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API, Best Use of MongoDB, Best User Experience, Best Progressive Web App",education,itsbonnie,Jacob-Gray,srijithkarippure
SCOPE Camera,http://pennapps-xiv.devpost.com/submissions/56705-scope-camera,"Inspiration

With a vision to develop an innovative solution for portable videography, Team Scope worked over this past weekend to create a device that allows for low-cost, high-quality, and stable motion and panoramic photography for any user. Currently, such equipment exists only for high-end dslr cameras, is expensive, and is extremely difficult to transport. As photographers ourselves, such equipment has always felt out of reach, and both amateurs and veterans would substantially benefit from a better solution, which provides us with a market ripe for innovation. 

What it does

In contrast to current expensive, unwieldy designs, our solution is compact and modular, giving us the capability to quickly set over 20ft of track - while still fitting all the components into a single backpack. There are two main assemblies to SCOPE: first, our modular track whose length can be quickly extended, and second, our carriage which houses all electronics and controls the motion of the mounted camera.

Design and performance

The hardware was designed in Solidworks and OnShape (a cloud based CAD program), and rapidly prototyped using both laser cutters and 3d printers. All materials we used are readily available, such as mdf fiberboard and acrylic plastic, which would drive down the cost of our product. On the software side, we used an Arduino Uno to power three full-rotation continuous servos, which provide us with a wide range of possible movements. With simple keyboard inputs, the user can interact with the system and control the lateral and rotational motion of the mounted camera, all the while maintaining a consistent quality of footage. We are incredibly proud of the performance of this design, which is able to capture extended time-lapse footage easily and at a professional level. After extensive testing, we are pleased to say that SCOPE has beaten our expectations for ease of use, modularity, and quality of footage.

Challenges and lessons

Given that this was our first hackathon, and that all team members are freshman with limited experience, we faced numerous challenges in implementing our vision. Foremost among these was learning to code in the Arduino language, which none of us had ever used previously - something that was made especially difficult by our inexperience with software in general. But with the support of the PennApps community, we are happy to have learned a great deal over the past 36 hours, and are now fully confident in our ability to develop similar arduino-controlled products in the future. In addition, As we go forward, we are excited to apply our newly-acquired skills to new passions, and to continue to hack. The people we've met at PennApps have helped us with everything from small tasks, such as operating a specific laser cutter, to intangible advice about navigating the college world and life in general. The four of us are better engineers as a result.

What's next?

We believe that there are many possibilities for the future of SCOPE, which we will continue to explore. Among these are the introduction of a curved track for the camera to follow, the addition of a gimbal for finer motion control, and the development of preset sequences of varying speeds and direction for the user to access. Additionally, we believe there is significant room for weight reduction to enhance the portability of our product. If produced on a larger scale, our product will be cheap to develop, require very few components to assemble, and still be just as effective as more expensive solutions.

Questions?

Contact us at teamscopecamera@gmail.com
",https://youtu.be/ah9MIiNp7gU,,,"Best Use of Rapid Prototyping, sposnored by AddLab, Most Entrepreneurial Hack - Blackstone",hardware,lichena,saurin078
Sine - Sunglasses that play audio only you can hear.,http://pennapps-xiv.devpost.com/submissions/56706-sine-sunglasses-that-play-audio-only-you-can-hear,"Inspiration

The headphone industry is currently worth almost 20 billion dollars. Top brands such as Beats, Bose, JBL, Skullcandy, and Audio Technica are bandied about by diehard audiophiles and everyday commuters alike. Yet today's popular headphones, both in-ear and over-ear, are not the most user-friendly. Ironically, as desktop computers and smartphone paradigms have evolved, the design of the headphone has remained overwhelmingly stagnant -- the archetypal case features a dangling, twisted cable coupled with an ear tip that needs constant adjustment. Across generations of iPhones and Samsung Galaxys, headphones have featured only minor upgrades, most of which are for glamour rather than utility. A headphone overhaul has long been overdue. 

When Apple introduced the iPhone 7 last week, our minds began to churn. The dethroning of the 3.5mm jack means that wireless audio and other technologies can finally take hold. But one thing has eluded the conversation -- the possibility of using bone conduction to fuel the next generation of smart devices.

The Product

We wanted to take what we knew about audio technology and push it in a different direction. We decided to build eyewear that allows you to listen to high-fidelity audio wirelessly via Bluetooth. Not just that, but audio only you can hear, not anyone else around you. Imagine walking on the beach with your Ray Bans wayfarers, except, instead of just looking like a boss and protecting your eyes, you're also jamming to Taylor Swift.

We did exactly that -- we outfitted eyewear with bone conduction technology. And in the process, we also wanted to solve a significant and dangerous issue that we see with typical headphones -- they often render you oblivious to your environment. We engineered our product in such a way that you can be completely aware of your context while still enjoying high-fidelity audio. Open-ear headphones don't even come close to providing the level of awareness that Sine provides. Also, they don't look as cool.

When we outfitted the eyewear, we worked to preserve the look and feel of regular top-tier eyewear. We spent hours making sure our prototype didn't look like most other hacky hardware projects. We soldered and manually wired our entire solution without any microcontrollers (Arduino or otherwise). We even worked to create a device-agnostic bluetooth solution. The solution does it's own digital signal processing and transmits stereo audio via bluetooth. Filters were generated to account for the attenuation of low frequencies and to ensure that frequency response curve was as close to horizontal as possible given the materials and time constraint.

An important, and often ignored point is the fact that 48 million Americans have significant hearing loss. Since bone conduction technology bypasses the ear drum, people with hearing loss would be able to hear audio from Sine with ease. This allows Sine to tap into a market that is much bigger than any pair of headphones. 

In addition, the nature of this wearable device is such that it allows for many interesting features to be included. Siri/personal assistant integration is possible and completely achievable. Moreover, an autopause feature could be implemented so that the user will never lose his or her place while listening. When the eyewear is taken off, this state would be detectable, and music could be automatically paused and rewinded a few seconds.

Construction

The design features two 8 Ohm & 1 Watt transducers -- one per ear. These were designed in a special housing to ensure that sweat from did not cause the component to short through the skin. The transducers were connected to an amplifier that provides DSP and enables a gain of 16 dB. We also configured a bluetooth solution that supports IAP (Apple protocol) as well as most other devices. The product was designed to have competitive battery life, up to 500 mAh, and fairly low current draw so as to make it usable for day to day activity. In addition, we soldered and wired components to minimize the overall size of the device as much as possible -- something you typically don't see in the run-of-the-mill 36 hour hack.

By the end of the 36 hour period, we were able to supply audio from any bluetooth device and listen to it via bone conduction from the eyewear.

Challenges

Electronics

This project was extremely difficult to pull off, especially given the time constraint. We manually wired and created the prototype with materials that are easily available to us. Connections were finicky, and the overall low-quality condition of materials resulted in many components breaking, even despite the short time constraints:


Two bone conducting transducers broke, resulting from loose connection of wires in the stock model.
One amp shortcircuited itself and was quickly rendered unusable
Soldering rendered numerous sections of components and entire components unusable, and often required significant and meticulous attention to detail.


Hours were spent debugging various circuits using an ammeter and voltmeter.

Luckily, the group was prepared for such issues . However, the time used in attempts to resurrect these components caused many ideas to be cut.

The center of all electrical related problems lies in the RN52 bluetooth module. Loose connections caused the on-board LEDs to be unresponsive until later. Dropped bluetooth connections was also present, as well as a low-volume output. The former two were resolved with extensive testing. The latter was solved by increasing the gain of the amp connected.

Power draw also caused several issues. The original idea had power forked from one power source. However, inconsistent power draw from the bluetooth module and the amplifiers rendered the use of a voltage divider unreliable. The team scrambled to find separate batteries for the two modules in the current prototype.

** Physical model construction**
On the second day, a request to 3D print a module for an enclosure was submitted at 1 pm. No response was given until 9 pm, and the statement told that we were to be unable to print our model. Within minutes, we quickly had to use laser cutting to provide a temporary housing.

Assembling
Assembling was a brutal task, but we took it as a great learning experience.

Accomplishments

How did we fare?
We finished our prototype within the allotted time. At the very end, we had to make some concessions -- We were having issues soldering the components and placing them to a 55mm x 30mm x 28mm box due to unanticipated issues with the wires. As is the case with these sorts of projects, it took us a very long time to debug. In the end, we managed to successfully finish the prototype, but it wasn't as pretty as we wanted.

Preface: Team Background
As a team whose expertise is mostly rooted in software and systems, we wanted to trudge into new territory with a hack that involved almost entirely hardware. We learned a ton in the process, and are very happy with that.

What we learned

We learned a significant amount from this project, especially in terms of designing and debugging complicated circuits, as well as assembling and designing a full-fledged accessory.

What's next for Sine

We believe that this product could be a really compelling vertical in the eyewear market, and think that more features could be built-in to make the experience even more compelling, such as autopause/autoplay.

In many ways, a lot of IoT wearables, especially eyewear, have tended to be unsuccessful and gimmicky. Take, for example, Google Glass, which was very much held in contempt by the general public. We believe that Google Glass failed for two significant reasons. The first is strictly social -- Google Glass featured a camera that would subtly record day-to-day interactions. Have no doubt, this easily comes across as constant leering, and many have described feeling bothered because someone was recording them with their Google Glass. The second reason, we venture, is the lack of painkiller features. Glass provided notifications, which act as great feedback loops for getting users engaged. Beyond that, however, new features were fairly limited and only replicated that which could be accomplished on an average smartphone. 

Sine, by contrast, avoids being pidgeon-holed like Google Glass. Sine's core functionality can be introduced into popular eyewear frames from numerous companies. It will not be perceived as a social failure; in fact, we are inclined to think that it would achieve the opposite effect. The glasses do not double as video cameras; instead, they bring forth a single feature that is arguably a painkiller -- the ability to listen to music without hassle and without sacrificing comfort.

As mentioned previously, Sine-like technology also allows eyewear brands to effectively pitch themselves towards the 48 million American citizens who have significant hearing loss.
",,,,"Most Entrepreneurial Hack - Blackstone, Best User Experience",hardware,gparanja,adcheng,maneetkhaira,agango
Lyft Offline,http://pennapps-xiv.devpost.com/submissions/56707-lyft-offline,"Inspiration

What it does

How I built it

Challenges I ran into

Accomplishments that I'm proud of

What I learned

What's next for Lyft Offline
",,,,"Best Use of Nexmo API, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Progressive Web App, Best Use of Lyft API",social + civic hacking,awadYehya
r-esc,http://pennapps-xiv.devpost.com/submissions/56708-r-esc,"Inspiration

One of our team members Raphael Chang has been working on this ESC for the past year. This weekend, we decided to make some improvements!

What it does

We added two features to the ESC. We started by adding a velocity control mode. The ESC accepts a velocity setpoint and performs feedback control to control the motor at the desired speed.

Additionally, we wrote a ROS driver for the speed controller. This will enable applications of the ESC in robotics. The ROS driver communicates with the ESC via a serial interface.
",,,,"","",anuragmakineni,raphaelchang,brentyi
AdmitMe,http://pennapps-xiv.devpost.com/submissions/56709-admitme,"Inspiration

Rumor has it, if you're good at basketball, you can easily get into CIT. Penn admits many outgoing girls. UChicago likes admitting artists. MIT is excited to see science olympiad recipients. 
Some people spend thousands of dollars hiring professional agencies to help with college application. Others post ""Chance me"" posts desperately on College Confidential. 
How do colleges decide who to admit? We are all curious, but no one actually knows.

What it does

AdmitMe has its unique machine learning algorithm to generate your chances of getting into top US colleges. Students can either log in through their common application account, or manually fill in a form of their personal information. Our web app will provide you a list of colleges of your choice that you are likely to be accepted (in order). You can check detailed statistic information on all the properties. We also provide you with personalized guidance on how you can most efficiently improve your chances, through improving parts of your application and the way you describe your extracurricular activities.

How we built it

We aggregated over 8000 detailed application information for the top 25 US colleges. The data comes from scraping ""Chance"" thread in College Confidential and a lot of data processing. The model is trained through diverse standard exam scores, senior course load, extracurricular activities and personal background. We used MongoDB as our database for all the data needed. The web app uses Python, Django, Javascript and html.

Challenges we ran into

Only one person on the team is familiar with the Django framework, which made it difficult to chain everything together. We ran into compatibility issues with Python 2.7 and Python 3 on the last day of the Hackathon, having to modify the whole algorithm. Natural language processing of self-reported data is challenging as well.

Accomplishments that we're proud of

We're unique. None of the other college counseling apps are using detailed natural language processing and machine leaning like us. We're able to process 8000 valid and detailed student data from nothing. We developed a 70% accuracy algorithm on top college prediction. We were also able to put a full stack website together in less than 36 hours.

What we learned


Django is difficult
Processing raw data to a good dataset is really time consuming. If we can find better data source in the future, that's a much easier way
Collaboration is very important for full stack programming, especially using Django


What's next for AdmitMe


We will develop a business plan to make our product more complete. (It's more of a prototype right now) The scalability needs to be improved. 
Talk to colleges and agencies to aggregate more complete data to develop a more accurate algorithm
Market for our product, we are considering B to B (selling to agencies) or B to C (directly to students). We realized there is a huge market in China as well, where 3 out of our 4 team members are from.

",,http://willtheyadmit.me/,,"Best Use of Data Visualization, Most Entrepreneurial Hack - Blackstone, Best Domain Name Registered in PennApps XIV, Best Use of MongoDB",education,summeryue514,JakeCooper,Chanlaw
Oyster ,http://pennapps-xiv.devpost.com/submissions/56710-oyster,"Inspiration

For the first 18 years of my life, I have lived in the same city. Despite living in the same place for almost two decades, I do not feel like I know the city on an intimate level. My friends and family usually stuck to the same couple of places we all knew we loved. Familiarity becomes comfortable, certainly, but we also know there's a wealth of undiscovered places situated right outside your backyard. 

We created an iOS application called Oyster to promote and embrace spontaneous exploration, whether you're touring a new city for the first time, or simply coming back to a place you've called home all your life. 

What it does

Oyster brings its users to places they wouldn't usually frequent and generates half-day, full-day, or single activity ""adventures"" for users to embark on. 

Input your desired location -- this can be a hotel you're staying at, your house, or anywhere you'd like to get to know better, and Oyster uses the Foursquare API generates an itinerary for the user. These destinations can be restaurants, museums, parks, or any points of interest in the surrounding area. Users can then navigate their walking progress with Apple Mapkit, or choose to take a Lyft.

In order to promote spontaneity, users are rewarded with points for accepting to go to a destination immediately. These points can then be turned into rewards from partnered businesses. In theory partnered businesses, in turn for their sponsorship, would receive increased foot traffic from our users and promotional events in our app, such as bonus points for visiting their locations on certain time periods. Once the user has gone to the location assigned by our app, they can ""check in"" and receive their points. 

Our app also facilitates spontaneous itineraries. Choosing ""Surprise Me!"" will give the user one location, while choosing ""Half-Day"" or ""Whole-Day"" will give itineraries to fill up a larger amount of time.

How we built it

We built the app using Swift 2.0. We employed several API's: we incorporated the Foursquare API using a Swift wrapper called Das-Quadrat and gathered potential points of interest located near the user. We also used the Lyft API to implement a ride-sharing option in case the user wanted mobile transportation to reach their destination. Lastly, we implemented the Apple Mapkit API for navigation, directions, and displaying locations. 

For front end, we used Sketch 3 and Keynote to design and implement the user interface. 

Challenges we ran into

Accomplishments that we're proud of

We are proud of building an app that we would actually like to use.

What we learned

This was our first iOS app we have developed, so learning Swift and how to create an iOS app was a wonderful experience. 

What's next for Oyster: Make the World Yours

There are many possibilities in the future for Oyster. Further development can be done to improve the predictive ability of Oyster to guide you to a place that you not only haven't been to but also will most likely enjoy, adding sponsored events/locations, and opening up potential locations to use user submissions. 
",https://youtu.be/qv6HXAUnfnY,,,"Most Entrepreneurial Hack - Blackstone, Best User Experience, Best Use of Lyft API",social + civic hacking,StephenHe,lindatxia,raunaksg
iPonzi,http://pennapps-xiv.devpost.com/submissions/56711-iponzi,"Inspiration

iPonzi started off as a joke between us, but we decided that PennApps was the perfect place to make our dream a reality. 

What it does

The app requires the user to sign up using an email and/or social logins. After purchasing the application and creating an account, you can refer your friends to the app. For every person you refer, you are given $3, and the app costs $5. All proceeds will go to Doctors' Without Borders. A leader board of the most successful recruiters and the total amount of money donated will be updated.  

How I built it

Google Polymer, service workers, javascript, shadow-dom

Challenges I ran into


Learning a new framework 
Live deployment to firebase hosting


Accomplishments that I'm proud of


Mobile like experience offline
App shell architecture and subsequent load times. 
Contributing to pushing the boundaries of web


What I learned


Don't put payment API's into production in 2 days.
DOM module containment


What's next for iPonzi


Our first donation
Expanding the number of causes we support by giving the user a choice of where their money goes.
Adding addition features to the app
Production

",,https://github.com/patrickkerrypei/iponzi,,"Most Entrepreneurial Hack - Blackstone, Best Use of the Capital One API - Nessie, Best Domain Name Registered in PennApps XIV, Best Progressive Web App",social + civic hacking,arthurhshen,patrickkerrypei,whenjackie
Life,http://pennapps-xiv.devpost.com/submissions/56714-life,"Inspiration

Discrimination, educational games.

What it does

Teaches children values about social equality and treating others kindly.

How we built it

HTML5, CSS, Javascript, Crayon

Challenges we ran into

Drawing with crayon, also I didn't really know Javascript before coming into this project, but I learned a lot.

Accomplishments that we're proud of

Finishing.

What we learned

How to program and understand Javascript better. Also how to art.

What's next for Life

Adding new characters to play as, new scenarios, more illustrations, comparisons between characters.
",,,,"Best Public Safety or Video Processing App, Most Entrepreneurial Hack - Blackstone, Best User Experience",social + civic hacking,lawrence914,matthewzhu
Person of Interest,http://pennapps-xiv.devpost.com/submissions/56715-person-of-interest,"Inspiration: Person of Interest TV show itself. We thought such an idea could be extended to serve a social and practical purpose in today's cities.

What it does: It is a Law enforcement assistant that aids police officials by providing them with an alerting mechanism to detect major past offenders in possible high risk areas. For eg: Past sex offenders in a children’s playground. It is a civic hack that uses face detection and recognition on live security feed to enable police to tighten security in an area if required.

Let's say a person X with a history of major criminal offenses enters an area where the occupants might be especially at risk, such as children or senior citizens. In this case, based on severity of the past offenses, the police are alerted of the presence of person X in the locality. The police may choose to take appropriate measures such as tightening the security in the area.

Our algorithm categorizes crimes based on their degree of severity and color codes them accordingly.

How we built it: We used


Python and OpenCV for face detection and recognition. 
SQL database for the backend
Twilio APIs for Text web services (To provide quick and robust alerts to policemen)


Challenges we ran into


Achieving high accuracy for face recognition with very limited data.
Sending MMS using Twilio APIs
Approaching a sensitive subject


Accomplishments that we're proud of


We built something for a use case we strongly and genuinely believe in.
Learnt many concepts of Computer Vision and Machine Learning on the go.


What we learned

Hackathons are super fun!

What's next for Person of Interest

It has a lot of scope for advancement -


Extend the project to narrow down suspects based on the modus operandi, when there is lack of visual data
Can also have use cases outside Law and order, e.g.: to support businesses by alerting the manager when a premium customer enters the venture.

",https://youtu.be/SImevMQraNs,,,"",social + civic hacking,cyogitha14
OpenCFood,http://pennapps-xiv.devpost.com/submissions/56717-opencfood,"Inspiration

We're college students who are constantly hungry after a long day of coding, but on our college budget, we often have to make do with the ingredients that we already have in our pantry. It can be easy to get stuck into a cooking routine. We're here to break you out of that with OpenCFood!

What it does

First, take a picture of all the ingredients you have and are willing to use. Send that picture to our app, and our program will find which ingredients you have using object recognition, then look up recipes that have use the ingredients that you have. Then, Amazon Alexa will help step you through the recipe you'd like to try. It's that simple!

How we built it

The image processing is implemented through the OpenCV Python SDK. Our front-end is deployed on Google Polymer. We also use Amazon Alexa and the Spoonacular API to look up recipes.

Challenges we ran into

We had a much bigger vision for this app but we soon realized that streaming live video and AR were not realistic for our time frame. However, we pivoted many times and found that this iteration of our idea was the most interesting, so we ran with it.

Accomplishments that we're proud of

Implementing the OpenCV object recognition. That was hard.

What we learned

We learned to be ambitious in the beginning but know when to take a step back to prioritize quality over quantity. Also we learned many new technologies including Google Polymer.
",,,,"Most Entrepreneurial Hack - Blackstone, Best Use of MongoDB, Best User Experience, Best Progressive Web App",health,jeanettepranin,wajihuddin,agamgupta,elicohen2018
Virtual-Augmented Reality,http://pennapps-xiv.devpost.com/submissions/56718-virtual-augmented-reality,"Inspiration

Virtual reality is a blooming technology. It has a bright future regardless of which sector we may use it for. From medical training (holographic human body analysis) to entertainment. Particularly for me, its more of an accessory that I believe one day our civilization may be obsessed upon like the television and internet.

What it does

This app is capable of closing the border between the real and “supposed to be” scientific fantasy world. This app uses the concept of both Augmented reality along with Virtual Reality bringing use one step closer to a different kind of holograms. This app intact is much more superior at what it does than the $4000 Hololens. While Hololens thrives on its projector in the side of the lens and with a very narrow field of view. This app opens up the world of virtual and augmented world in a way which makes it way cheaper alternative than its counterpart along with many other possibilities that ranges from virtual observatory to confidential information transfer in the form of hologram.

How I built it

Using Unity, Vuforia, Google Vr, Android Sdk and along with the support of google cardboard this app was designed in a way that opens up the world our own world like virtual reality along with augmented reality being turned in to accepted reality. For a while I had to work with the different aspect ratio of the object file respective to the surface and the target photo being properly detected by the camera. The connection and proper conversion of apk files took many trials due to rendering and processing speed of the app. Finally it took me 25 hours to achieve a successful prototype. 

Challenges I ran into

The hardest part was setting up the target image and detecting it. The problem was discovered after 3 hours of brute-forcing. The image used in the creation of target image file has significantly lower resolution than the one printed which led to the recognition of the app become sloppy and sometimes undetectable. Getting the app to work on VR cardboard wasn’t easy. Due to the coagulation of pixels and many other rendering errors and bugs, the output achieved from the simulations(which were perfectly normal) were drastically different form the ones in my android device. The object, most of the time, was hovering  half a meter from the point of target image and sometimes was no where to be seen. This was fixed to relocating the image points my changing the co-ordinates.

Accomplishments that I'm proud of

This is my first hackathon and I can’t believe how hard I have worked for this and finally completed my hack. The first day I was planning on an entirely different hack regarding fingerprint and security but I got in to deep trouble when I realized that hardware collection has fingerprint scanner but not the modules that are absolutely necessary for it to be even used. I had almost given up. I had little, almost none experience in apps development but I started learning the whole concept and idea from scratch slowly. Finally when I realized what I could do with VR hack, I pulled all nighters until I could present something worthy of this hackathon.

What I learned

This whole week has been very educational. I recently bought macbook and realized how easy it is when I can use terminal in mac, run vuforia exclusively for making augmented reality and on the other hand how I had to pay to use macbook version of adobe 123d catch and windows one was free but most importantly how two finger swipe across the touchpad cost me my whole night of writing a different version of this essay/cover letter. All jokes aside. I learned a lot about app designing, use of different api, learned how to deal with bugs in apps development . Nonetheless I learned about many of the technologies that I never knew existed like galvanic vestibular stimulation.

What's next for Virtual-Augmented Reality

This projects is merely a prototype compared to the future this idea has. With the upcoming technologies capable of photogrammetry will give rise to higher quality obj file/3D file which if incorporated in augmented reality using camera vr which confuse anyone about what is real or what is fantasy. People can actually see their wildest dreams bring a new era in our society.
",https://www.youtube.com/watch?v=PLX3KOtg55M,https://github.com/Potato-coder/Virtual-Augmented-Reality/,,"Best Use of VR/AR for Content Discovery, Best Use of Rapid Prototyping, sposnored by AddLab, Best User Experience",vr/ar,prangon
ScoreBored,http://pennapps-xiv.devpost.com/submissions/56730-scorebored,"I have been a talkative student my entire life. Expressing myself at silent places has always been hard to control. Afterall, none of us want to get punished right? 
We all pass comments in class -
 ""Damn she's so boring!"",  ""Woah! Did he just say that?"", ""I love the way he teaches!""  

Meeting a bunch of new people with my Friends. How to pass comments about them without being in an awkward situation? 
How to tell my friend about how crazy I feel for the girl who we just met, with her having 0% doubt? 

I just finished my High School. 
So I was wondering, why not create something small, to help millions of people avoid such situations? 

When I finally found my solution to it. I realized, this can be used in so many other MAJOR fields.
AN INTERVIEW.  Judging panel can exchange their views without letting the candidate know.

What it does

Allows 2 people to judge a 3rd person just by Tapping 

*It has 2 columns. Each for a friend. They can tap on BORED, WEIRD, NICE. 
Each button allotted with specific score. Color co-ordinated. So express yourself freely without even looking on screen. * 

Row2 has a bonus usage for students, who count the number of times a few people keep repeating words. Fun part of the App. 

How I built it

Built it purely with JAVA for my Backend. 
Providing simplicity for Frontend of the Android App using XML. 

Challenges I ran into

Having 3 days in my hand. 1 day of Brainstorming. Building the best out of the time I had left. 

Accomplishments that I'm proud of

Building the Entire Frontend and Backend of the AndroidApp on my own. 
Being a freshman student, getting a chance to compete with world's most innovative developers. 
Most important, doing what I love doing the most. 

What I learned

People can surprise you with their Skills.  

What's next for ScoreBored

It can be used to Rate Professors for their perfomannce in a class. 
Will make it an asset for socially awkward people to interact. 
",,https://github.com/pranavjain97/ScoreBored,,"Most Entrepreneurial Hack - Blackstone, Lutron IoT Prize, Best User Experience, Best Progressive Web App",vr/ar,pranavjain97
Lyft4Humanity,http://pennapps-xiv.devpost.com/submissions/56731-lyft4humanity,"Inspiration

""I didn't realize how big the homeless problem was here.""

""I saw a man outside of the food store, begging for change so he could catch a ride home. Later, I was in a Lyft, and I jokingly asked the driver how she would feel about driving homeless people around, and she said she wouldn't mind it. I mean they can't pay for their own rides. So I was like, you know, why not crowdfund it?"" -Tony

Oftentimes, people find themselves in situations where their burden could be eased if they just had a means of transportation: jobs, hospitals, distant family; people miss out on opportunities by not having transportation, and by crowdfunding a way for them to get around, we can help with commutes, relocation, and so much more. Together it can all come together to help a person get back on their feet.

What it does

Lyft4Humanity is a web application designed to assist homeless shelters in getting people back on their feet. It acts as a database for shelters to keep track of and more efficiently assist the displaced as they get back on their feet. More importantly, it connects to the community, providing them with an opportunity to play a role in making the community a better place. Lyft4Humanity acts as a crowdfunding application, aimed specifically at bringing beneficiaries an often overlooked but essential tool, transportation. This way, shelters can provide members with access to resources, which in turn can lead to an opportunity to get a new start.

How we built it

The back-end of Lyft4Humanity is written in Python. The database is powered by MongoDB, running off of a Linode Ubuntu server. The Lyft API handles rides, while Nessie simulates payments and transactions between contributors and accounts. It utilizes Flask to connect the back-end to the front-end, which is written using HTML, CSS, and JS, built on Node.js and utilizing the Typeform API for form filling. Nexmo is used for voice and SMS notifications to keep everyone involved in the loop. 

Challenges we ran into

One of our biggest challenges was connecting the front-end to back-end in different languages (Python and JS). There was also a gap in experience between members of the team, so finding a way to merge different coding practices further exaggerated such issues. We also ran into problems running MongoDB on our own computers, most likely because of the school's firewall, and we used Linode as a solution. Another common issue was incorporating APIs into Python. For example, Lyft's API presented many issues with HTTP requests that were difficult to transfer into Python. Typeform proved to be another challenge to work with in Python when it returned malformed, difficult to fix JSON collections. Retrieving data from MongoDB with Typeform also proved difficult. Overall, although we ran into some hurdles, each of these challenges was eventually overcome in the completed proof of concept.

Accomplishments that we're proud of

Learning a noSQL database like MongoDB was an interesting and challenging experience. The Typeform integration with the UI came out very nice. Learning so many new tools and APIs in such a short time was challenging but rewarding.

What we learned

We each learned a lot about new tools, and along with that some self teaching that will come in handy when learning new technologies in the future. We also discovered a rewarding twist on a previously existing idea and used to it to create something that can benefit others.

What's next for Lyft4Humanity

As with any application that handles money and personal data, security is a major issue that needs to be addressed. Lyft4Humanity is not an insecure app, but security is something that should have a closer look taken at as it is further developed. Expanded database functionality would also be great; right now there is a wealth of unused data being stored. With the use of this data would naturally come new features, such as a ""where are they now"" that could allow people to see the progress of the people they have aided. Alongside this could come meetings or connections between users of Lyft4Humanity, and it is our hope that in this way communities will nurture their people and become closer.
",,https://github.com/inandi2015/lyft4humanity,,"Best Use of the Capital One API - Nessie, Best Domain Name Registered in PennApps XIV, Best Use of Nexmo API, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Progressive Web App, Best Use of Lyft API",social + civic hacking,Jeroxoid,Danny_Wilkins,ynot269,inandi2015
StockTunes,http://pennapps-xiv.devpost.com/submissions/56733-stocktunes,"Inspiration

People are fantastic at pattern recognition.  Much better than any other known organism, in fact.  But are we really leveraging our abilities by looking at charts and graphs?  Perhaps we can find better ways to easily identify patterns in the stock market, and use that analysis to make accurate predictions ...

What it does

StockTunes takes market data and move it from the financial space to the music space.  The data is turned into aesthetic, identifiable music, but retains all of its key characteristics and behavior.  

How I built it

StockTunes runs across three servers.  One server services the client, one handles computation, and one handles midi output.  The client server is built with Electron and Node.js, which is distributed as a desktop application, but can easily be migrated to the web.  The computation server is built with Python and Flask, making it capable of leveraging Python's powerful data science libraries and performing asynchronous analysis.  The third server was built from scratch specifically to process data frames and output the new music.  

Challenges I ran into

This was my first (and hopefully only) solo hack.  I'm a firm believer in teamwork, but I definitely wanted to try one solo hack before I graduated.  Working on this scale without a team is difficult, and not nearly as fun.  

The hardest conception part of the project was creating protocol for allowing the computer to understand music, turning data into music, transforming that music into usable MIDI streams, and playing that MIDI with that ""live"", ""human"" feel.  

The true tech challenge here was creating a platform that reads market data, converts it to music, and outputs that music to midi devices in real time.  This is what consumed most of my time.

Accomplishments that I'm proud of

I build a small server from the ground up capable of servicing high amounts of traffic and outputting MIDI.  

What I learned

I know so much more about asynchronization inside servers now, working with MIDI devices live, and even a little more data science.  

What's next for StockTunes

Make it better!  The analysis can only get deeper and more complex.  The platform can also be used for any data set -- who knows where it can be applied next.  
",,https://github.com/nightCapLounge/StockTunes,,Most Entrepreneurial Hack - Blackstone,social + civic hacking,nightCapLounge
nullHands,http://pennapps-xiv.devpost.com/submissions/56735-nullhands,"Inspiration

Our inspiration comes from our own experiences as programmers, where we realized that it was sometimes difficult (and a productivity drain), to move our right hands between our keyboard and our mouse. We wondered whether there was any possibility of redesigning a new version of the mouse to work without the need to move our hand from the keyboard.

What it does

nullHands utilizes a variety of measurements to provide users with an accurate mouse movement system. First, we utilize a gyroscope to grab the user's head movements, mapping that to the mouse movements on the screen. Then, we monitored the user's eyes, mapping the left and right eyes to the left and right respective mouse clicks.

How we built it

We built this using iOS Swift 3, to monitor the gyroscope. We then used Python with sockets to run the backend, as well as to run the OpenCV and Mouse movement libraries

Challenges we ran into

We did not have the hardware we needed, so we had to improvise with an iPhone, as well as a headband.

Accomplishments that we're proud of

We are proud of managing to create a working prototype in under 24 hours.

What we learned

We learned about sockets and direct communication between iOS and computer.

What's next for nullHands

We are really passionate about nullHands, and think that this is a project that can definitely help a lot of people. Therefore, we plan on continuing to work on nullHands, improving and adding functionality so that we can one day release this as a product so that everyone can experience nullHands.
",,http://lookmanullhands.me,,"Best Public Safety or Video Processing App, Best Domain Name Registered in PennApps XIV, Best User Experience",hardware,dingwilson,allencheng,AdityaPurandare,alexgao
Internet of Trees,http://pennapps-xiv.devpost.com/submissions/56736-internet-of-trees,"The Internet of Trees

What we noticed during our lives at school and volunteering in the community is that currently most townships have no system for maintaining or analyzing dendrological data. This means that they have a very small idea of how the trees that they have planted through their developments are growing. This leads to wide multitude of problems. The most essential two are inability to perform triage or respond to problems immediately. Meaning that for example if a disease was spreading through the trees they would not be able to respond and resolve the issue until it was too late. 

We did some research on the problem to see if there were any solutions to this problem. We found several, however they were all incredibly expensive and required large amounts of time and resources to maintain. Many townships would like to have more accurate knowledge of their trees, but simply lack the resources to do so.

So now we introduce our solution that we called internet of trees. Essentially bringing trees to the web through data. Our app is a proof of concept for the ability to create an open source software that could be used by the government to help them better manage their parks and trees. Through our application they can keep accurate records of their trees and view it at any time. They can also generate statistical models and graphs to see the data in perspective and assess any problems that are occurring.  
",,https://github.com/VVoruganti/Internet-of-Trees,,"Best Use of MongoDB, Best Use of Data Visualization",social + civic hacking,VineethVoruganti,18vshanmugam,akhiy2k,18dgao
graffiti,http://pennapps-xiv.devpost.com/submissions/56737-graffiti,"graffiti

Graffiti is an iOS application which allows you to leave your mark anywhere in the world. Simply load up the application, and you will be able to see virtual graffiti at your location left by other people. 

You can join in on the fun by simply tapping the spray can, which will open up a canvas for you to create your own masterpiece. 

Chose any variety of colors, stroke width, or even text to share your creativity with the world! If you come across a piece of graffiti you particularly like, feel free to give it an upvote!

How it works

Graffiti uses your GPS location as well as magnetic heading and accelerometer data to accurately place graffiti in your environment. When another user comes across your artwork, it appears on their screen just as you left it. Graffiti also uses your camera so that your virtual ""tag"" is placed in context and enhances reality. (aka this is an AR app)

Nerd talk

Cocoapods: jot by IFTTT, Colorslider, SnapKit, and Toast-Swift

Libraries: CoreMotion, CoreLocation, AV Foundation (<- this thing sucks)

Server done in flask and sqlite

why did you apply for the best domain name?

Harambe and Me
",,https://github.com/sayalvarun/graffiti,,"Best Domain Name Registered in PennApps XIV, Best User Experience, Best Use of VR/AR for Content Discovery",vr/ar,LinkCable,GitsAndShiggles,VarunForTheHill
Secure Galaxy,http://pennapps-xiv.devpost.com/submissions/56738-secure-galaxy,"Inspiration

Computer security has been in the news for many reasons lately.  In today's world, numerous security tactics exist, but hackers are often finding ways around them.  Companies for these reasons are going to great lengths to protect their user's data, but is it enough.  

What it does

Secure Galaxy is a tool that allows users to protect their files on the cloud from hackers and cloud storage companies themselves.  By linking together numerous cloud storage accounts, it creates a secure ""galaxy"" in the sky with unique and clever security properties.  The galaxy is meant to resemble the structure of the Google File System.  

How I built it

Secure Galaxy uses three main algorithms to secure data.  First, the file contents are scrambled using a pseudo-random number generator to generate indices that swap characters (similar to the modern Fisher-Yates Shuffle Algorithm).  Storing the seed used to initialize the generator allows the results to be reproducible/reversible.  Next, the data is split evenly, and then each piece is encrypted with EAS authentication encryption.  Lastly, each piece is sent to a Dropbox account (typically a lot of accounts).  This scheme means that a breach on any Dropbox account will reveal at most 1/n part of the file, and that part is scrambled with no information on where each byte goes.  Tampering is detected by the authentication encryption, and data repairs occur through stored parity data similar to RAID.  Encryption keys and metadata are stored locally and encrypted on a Mongo database.  

Challenges I ran into

Making sure everything is synced was tricky.  Byte data was hard to debug at times, and managing multiple cloud accounts and testing various types of data corruption was difficult.  Finding the proper libraries to use was also time consuming and cumbersome.  

Accomplishments that I'm proud of

Data can be fully stored and retrieved successfully.  I'm glad to be finished on time.  

What I learned

This was a great experience to apply algorithms that are common in computer science.  We learned a lot about data storage and how encryption works.  

What's next for Secure Galaxy

Hopefully a better user interface.  
",,,,Best Use of MongoDB,cybersecurity,smetaxas
InTrack,http://pennapps-xiv.devpost.com/submissions/56741-intrack,"Every minute, thousands of patients worldwide are administered an insulin dose. Unlike syringes, pills and IV`s, which are immediately discarded , the insulin pen is reused multiple times for the same patient. 

The situation right now is as follows: At the patients dosage time, the nurse goes to the hallway medicine cart, picks out the patients insulin pen by matching his name on a large sheet with the number on the pen, administers the insulin, and returns the pen back to the cart.

Very often, between going to and fro to the patient, the nurse gets distracted with other work and misplaces the insulin pen. Sometimes the small font and large lists confuse the nurse and she mixes up the pens and users. 

Mixups in insulin pens can lead to cross contamination causing Hep-B , HIV etc. 

In-Track is a carrier for insulin pens, that takes advantage of the already color coded bar codes to track, monitor and log insulin pen usage in real time and notify the nurse and the management via text and emails.

Intrack can :


Show the nurse the corresponding patient/pen color via colored LEDs
Log pen usage on a central server
Identify a lost pen and notify the nurse about which pen is missing via texts, emails and vibrations.
Hospitals can use the data in this system on their own webpages. 


Intrack uses:


A particle photon
Google sheets
RGB sensors

",,https://github.com/cheragnb/InTrack,,"Best Public Safety or Video Processing App, Lutron IoT Prize, Most Entrepreneurial Hack - Blackstone",hardware,cheragnb
cityscapes,http://pennapps-xiv.devpost.com/submissions/56742-cityscapes,"Inspiration

We were inspired by the field of proceduralism and its use in places like Pixar's Cars (procedural city based on London). In addition, we had an interest in graphics, and wished to play around with VR. There was a SIGGRAPH paper that suggested a psuedo infinite city that would use a constant amount of memory, so that was our challenge in creating a mobile app that was compatible with VR. 

What it does

Everything in the project was generated procedurally, there was no hand modelling or texturing. A camera drifts around an infinite pseudo-randomly generated city (both the building shapes and the building textures are random).

How we built it

Procedural Texture
At runtime, we generate a random texture of size 512x512, partitioned into windows. The windows have a random overall grayscale value to represent the shift in light for the building, and the bottom of the windows have noise to represent activity that you may see from a window. Each face of a window has randomized subset of this single texture, which we applied with UV mapping.

Procedural Geometry
Geometry is created as a rectangular building. Each building is a group of rectangular prisms that have the same local transformations, with randomized offsets to show variation in the geometry. These buildings are created at runtime.

Graphics and CPU Optimization
We attempted to use a technique called frustum filling. We don't want to create geometry that we know will not be in the viewport. Because of errors with the near clipping plane, we decided to draw buildings that are within a certain range of the camera, and offset the entire city plan to be in front of the camera. We optimized the CPU usage and memory usage by deleting buildings that are no longer in the viewport, and recreating them when they are in the viewport.

Procedural City Layout
Geometry is the same with every generation of the building. We do this with Psuedo Random Number Generation and Hashing based on the location of the building on the grid, so though we recreate/create buildings that are new in the viewport of the camera, it will always be the same, so technically, you can ""return"" to a particular building!

VR Interaction

Challenges we ran into

The first challenge we ever ran into was hardware limitations - using Unity for the project allowed us to rapidly prototype, test, and deploy on both iOS and Android devices, however, we ran into a glaring bug involving the procedural texture. The texture was created at runtime, but would simply not be mapped onto any mesh. In fact, even setting the texture of the geometries to a solid color algorithmically did not work. We ended up creating a material from the generated texture, and using sampled regions of this texture for each building.

Once we got the texture to render, we ran into issues because the texture was expensive to render. The textures would not render completely, which made the viewing experience subpar. We needed to be really efficient because we needed to rerender every time there was a shift in view. We did a lot of things to make sure that rendering in realtime was as efficient as possible. We slowed down the flythrough so that textures from far away could render while in the background. We also turned off shadows, lighting, anti-aliasing and other unneeded effects, and used a flat texture for the buildings.

For the frustum filling, we successfully created and removed buildings based on the location and if it was inside the camera view. It was probably the most efficient way to place and create geometry, since there was not a single geometry outside of the camera view, but it was not a good viewing experience since the near clipping plane would cut off geometries suddenly if they were too close to the camera, so we did a very hacky way of rendering only visible geometry. We rendered geometry that was only within a certain distance, which created a circle of buildings around the camera. We then moved it forward so that most of the geometry would be in front of the camera.

Accomplishments that we're proud of

Our hacky way of fixing our camera viewport problem, everything is procedural (all code, no modeling!),  the aesthetic and pleasant viewing experience, our first multiplatform app, our first graphics related app, our first VR app

What we learned

You don't have to implement things exactly as a paper or other people suggest (especially at a hackathon!)

What's next for cityscapes

More building geometries, twinkling stars using sprites, details such as streets, lights, and cars, STABLE BUILDS :(, maybe changing the windows from a texture to planes that can have their own colors (can simulate the animation of lights!)
",https://www.youtube.com/watch?edit=vd&v=gT-Rpl18MXI,https://github.com/emily-vo/cityscapes,,"Best User Experience, Best Use of VR/AR for Content Discovery",vr/ar,ascn,emvo
TrolleyStory,http://pennapps-xiv.devpost.com/submissions/56744-trolleystory,"Inspiration

Finding specific behavioral biases in players solving by solving the trolley problem.

What it does

Generates a procedural chain of trolley problem decisions, while saving the player choices and the levels created. This data can be later mined to find conclusions about behavioral biases.

How we built it

We built a client-server architecture with Unity 3D as a multi-platform client and a python/PHP backend with MongoDB. The server generates a procedural track definition, which the client consumes and visualizes to the player. 

The player choices are then uploaded to the server, where they can be interpreted afterwards.

Challenges we ran into


The idea itself, trying to prevent unnecessary controversy in its definition.
The procedural track generation itself.
The way to portray the victims, which is not completely clear right now, specially in small devices.
Setting up the backend, we ran into some minor problems.
Visually we had to simplify everything to submit within the deadline. Various elements can be improved.
Performance wise, there are a lot of things that can be optimized in the future.


Accomplishments that we're proud of


We started implementing the game on Saturday at 4pm and still managed to present.
Integration between client/server was very straightforward and well thought out.
Possibly scalable, since it runs over MongoDB.
Aesthetically cohesive and multi-platform.
Procedural generation of splines works smoothly.


What we learned


Coffee is king.
Sometimes it is best to wait for an idea than to start coding impulsively. Brainstorming is key.
MongoLab was not used before by any member.


What's next for TrolleyStory


Doing the data mining itself over the collected data.
Find if there are any statistically significant racial/gender biases, and expose all the data through a webpage or an API.
Build a WebGL version and test it over large groups.
Submit it to the app stores.
Some sleep for the team.

",https://www.youtube.com/watch?v=fIw1DPnG7dw,https://bitbucket.org/mmerchante/pennapps14-trolley,,"Best Use of MongoDB, Best User Experience",social + civic hacking,mmerchante,gmontamat
Financial Kideracy,http://pennapps-xiv.devpost.com/submissions/56745-financial-kideracy,"Inspiration -  Good financial management is a crucial skill, but many children have few if any opportunities to learn about how money works and how to use it well. Our webapp allows kids as young as eight years old to see the dips and spikes in their spending patterns, be notified of spending trends, track expected dates for financial achievements, and even experient in the stock market in a risk-free way. We built it using the CapitalOne and BlackRock APIs. Some of our challenges included CSS and graph production, but we're proud to say the app looks great and can be easily used and understood. We learned that it's often best to pivot in favor of efficiency when things are progressing slowly. In the future, we plan to smooth out the input and feedback process. 
",,http://www.financialkideracy.me,,"Best Financial Hack Using the Aladdin API, Best Use of the Capital One API - Nessie, Best Domain Name Registered in PennApps XIV, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Use of Data Visualization",education,sarastanway,bigolu96,ipat81
TWOFOLD,http://pennapps-xiv.devpost.com/submissions/56748-twofold,"Inspiration

Relevant related news stories can be difficult to obtain on the average news website. We wanted a better way to visualize the world news and how each story was related to the others through time.

What it does

Our application shows suggested news related news stories in time to the one the user is looking at. It displays a short blurb of each article, a main article picture and visually compares other news stories in time to the one the user is looking at.

How we built it

We built it using d3 java, html, css3 and processing.

Challenges we ran into

None of our team members have a background in coding, and we had to use d3 to accomplish our goal, which took a real concerted effort to learn something new.

Accomplishments that we're proud of

The first moment our application returned a result we knew it had all been time well spent. There were trials and we moved through them, we didn't let setbacks or a previous lack of knowledge prevent us from completing out dream. 

What we learned

Learning to try and try again, learning is the persistence of a dream, and we realized that dream this weekend. We came in with an idea and no real idea how to make it, and together we figured worked until we got it.

What's next for TWOFOLD

We are all in the same program at school, and as of now would like to continue working on the product.
",,,,Best Use of Data Visualization,social + civic hacking,dsipz,McGaat,nayyarranjan,KatherineLH
Finalytics,http://pennapps-xiv.devpost.com/submissions/56749-finalytics,"What it does

Finalytics is an all in one dashboard for financial portfolio visualization, analytics, and optimization. The platform makes it easy for anybody to see quantifiable data and how it impacts their investment decisions. 

How we built it

We built up Finalytics with a Python / Flask backend and React in the frontend. We were able to set up the visualizations by querying from multiple finance related APIs such as Yahoo Finance, NY Times, and Alladin API by BlackRock. The data and calculations given here were then put up with display on the dashboard for easy access and use in future insights.

Challenges we ran into

This was our first time using React. As such, we found it difficult to get comfortable with the framework. Also a multitude of issues dealing with Flask and our server. However, we made big progress by talking to the Hacker Gurus and Sponsors who helped us solve our issues.

Accomplishments that we're proud of

We built something cool that was finance-related and allowed users to view historical financial data in a new light (e.g. expected return on investment).

What we learned

When you graph a financial portfolio - it looks very cool. Also very fun to see how investing in a certain set of stocks in the year 2004 and the risk and return factor that was at play from the numbers provided by our API calls.

What's next for Finalytics


Develop a more intuitive interface for the information we have integrated into our dashboard.
Optimize the dashboard so that it may load faster.
Integrate more of the Alladin API for financial calculations.

",,https://github.com/WilliamY97/Finalytics,,"Best Financial Hack Using the Aladdin API, Best Use of Linode Services, Best Domain Name Registered in PennApps XIV, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App, Best Use of Data Visualization",education,WillYApps,tylerjxzhang,jas32096
FitMe,http://pennapps-xiv.devpost.com/submissions/56751-fitme,"Inspiration

The difficulty in deciding outfits when shopping—how do I know if something matches what I already currently own?

What it does

Scans a QR code linked to the clothing item, and pulls up the image on the app. The user is free to scan multiple images and mix and match clothing items.

How we built it

In the eve of the second to last day, we learned that our original idea was taken, won, and already a functioning startup in the UPenn community (XpressCart). We formulated a version of this for clothes (with/the ability to view custom outfits).

Challenges we ran into

The login kept crashing. We also faced many memory issues which would cause the application to crash.

Accomplishments that we're proud of:

We have drastically increased the efficiency of common digital transactions by reducing the amount of clicks from 5 to 1 (in order to perform a transaction).

What we learned

This was more of an application after learning experience (except for the newbie on our team). Basically everyone had some general idea of what to do; and if we didn't know, we knew exactly what to search for in those moments.

What's next for FitMe

More features, such as usability
",,https://github.com/SujeethJinesh/FastCart,,"Best User Experience, Most Entrepreneurial Hack - Blackstone",social + civic hacking,stephanieshi,SujeethJinesh,aayushkumar372
Eggsy,http://pennapps-xiv.devpost.com/submissions/56753-eggsy,"Inspiration

While discussing potential ideas, Robin had to leave the call because of a fire alarm in his dorm — due to burning eggs in the dorm's kitchen. We saw potential for an easier and safer way to cook eggs.

What it does

Eggsy makes cooking eggs easy. Simply place your egg in the machine, customize the settings on your phone, and get a fully-cooked egg in minutes. Eggsy is a great, healthy, quick food option that you can cook from anywhere!

How we built it

The egg cracking and cooking are handled by a EZCracker egg cracker and hot plate, respectively. Servo motors control these devices and manage the movement of the egg within the machine. The servos are controlled by a Sparkfun Redboard, which is connected to a Raspberry Pi 3 running the back-end server. This server connects to the iOS app and web interface.

Challenges we ran into

One of the most difficult challenges was managing all the resources that we needed in order to build the project. This included gaining access to a 3D printer, finding a reliable way to apply a force to crack an egg, and the tools to put it all together. Despite these issues, we are happy with what we were able to hack together in such a short period time with limited resources!

Accomplishments that we're proud of

Creating a robust interface between hardware and software. We wanted the user to have multiple ways to interact with the device (the app, voice (Siri), quick actions, the web app) and the hardware to work reliably no matter how the user prefers to interact. We are proud of our ability to take a challenging project head-on, and get as much done as we possibly could.

What we learned

Hardware is hard. Solid architecture is important, especially when connecting many pieces together in order to create a cohesive experience.

What's next for Eggsy

We think Eggsy has a lot of potential, and many people we have demo'd to have really liked the idea. We would like to add additional egg-cooking options, including scrambled and hardboiled eggs. While Eggsy still a prototype, it's definitely possible to build a smaller, more reliable model in the future to market to consumers.
",,,,"Lutron IoT Prize, Best Use of Rapid Prototyping, sposnored by AddLab, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App",hardware,nathangitter,acbrzows,robinonsay,pranavm13
InteGreat!,http://pennapps-xiv.devpost.com/submissions/56754-integreat,"Inspiration

Our inspiration came from the countless times we have had to manually calculate the solution to a math problem. 

What it does

InteGreat allows the user to take a picture of any mathematical formula and view the solution.

How we built it

We used optical character recognition to identify the mathematical formula and WolframAlpha's API to do the computation. We used Google Polymer for the front-end. 

Challenges we ran into

We ran into some trouble with the optical character recognition, because it was hard to determine individual symbols and characters when they were close to one another.

Accomplishments that we're proud of

We were able to successfully recognize mathematical formulas and compute the solution. We were also able to implement a user-friendly UI.

What we learned

We learned how to use Google Polymer to build a seamless UI along with Python, Flask, and OpenCV for the back-end.

What's next for InteGreat!

We plan to expand the application to add caching of previously entered formulas. 
",,https://github.com/bstadt/hitex,,Best Progressive Web App,education,ag4642
TravelAR,http://pennapps-xiv.devpost.com/submissions/56755-travelar,"Inspiration

Since the release of Pokemon Go, there has been an increase in the number of pedestrian injuries in the U.S. A major factor is that people are spending more time looking down at their phone and paying less attention to their surroundings, leading to traffic accidents. Similarly, when navigating a person must constantly look down to check his or her phone, breaking focus and increasing the chance of an accident when driving or crossing a street. TravelAR can mitigate this problem and make it easier to check the route one is walking or driving without diverting his or her eyes from their surroundings.

What it does

TravelAR aids navigation by providing a minimap of the user's current position and showing the route to take on the map. Additionally, an arrow is at the top of the user's vision, showing where to move next. These features are displayed in augmented reality, allowing the user to pay attention to their surroundings while still being able to navigate with minimal distraction.

How we built it

Technologies


Google Cardboard
Leap Motion
VuForia
Unity
Google Maps API


Our project was built using Unity, which allowed us to interface with all of the technologies listed above. We used google cardboard to display the mini-map, arrow and the menu. We sent leap motion sensor data to a server which we then queried from the phone to display where the users' hands were as well as determine which button on the menu they were touching. VuForia was used to display the hands on the screen. The Google Maps API allowed us to dynamically re-calculate the map displayed on the user's google cardboard.

Challenges we ran into


A big issue we ran into was the direction in which the arrow would display. We attempted to use gyroscope data, but it was too inaccurate for the arrow to actually seem as though it was pointing in the correct direction. To fix this we used compass data instead as well as clever use of trigonometry.
Merging of Vuforia and Google-cardboard and integration into Unity. We attempted to merge the two technologies together, but eventually decided to switch from head-tracking to compass data.


Accomplishments that we're proud of


The minimap constantly updates the user's position and is usually accurate within 10 meters. It is very similar to the minimaps that are seen in videogames and is useful for navigation without being too distracting.
The ability for us to determine, based on leap motion data which menu item the user is clicking.


What we learned


Most of the team was unfamiliar with Unity and picked up a lot of its core concepts as we built this project.
We learned how to integrate augmented reality, including Leap Motion, into Unity


What's next for TravelAR


Voice command navigation (be able to say ""nagivate to X"" and receive directions
Expand directions to other form of transportation, such as driving, biking, etc.

",,https://github.com/ZhengyiLuo/TravelAR,,"Best User Experience, Best Use of VR/AR for Content Discovery",vr/ar,TrevorOver9000,Sadat_Shaik,ZhengyiLuo,ggross343
Fulfill,http://pennapps-xiv.devpost.com/submissions/56756-fulfill,"Inspiration

As students, finding dedication can be difficult sometimes, especially for boring and trivial chores.

What it does

Fulfill is a system where your friends and family assign you tasks to perform through text to your phone number, which can be anything from homework to taking out the trash, to completing your bucket list. When you complete the task, you get points for performing tasks, based on the difficulty of task decided by the other party. If you do not perform the task in the allowed time, you lose points. This basic system of gaining and losing points is then displayed on a leaderboard, which adds a social to doing your chores. This system also has an ar aspect, which displays tasks when you use gesture control, and has text displaying the number of tasks left. 

How I built it

We used the vonage nexmo API for the text and call assignment/reminders, we used capital one for the points system, we used an ios library to handle all augmented reality parts of the hack, and we used pebble for gesture controls. 

What's next for Fulfill

The future for fulfill  would be to allow users to cash in their points for real life  prizes, such as gift cards which would be enabled by companies paying us to advertise a product in the form of a task, for instance burger king telling someone to go buy a burger and fries, or visit a burger king store. The company would pay us to allow this form of advertisement, and part of that money would then go towards giving the users even more incentive to perform their tasks.
",,,,"Best Use of the Capital One API - Nessie, Best Use of Linode Services, Lutron IoT Prize, Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API, Best Use of VR/AR for Content Discovery",vr/ar,shashank135sharma,DarthAnakin,meeshbhoombah,nikhilJain17
Voice Controlled Vehicle,http://pennapps-xiv.devpost.com/submissions/56757-voice-controlled-vehicle,"Inspiration

Nowadays, speech recognation has been a hot topic in technology. With higher accuracy and recognition speed, this technology could benifit people’s daily life and bring revolution to our lifestyle.
We would like to bring this technology to daily life and implement this technology to create an advanced product.

What it does

Genrally, our prototype will take user speech input and tranform into speech command. With each command, execute different task functions.

Speech recognition
Our central control unit will take user speech input and recognize it.  Then the result will forward to robot via bluetooth. For example, “Go”, “Stop” and send command to our VCV ( Voice Control Vehicle ).

Command execution
Once the VCV take input command, it will execute the task, control motor to go or stop or turns.  

Emotional reaction
Our prototype also has ability to recognize some ""emotion"" of the user input. For example, if the input command has a large volume which means user may want to execute that function in a hurry, then out VCV would react to that emotional input. For example, speed up or turn fast.

How we built it

Software


Language: C++, python, Arduino
Library: CMU Sphinx
Tool: ROS


Create ROS Node to send speech recognition result to the robot via Bluetooth. The results are generated by the CMU Sphinx library. We designed the speech dictionary to fit  our own need.   

Hardware

Mechanical
    Used SOLIDWORKS to build the robot model and manufactured by laser cut.  

Electrical
The electrical design of our VCV contains the following parts:   


Bluetooth communication with central computer
Use HM-10 Bluetooth module embedded with Arduino UNO board to communicate with central control unit
High current motor control system
Self designed dual H-bridge circuit to control high power DC motor with max 6 amp current limit.
User input voice control
Using op-amp to amplify user input voice level.
Emotional and volume detect circuit
Using 4 electrical microphones to detect user input volume and emotional on VCV.


Challenges we ran into

We have ran into serval challenges and problems while we building our prototype:  

Speech recognition delay
Natural language recognition using Google Cloud Platform takes long time for processing which may also cause delay for VCV control in real time  

Voice filter circuit
There are 4 electrical microphone on out prototype VCV. Since the microphone we got is too basic, designing a proper voice input filter circuit and amplifier is a big challenge.  

Distance recognition plan drop
The initial plan for the on VCV microphones is to also detect distance when user send commands. However, because of the voice filter circuit design challenge, we can not have a very filtered voice signal. In order to finish out plan, we have to give up on detecting distance using voice input.  

Accomplishments that we're proud of

We have a great synthesis of software and hardware. We designed, manufactured our robot,  soldered and optimized the electric ciruit and incorporate the state of the art voice recognition technique. We are proud of our robot and embraced the diversity of our team and truely believed together we are stronger.   

What we learned


Use google cloud platform API and CMU Sphinx to recognize speech
Configure the bluetooth and read/write of serial port
Familiarized ourselves with Arduino
Filter and amplify voice signal


What's next for Voice Controlled Vehicle

Regarding to our challenges and problem we encounter, the following points are the next step for out VCV:  


Improve microphone input control
Switch new microphone device
Design new voice filter circuit and amplify circuit which would accomplish a good noise filter and have a larger input voice command signal.
Improve Speech Recognition Detection
Train new module for speech input recognition
Improve device adaptive to different language
Train new module for different language and accent so that more people would be able to use out device rather than just saying in english.

",https://youtu.be/fKbNj95dJjw,,,"Lutron IoT Prize, Best Use of Rapid Prototyping, sposnored by AddLab",hardware,xiaozhuo,xiaojuns,yichenglin11
Remote Gesture Control,http://pennapps-xiv.devpost.com/submissions/56758-remote-gesture-control,"Everything from televisions to hand-held devices can be controlled hands-free with remote gesture controls.  The inexpensive solution presented here communicates hand gestures over wifi using two ESP8266 modules.  The wrist-wearable host reads acceleration and velocity data from a 10 DOF IMU, creates its own wireless access point, and publishes processed gestures.  The second ESP8266 acts as a client that listens and responds to the host (demo: LED).

This was my team's first foray into wifi, IoT, and IMU's.  The barrier to entry was encouragingly lower than expected, and version 2.0 will be smaller, faster, and more user-friendly.
",,,,Lutron IoT Prize,hardware,8bitch
Trimaran (because 3 things on waves),http://pennapps-xiv.devpost.com/submissions/56761-trimaran-because-3-things-on-waves,"Inspiration

I read a paper on an app named fingerIO that would use active Sonar and two microphones to trilaterate(length based triangulation) to map where a hand moved. I thought that if you just had the source attempting to identify itself you could take it a step further.

What it does

It will track in 3D space a phone emitting a designed series of chirps 13 times second. These chirps are inaudible to humans.

How we built it

We used 3 laptops and and IPhone. We put the coordinates of the laptops and the phones starting position and then began playing the chips at regular intervals. we used this to calculate how far the phone was from each laptop, and then trilaterate the position. We would then plot this in 3D in matplotlib.

Challenges we ran into

The clock speed of each of the computers is slightly different. Because sound travels at 340 meters per second a drift of less than milliseconds would make it impossible to track. We ended up hard coding in a 0.0000044 second adjusted period of chirps to compensate for this.

Accomplishments that we're proud of

That it actually worked! Also that we overcame so many obstacles to make something that has never been made before.

What we learned

We learned a lot about how sonar systems are designed and how to cross-correlate input signals containing random white noise with known signals. We also learned how to use many of the elements in scipy like fourier transforms, frequency modulated chirps, and efficient array operations. 

What's next for Trimaran

I would like to use the complex portion of the fourier transform to identify the phase offset and get distance readings more accurate than even the 96000 Hz sound input rate from our microphones could find. Also, it would be cool to add this to a VR headset like google glass so you could move around in the VR space instead of just moving your head to look around.
",https://www.youtube.com/watch?v=VM-UtHtqS8s&feature=youtu.be,,,"Best Use of Data Visualization, Best Use of VR/AR for Content Discovery",hardware,tyleraltenhofen,htan295
SafePath,http://pennapps-xiv.devpost.com/submissions/56762-safepath,"Inspiration

Our inspiration to create this webapp was to keep people safe. We both have had experiances where we have had friends walk home in unsafe situations. The goal of our app is to help prevent thoes situations from happening.

What it does

SafePath allows you to see your the path you are planning on taking and what crimes have occured near and on that path. Alowing you to avoid crime dense areas.

How we built it

We built it using Google's Polymer, running on a node.js server on linode. We used The city of Philledelphias Crime API to get all criminal homocides, rapes, agrivated assualts with a firearm, and robberies with a firearm in the city of Philledelphia. We then plotted this along the route you are planning to take.

Challenges we ran into

We were both new to Polymer so it drasticall slowed down development and restriced us from all of our ideas we orignally had.

Accomplishments that we're proud of

A fully implemented and deployed webapp using multipul API's

What we learned

WE leared a lot about frontend apps, and how to call and serve API requests.

What's next for SafePath

SafePath's next step is to include a toggle for different types of crimes, allowing you to display any crimes you want to see and suppress the other ones. We want to make a more elegant way to display the info such as a heat map. And a way to reroute your path automatically away from crime dense areas. After all of this we can easily use other cities API's to expand all over the country.
",, http://45.33.77.213:8080/,,"Best Use of Linode Services, Best Public Safety or Video Processing App, Best Progressive Web App, Best Use of Data Visualization",social + civic hacking,Andrew-Casner,samsoares
RotMeNot,http://pennapps-xiv.devpost.com/submissions/56763-rotmenot,"Inspiration

When was the last time you cleaned your fridge and didn’t find half of a sandwich, few vegetables/fruits that had been rotting there for two weeks? Or having to throw away the excess food cooked as a result of overestimation? 
United States sees a total wastage of about 35 MILLION TONS of food each year worth $165 BILLION, and for an average American family, that approximately evaluates to $2,200 per household per annum. An incredible 40 percent of the available food supply in the U.S. is never consumed, meanwhile, millions of Americans are suffering due to lack of food. 
Landfills due to food wasted are increasing and contributing to excessive generation of greenhouse gas along with land and water pollution. 
This is not just a problem faced in the United States, but is a Global issue. 
On January 13, 2016, France became the first country in the world to pass a law requiring supermarkets to donate food that is approaching its expiration date instead of throwing it away.

What it does

“RotMeNot” is a software solution which runs seamlessly on all platforms, that provides the user a real time inventory status of all the food items previously purchased and available in his/her personal space, by incorporating automatic update of inventory list by capturing the image of the shopping receipt, extracting information like the items purchased, quantity, cost, category and its expiration date. 
Very often we find ourselves in the confusion of having to decide what to cook with the ingredients available to us.
Our system uses the information in the inventory to then provide alerts of food items with approaching expiration dates and provide an option to either consume the food item by suggesting recipes or donate it.
As the application is already aware of what and how much the user has consumed throughout the day, it informs the user of the dietary supplements consumed and suggests recipes or food items with low/high percentage of remaining necessary nutrition for a balanced diet.
“RotMeNot” also provides a platform to plan your shopping using information about the availability of the items in your fridge and the quantity to be bought. 
Each user is offered the functionality of segregating his groceries into private and public. Food listed under public is up for grabs and available in a list, “Food near me”. Selection of a particular food item leads the user to a google map with the location where it is available. This includes a rating system for the provider, to maintain trust and quality of products.
When the user wants to sell excess food or groceries, prices of the same is automatically suggested by the application according to the market rates at which the user purchased but the user always has the option to override this.

How I built it

The server is hosted on Digital Ocean.
The UI/UX of the application is built and tested on Ionic platform, which is built on top of Apache Cordova and Angular.js. 
Optical character recognition uses Cloud Vision API with a custom parser to extract food items, costs and purchase dates. 
Locations of nearby food is populated on maps using the Google Maps API. 

Challenges I ran into

The team had not previously worked with AngularJS, but worked swiftly to pick up the language and build the application within 36hrs. 
Getting accuracy of the OCR was tricky and time consuming. We started out with an accuracy of 40% and worked our way up to 80% accuracy. 

Accomplishments that I'm proud of

Getting OCR to work was tricky and time consuming.
The team managed to build an application with 7 crucial features within 36 hours.

What I learned

Setting up the Android and Ionic platform definitely builds character -- lots of syntactic differences in Mobile and Web versions even though both use HTML/CSS/JS.
Ionic is not as easy to understand and use as they claim to be.

What's next for RotMeNot

Suggest recipes according to the user’s diet plan. In app payment system between the buyer and seller
Powerful machine learning algorithms for more accurate estimation of food item.
Gamification with respect to user with least amount of food wastage.
Prescriptive analytics according to nutrition in diet.

Citations


according to a recent study by the Natural Resources Defense Council (NRDC)
http://www.cnbc.com/2015/04/22/americas-165-billion-food-waste-problem.html
http://www.dogonews.com/2016/3/7/france-becomes-the-first-country-in-the-world-to-ban-supermarket-food-waste

",,,,Most Entrepreneurial Hack - Blackstone,social + civic hacking,nswathi,abhshkdz,rswat
ShadowPost,http://pennapps-xiv.devpost.com/submissions/56764-shadowpost,"Inspiration

The digital age has ushered in the largest flood of information the world has seen since the widespread adoption of the printing press. Yet this continuous stream of content has in many ways diminished the value of our daily communications, and consequently the value of our expression. Tweets limit our ability to explore nuanced, complex ideas, Snapchats by their nature are fleeting and are prone to superficial contexts, and the age of social networks in general places a strong priority on a steady stream of messaging rather than on carefully crafted messages and ideas. We wanted to provide a service that would enable our users to simultaneously communicate with a wide audience while still maintaining the weight and meaning behind the content being sent. We wanted to create the message-in-a-bottle for the modern world.

What it does

ShadowPost is an anonymous messaging system where the encryption standard is the physicality of the real world. With ShadowPost, you can write text messages and save them in the form of ""ShadowCodes"", stylized QR codes that allow for instant audience recognition and interaction. These codes are designed to be printed and posted in any location: cafes, school bulletin boards, and so on. ShadowPost is named after the possibility of finding posts in unexpected places; a haiku under a table, a love note behind a lonely bench, a memoir hidden inside a tree trunk. As opposed to many other social networks where a vast majority of posts can be found by searches, ShadowPosts are only shared with the people that can physically access them, allowing for a unique sense of belonging and intimacy in anonymous contexts. The effort required to create and distribute ShadowPosts allows for more deliberate, meaningful messages to be sent and received by others. ShadowPosts time out at most in five weeks, though this value can be modified by the user. This allows the user to voice his or her ideas in a longer-lasting timeframe while still capturing the ephemerality of ideas and moments in life. ShadowPost also allows users to communicate in closed groups or create scavenger hunts by optionally restricting ShadowCode access with a customizable password.

How we built it

ShadowPost is a web application written in HTML, CSS, and JavaScript that saves data to mongodb using Node.js. ShadowCodes are generated with Google Charts API along with HTML Canvas support to overlay the images. The application's logic structure is written in JavaScript.

Challenges we ran into and accomplishments that we're proud of

Our most challenging struggles became our greatest accomplishments once we had surmounted them. Our team struggled with stylized QR code generation, database integration, camera support and QR scanning, as well as ensuring proper communication between the four main elements of the application: the front-end, the database, the application logic, and the QR APIs.

What we learned

ShadowPost is our team's first web project. We gained invaluable experience in database usage, mongodb API usage, web application development in the contexts of JS, HTTP, and CSS, teamwork skills, and creating a functional, unified experience from start to finish.

What's next for ShadowPost

We are planning on continuing development of ShadowPost. Our main goals are to streamline the user experience even further, as well as to enable more customization options such as jigsaw mode (ShadowCode is divided into unique jigsaw pieces to be collected and assembled) and custom color print options.

Our Team

Sebastian Bartlett: App logic, graphical design, public relations

Shixiong Jing:  Server and research, QR writing and reading

Tianke Li: Front-end design and programming, server code

Zachary Zhao: Database and core transfer functionality
",,http://sebastianbartlett.com/shadowpost,,"Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone",social + civic hacking,sebastianb42,tkl,shixiongjing
Conservar,http://pennapps-xiv.devpost.com/submissions/56765-conservar,"Inspiration

Most banks spend thousands of dollars in an effort to attract new customers into their business. But even after a successful attainment, most banks don't really have a means of 'preventing' a customer from leaving.

What it does

Conservar provides banks with the opportunity of detecting customers who might potentially be on the verge of leaving them hence allowing them to do something to entice them into staying. The detection is based off of two categories:
Transactional History
Conservar processes each customer's data and provides an aggregate of their transactional history. Accounts with an extreme drop in activity or below a certain set threshold, are flagged as a potential defector.
Characteristics Exploration
Through machine learning, Conservar is able to relate a customer's current credit score to their location in order to provide banks with predections of their the customer is most likely to stay or not. Along with this, this tool can also be used by banks to target a certain customer audience.

How I built it

We used Google's Tensorflow along with sklearn to build the brains of the system in Python. A splash of Javascript, CSS and HTML made the dashboard possible. Thanks to Capital One's API we were able to get the data format required to feed into our classifier.

Challenges I ran into

Machine learning pretty much. This was our first time doing it and we really wanted to challenge ourselves and my oh my were we challenged..

Accomplishments that I'm proud of

Getting the machine learning portion working.

What I learned

Tensorflow, SKLearn and some data analysis methods

What's next for Conservar

Improve the machine learning algorithm by not only adding more attributes, but also collecting more data in general. The end goal is to have the platform even find different potential earning means for businesses and not just banks.
",,http://tiny.cc/conservar,,"Best Use of the Capital One API - Nessie, Best Use of Linode Services, Best Use of MongoDB, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App, Best Use of Data Visualization",social + civic hacking,eshirima1,azimshaik
Space InVRders,http://pennapps-xiv.devpost.com/submissions/56766-space-invrders,"Inspiration

Wanting to build an FPS VR game.

What it does

Provides ultra fun experience to all players, taking older folks back to their childhood and showing younger ones the beauty of classic arcade types!

How we built it

Unity as the game engine
Android for the platform
socket.io for multiplayer 
c# for client side code

Challenges we ran into

We coded our own custom backend in Node.js to allow multiplayer ability in the game. It was difficult to use web sockets in the C# code to transfer game data to other players. Also, it was a challange to sync all things from player movement to shooting lazers to map data all at the same time. 

Accomplishments that we're proud of

Were able to make the game multiplayer with a custom backend

What we learned

Unity, C#

What's next for Space InVRders

Add other game modes, more kinds of ships, store highscores
",,https://github.com/AlstonLin/PennAppsXIV,,"Best Use of Linode Services, Best Use of VR/AR for Content Discovery",vr/ar,melissali343
Hobo Hockey,http://pennapps-xiv.devpost.com/submissions/56767-hobo-hockey,"Inspiration

What it does

We have a control scheme where you pump your arms to move, just like you would when you're skating, and the goal is to make a goal without having the hobos get the puck.

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Hobo Hockey
",,,,Best Use of VR/AR for Content Discovery,vr/ar,Nouser76,sambarnes
Ringman,http://pennapps-xiv.devpost.com/submissions/56768-ringman,"Inspiration

Originally intended to be a quick way to send a message to someone, but evolved into the solution to your fears of calling.

What it does

Ringman does the calling and the texting for you when you're too nervous to pick up the phone

How I built it

Nexmo API running on node.js server. 

Client side retrieves phone number and message, node.js server makes nexmo API call to perform the action and return the results to the client.

Challenges I ran into

Phone service was very low in the building, so at times I wasn't sure if it was my code not working or just the call not going through.

Getting feedback to the client about the status of their request.

Accomplishments that I'm proud of

Really easy to use and fun to look at

What I learned

How to set up node.js server and how to use nexmo API

What's next for Ringman

Deployment using AWS and a domain host
",,,,"Best Domain Name Registered in PennApps XIV, Best User Experience, Best Use of Nexmo API","",gagikm
Sigma,http://pennapps-xiv.devpost.com/submissions/56770-sigma,"Inspiration

One of our team members, Josh, was stuck in an endless cycle of League of Legends due to it's highly competitive nature. He spent hours upon hours glued to his computer screen, smiling as his rankings went up. However, Josh realized that the hours he spent improving his rankings on this game were essentially useless as they had no tangible effect on his life outside of his computer screen. This got him into thinking-- what if there was a way to actually make these hours spent on a game actually mean something... and, thus, Sigma was born.

Sigma isn't your ordinary game: it's a fast-paced and highly-competitive mobile experience that uses technologies similar to those found in League of Legends and Dota -- all while being educational. Our goal is to take the individuals' inherent competitive and easily-addictive spirit and use it to make the world a smarter place.

What it does

Sigma is a multiplayer experience that matches players with those of similar levels in math battles. Players are to answer nine questions, with fifteen seconds for each math word problem, both correctly and before the opposing player. At the end of the match, we use the ""elo"" algorithm (see link) to rank the players. If one wins, they can expect their rating to go up, however, if one loses, they can expect their rating to go down (sometimes by a very significant factor). By doing this, not only do we create a very competitive atmosphere, but we also sharpen our users' math skills.

How we built it

For this project, we built our own stack to handle everything! On the back-end side, we used Python's Flask framework to build our REST API. In our API, we built all of the basic app functionalities like creating accounts, logging in, and generating authentication tokens. Then, we went one step further and did some pretty cool stuff like implement an elo rankings system that was able to rank users to other users based on their skillset; create a question generate engine that could create word distinct word problems to offer some creativity in the app; and, even handle multiplayer interaction. We hosted our REST API on Linode, and made GET/POST requests to it from our front-end. 

On the front-end side, we implemented Facebook's React Native framework to develop both an Android and iOS app. We used generic components that would work on both the iOS and Android side to give both sets of users very similar experiences. In our app, we made calls to our REST API to implement the functionality.

Challenges we ran into

The biggest challenges we ran into was multiplayer functionality. Our original idea to implement this was opening a web socket between our server and the devices in the match, but with our server and front-end setup, it wasn't very possible to complete this efficiently. As a result, after much brainstorming, we saw that the best way to implement multiplayer was to use the API and make a series of POST and GET requests. Even after getting this idea, we had to account for challenges like race conditions, and network connectivity issues. We overcame many of these issues through trial and error.

Accomplishments that we are proud of

By the end of the hackathon, we are proud of building a full-stack application. Rather than taking the easy way out and using a third-party API to handle our application, we built our own API, and then we used a brand-new framework to build an app for the two biggest mobile platforms concurrently. 

What we learned

In addition to learning more about frameworks like Flask and React Native, we also learned about system design. We saw the issues when more users try to use it and how to handle issues like concurrency and lag. As we were a team of four people, we also learned how to work better with a group of engineers.

What's next for Sigma

With this app, Sigma aims to go further than simply basic math questions. We see potential for it in different fields like vocabulary, science, and writing. We want to see Sigma as the app people love to play not only because of it's fun and competitive nature, but also because of it's educational impact.
",,https://github.com/mayankmmmx/ProjectSigma-Mobile,,"Best Use of Linode Services, Best Domain Name Registered in PennApps XIV, Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone",education,Mayankmmmx,JoshuaRLi,davidz-eng
TA Timer,http://pennapps-xiv.devpost.com/submissions/56771-ta-timer,"Inspiration

I'm a first year MCIT student at Penn and I just began studying computer science. I'm quite new to coding and only learned HTML, CSS, and Javascript last weekend at a workshop hosted by a student organization (the Dining Philosophers). The 'intro to web development' workshop by Horizons that I went to yesterday during PennApps was also quite helpful.

I got an idea to create this webpage after watching TAs adjust multiple windows (stop watch on google, notepad for announcements) while they were proctoring exams.

I thought it would be a good idea to have the two most essential features on a clean, simple page.

What it does

It is a timer with a text area attached to the bottom.
Nothing fancy, but it gets the job done.

How I built it

I used the website codepen to constantly refresh my code to see if it worked.
I also used the inspect feature of Google Chrome to check the console to print out values for debugging.
Stack overflow and w3schools were extremely helpful.

Challenges I ran into

Syntax for Javascript was very confusing. I did not know that adding numbers represented as strings would concatenate them instead of mathematically adding them. Finding the cause of the problem and creating a solution took up a couple of hours.
Basically everything was a challenge for me this time, but I'm glad to have done it.

Accomplishments that I'm proud of

I made a working webpage and learned three different languages! (do HTML and CSS count as languages?)
I've actually had this idea to create TA Timer for several months. I even bought a domain tatimer.com early this year.
Sadly I didn't have the willpower to study and get down to making the page.
I'm glad to have gotten an item off my to-do list.
And I'll also probably use this in the future when I'm TAing for a class. 

What I learned

Coding is fun.
Time management seems important. This project took much longer than I anticipated. 
I should print stuff out while I'm coding so that I don't have a million things to look over later.

What's next for TA Timer

Since I have the domain and the files, I should now try to find a way to host it on a server. 
I want to get this website up and running.

Feature-wise, I want to add a script so that people can adjust their fonts, colors, and sizes in the text area.
I'll even add comic-sans as an option for TAs who wish to torture their class.

Making the elements drag-and-droppable would be also quite nice for customization.
",,https://kimyoonduk.github.io/penn_apps_xiv/,,"Best User Experience, Best Progressive Web App",education,kimyoonduk
Notebook,http://pennapps-xiv.devpost.com/submissions/56772-notebook,"Inspiration

As engineering students, our notes are often filled with intricate equations and diagrams. For the former, using a markdown/latex note-taking tool (i.e. Atom) is much faster than simple pen and paper. Unfortunately, this solution completely falls short in drawing any sort of diagram. We were inspired to fix this problem by combining the strengths of handwriting and electronic note-taking.

What it does

Notebook let's you handwrite diagrams and seamlessly integrate them with your electronic notes. All you have to do is take a picture.

How we built it

The core of Notebook depends upon our Atom package and iPhone app. The Atom package spins up a node server running socket.io on localhost, and generates a qr code to initiate a handshake with the iPhone app. Upon scanning, the app will be connected via socket to your notes! From here, Notebook renders your Markdown/latex notes live on your phone and defines a custom syntax for uploading your own handwritten notes via photo.

Challenges we ran into

Data compression to minimize the transfer rate of photos to produce a truly seamless note-taking experience.
Centering the QR code in Atom's dropdown modal :(

Accomplishments that we're proud of

It works! And it's wicked fast.

What we learned

Learned the Atom API and package creation from scratch.

What's next for Notebook

VIdeo!
",,https://github.com/jasonscharff/PennAppsXIV,,"",education,ezhao,jasonscharff,andrewaday
Autonomous Suitcase,http://pennapps-xiv.devpost.com/submissions/56773-autonomous-suitcase,"Inspiration

As lazy people we don't like to do many things, especially pulling suitcases. To combat this, we have created Autonomous Suitcase!

What it does

Autonomous Suitcase is a machine that tracks you and follows you around wherever you go. It is designed to fit a suitcase or carry-on luggage, allowing you to move around places like the airport hands-free.

How we built it

We first CAD-ed the frame that the suitcase would be resting on. Afterwards, we explored existing Kinect motion tracking code and combined our findings with ROS python packages so the wheels mounted on the frame would move in the same direction and angular velocity as the person being tracked.

Challenges we ran into

The speed controllers blew up, so we had to find new ESCs for our project to work. We also had some trouble in finding space for all of the electronics.

Accomplishments that we're proud of

On top of using CAD and a laser cutter to create the main frame of Autonomous Suitcase, we are most proud of integrating all of the software into our project.

What we learned

We learned how to implement ROS to change the tracking information from the Kinect to usable data for our motors. In addition, we also derived kinematic equations for the suitcase.

What's next for Autonomous Suitcase

In the future, airports will be equipped with Autonomous Suitcase. People will be able to put their luggage onto it and walk around like their luggage is a pet. 
",,https://github.com/jjyuan/suitcase,,"Best Use of Rapid Prototyping, sposnored by AddLab",hardware,moylau27,nikunj101,owenli10
Multi Players in Kinect World,http://pennapps-xiv.devpost.com/submissions/56774-multi-players-in-kinect-world,"Inspiration

We wanted to experience/learn with Microsoft Kinect for Windows V2, skeletal tracking, VR, and Unity

What it does

Allow multiple players with Kinects to interact/see each other in a virtual environment.

How we built it

Countless hours of looking up documentations and solving various problems due to deprecated features.

Challenges we ran into

We never got  a chance to experience with the Oculus due to miscommunication with organizer :(  .Only works on PC & Windows. Deprecated libraries. Microsoft drops support for Kinect V2 for Windows and it hasn't been updated since 2014 (the release of Windows 8). Synchronization among Kinect is super complicated and unsupported so we spent many hours messing with standard unity networking utility, websocket, NodeJS, and socket.io, etc

Accomplishments that we're proud of

Partially working synchronizing movement between Kinects

What we learned

Despite running into numerous problems due to deprecated libraries and problems due to unsupported Kinect, we thoroughly enjoyed hacking the Kinect and solving these challenges.We picked up various new technologies during this weekend and got a good glimse into VR as well as game development

What's next for Multi Players in Kinect World

Enhance synchronization and animation in real time so multiple players have the most responsive experience!
",,https://github.com/vanstorm9/kinect-unity-experiments,,"",vr/ar,megatran,Antlowhur
Squirrel,http://pennapps-xiv.devpost.com/submissions/56775-squirrel,"We made a robot.
",,,,"",hardware,jackcook
Neighbors,http://pennapps-xiv.devpost.com/submissions/56776-neighbors,"Inspiration

Our busy lives have modified our view of “tasks”
-It has become difficult for people to allocate time to do the simplest of tasks such as mowing the lawn, buying groceries etc
-Jobs like handyman, plumbers that were once devalued are now rising in value
-60% of the world population still does not have access to Internet service. As a result, they are deprived of the various benefits that comes with the internet.

What it does

Neighbors is a android based app that connects people with internet connections who have job offer to people who are qualified for the job (regardless of whether they have access to internet or not). 

How we built it

We used Android Studio to create our app. 
Nexmo's API, a Vonage company, was used to send job offers in the form of text message to qualified people and to get their response. This response was recorded and displayed in the profile of the user who posted the advertisement for the job.

Challenges we ran into

None of our team members had any experience with FireBase and this was our first time using Nexmo's API. Coming up with an idea that had real world application was a real challenge. Apart from that we had some other challenges related to coding that were solved by watching youtube video or referring to company representatives in PennApps XIV.

Accomplishments that we're proud of


Being able to finish major portion of our app
Approximately only 6 hours of sleep in last 48 hours

",,https://github.com/tushar5/fireapp,,Best Use of Nexmo API,"",chetanparakh,shl1208,tuskythegreat
Dojo,http://pennapps-xiv.devpost.com/submissions/56777-dojo,"Learning a new language doesn't happen inside an app or in front of a screen. It happens in real life.

So we created Dojo, an immersive and interactive language learning platform, powered by artificial intelligence, that supports up to 10 different languages.

Our lessons are practical. They're personalized to your everyday life, encouraging you to make connections with the world around you.

Go on a scavenger hunt and take photos of objects that match the given hint to unlock the next level. Open the visual dictionary and snap a photo of something to learn how to describe it in the language you're learning.
",,,,"Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Use of VR/AR for Content Discovery",education,karinamio,sitefeng,kevinshi,rockyhchen
EduSmart,http://pennapps-xiv.devpost.com/submissions/56778-edusmart,"Inspiration

E-Learning is widely becoming popular with tonnes of online courses such as Coursera and EdX. However since these courses are for a general wide audience, they are sometimes harder for few people to understand and follow. We wanted to build something to make this a closed loop system by monitoring the student's response or how well he/she is able to follow the material. We incorporate this feedback to dynamically change content according to the students ease of understanding.

Also, since majority of the time is spent on computers, it is increasingly easy to develop health problems such as headaches, eye-strains and other issues. We also wanted to keep a watch on such conditions and inform students and warn them accordingly.

What it does

The system monitors the facial expressions of the user via the laptop's web cam and analyzes the emotion of the user from it. Based on the emotion, it then identifies if the person is able to follow the course content well, if its interesting etc. This feedback is then used to decide which course topics will be elaborated in more detail or skipped in the upcoming content. If a student seems to be stuck on some material, a friendly pop up is shown to ask if he/she needs help and accordingly some hints and more detailed explanation is loaded. We therefore provide content that's personalized and paced at the right rate according to the learning curve of the user. This can also be used in other domains to obtain feedback regarding users response to web content.
Also, the blinking rate of the user is monitored to figure out eye-strains and sleepiness. The algorithm also identifies yawning and based on a threshold applied to a combination of such parameters, it determines if the user is tired, sleep deprived or is stressed and accordingly suggests to take a break or go rest. Long periods of working in front of the computer are also noted and warnings are generated to prompt users to take breaks. The system tries to enforce the highly efficient and popular ""Pomodoro Technique"" of interleaving certain minutes of work and breaks to ensure maximum efficiency.

How we built it

The whole system is build in two parts. There is a machine learning algorithm which performs face detection, recognizes facial features and emotions from them as well as identifying blinking and other parameters mentioned above. There is a cloud server and backend which takes the feedback from the algorithm and also based on the amount of time the user spends on different contents - identifies whether the user is comfortable with the learning pace or not and accordingly changes the content dynamically. We use Google Text to Speech for generating loud alerts to wake people up in the middle of study sessions and also to provide interactive feedback and warnings.

Challenges we ran into

Setting up and learning OpenCV was challenging. The machine learning algorithm for detecting facial expressions was hard and not up to the required level of accuracy. We had a tough time extracting the required features and processing them. System integration was also challenging. Setting up dynamic changing of contents and synchronizing them with the feedback obtained took a while.

Accomplishments that we're proud of

Putting together a project with new stuff like OpenCV and machine learning algorithms which we hadn't dealt with before was extremely satisfying.

What we learned

We were new to Face Recognition technology, specially OpenCV and learnt a lot about it.

What's next for EduSmart

Incorporate some more detailed analytics such as stress levels. We would also like to make this generic to obtain emotional feedback for all websites the user visits and generate a real-time stress level graph. This would pave the way to personalized web content which is more relevant and useful for the user. 
",,https://github.com/arsheth/EduSmart,,"Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App",education,Arsheth25,Rishikanth,zbauer
ROSDev,http://pennapps-xiv.devpost.com/submissions/56779-rosdev,"Inspiration

Learn ROS and build something that is going to be useful for me in the future. 

What it does

Provides ROS support for Slackware Linux and ROS-based control mechanism for communication with an mbed microcontroller form a Linux machine. I need it as extension to a previous project of mine - a Walking Quadcopter robot, in order to easily control it from my laptop. 

How I built it

Using ROS-mbed libraries, cross-compilation, ROS subscribers and publishers.

Challenges I ran into

Deciding on a project

Accomplishments that I'm proud of

Contributed to an open-source project (ROS)

What I learned

ROS basics

What's next for ROSDev

Including a Pixhawk microcontroller in the communication graph and using XBee and MAVLin for wireless communication.

NOTE:

Only part of the code in the github link was developed during the hackathon
",,https://github.com/nikonikolov/WalkingQuad/tree/ros,,"",hardware,nikonikolov
Project Em,http://pennapps-xiv.devpost.com/submissions/56780-project-em,"Inspiration

We are all software/game devs excited by new and unexplored game experiences. We originally came to PennApps thinking of building an Amazon shopping experience in VR, but eventaully pivoted to Project Em - a concept we all found mroe engaging. Our swtich was motivated by the same force that is driving us to create and improve Project Em - the desire to venture into unexplored territory, and combine technologies not often used together. 

What it does

Project Em is a puzzle exploration game driven by Amazon's Alexa API - players control their character with the canonical keyboard and mouse controls, but cannot accomplish anything relevant in the game without talking to a mysterious, unknown benefactor who calls out at the beginning of the game. 

How we built it

We used a combination of C++, Pyhon, and lots of shell scripting to create our project. The client-side game code runs on Unreal Engine 4, and is a combination of C++ classes and Blueprint (Epic's visual programming language) scripts. Those scripts and classes communicate an intermediary server running Python/Flask, which in turn communicates with the Alexa API. There were many challenges in communicating RESTfully out of a game engine (see below for more here), so the two-legged approach lent itself well to focusing on game logic as much as possible. Sacha and Akshay worked mostly on the Python, TCP socket, and REST communication platform, while Max and Trung worked mainly on the game, assets, and scripts. 

The biggest challenge we faced was networking. Unreal Engine doesn't naively support running a webserver inside a game, so we had to think outside of the box when it came to networked communication.

The first major hurdle was to find a way to communicate from Alexa to Unreal - we needed to be able to communicate back the natural language parsing abilities of the Amazon API to the game. So, we created a complex system of runnable threads and sockets inside of UE4 to pipe in data (see challenges section for more info on the difficulties here). Next, we created a corresponding client socket creation mechanism on the intermediary Python server to connect into the game engine. Finally, we created a basic registration system where game clients can register their publicly exposed IPs and Ports to Python. 

The second step was to communicate between Alexa and Python. We utilitzed Flask-Ask to abstract away most of the communication difficulties,. Next, we used VaRest, a plugin for handing JSON inside of unreal, to communicate from the game directly to Alexa. 

The third and final step was to create a compelling and visually telling narrative for the player to follow. Though we can't describe too much of that in text, we'd love you to give the game a try :)

Challenges we ran into

The challenges we ran into divided roughly into three sections:


Threading: This was an obvious problem from the start. Game engines rely on a single main ""UI"" thread to be unblocked and free to process for the entirety of the game's life-cycle. Running a socket that blocks for input is a concept in direct conflict with that idiom. So, we dove into the FSocket documentation in UE4 (which, according to Trung, hasn't been touched since Unreal Tournament 2...) - needless to say it was difficult. The end solution was a combination of both FSocket and FRunnable that could block and certain steps in the socket process without interrupting the game's main thread. Lots of stuff like this happened:

while (StopTaskCounter.GetValue() == 0)
    {
        socket->HasPendingConnection(foo);
        while (!foo && StopTaskCounter.GetValue() == 0)
        {
            Sleep(1);
            socket->HasPendingConnection(foo);
        }
        // at this point there is a client waiting
        clientSocket = socket->Accept(TEXT(""Connected to client.:""));
        if (clientSocket == NULL) continue;
        while (StopTaskCounter.GetValue() == 0)
        {
            Sleep(1);
            if (!clientSocket->HasPendingData(pendingDataSize)) continue;
            buf.Init(0, pendingDataSize);
            clientSocket->Recv(buf.GetData(), buf.Num(), bytesRead);    
            if (bytesRead < 1) {
                UE_LOG(LogTemp, Error, TEXT(""Socket did not receive enough data: %d""), bytesRead);
                return 1;
            }
            int32 command = (buf[0] - '0');
            // call custom event with number here
            alexaEvent->Broadcast(command);
            clientSocket->Close();
            break; // go back to wait state
        }
    }

Notice a few things here: we are constantly checking for a stop call from the main thread so we can terminate safely, we are sleeping to not block on Accept and Recv, and we are calling a custom event broadcast so that the actual game logic can run on the main thread when it needs to.

The second point of contention in threading was the Python server. Flask doesn't natively support any kind of global-to-request variables. So, the canonical approach of opening a socket once and sending info through it over time would not work, regardless of how hard we tried. The solution, as you can see from the above C++ snippet, was to repeatedly open and close a socket to the game on each Alexa call. This ended up causing a TON of problems in debugging (see below for difficulties there) and lost us a bit of time. 


Network Protocols: Of all things to deal with in terms of networks, we spent he largest amount of time solving the problems for which we had the least control. Two bad things happened: heroku rate limited us pretty early on with the most heavily used URLs (i.e. the Alexa responders). This prompted two possible solutions: migrate to DigitalOcean, or constantly remake Heroku dynos. We did both :). DigitalOcean proved to be more difficult than normal because the Alexa API only works with HTTPS addresses, and we didn't want to go through the hassle of using LetsEncrypt with Flask/Gunicorn/Nginx. Yikes. Switching heroku dynos it was. 


The other problem we had was with timeouts. Depending on how we scheduled socket commands relative to REST requests, we would occasionally time out on Alexa's end. This was easier to solve than the rate limiting. 


Level Design: Our levels were carefully crafted to cater to the dual player relationship. Each room and lighting balance was tailored so that the player wouldn't feel totally lost, but at the same time, would need to rely heavily on Em for guidance and path planning. 


Accomplishments that we're proud of

The single largest thing we've come together in solving has been the integration of standard web protocols into a game engine. Apart from matchmaking and data transmission between players (which are both handled internally by the engine), most HTTP based communication is undocumented or simply not implemented in engines. We are very proud of the solution we've come up with to accomplish true bidirectional communication, and can't wait to see it implemented in other projects. We see a lot of potential in other AAA games to use voice control as not only an additional input method for players, but a way to catalyze gameplay with a personal connection. 

On a more technical note, we are all so happy that...

THE DAMN SOCKETS ACTUALLY WORK YO

Future Plans

We'd hope to incorporate the toolchain we've created for Project Em as a public GItHub repo and Unreal plugin for other game devs to use. We can't wait to see what other creative minds will come up with!

Thanks

Much <3 from all of us, Sacha (CIS '17), Akshay (CGGT '17), Trung (CGGT '17), and Max (ROBO '16). Find us on github and say hello anytime. 
",https://www.youtube.com/watch?v=t-2w_q-RPyk,https://github.com/sachabest/pennapps-2016f,,"Best Amazon Alexa Hack, Best User Experience, Best Use of VR/AR for Content Discovery",vr/ar,sachabest,aksxay,trungtuanle,maxlgilbert
CSShool,http://pennapps-xiv.devpost.com/submissions/56781-csshool,"Inspiration

Every time I talk to other developers, the consensus seems to be that CSS (Cascading Style Sheets) is terrible. I even find myself forgetting certain common selectors and properties frequently - I wanted to put an end to this. 

What it does

CSSchool provides the user with various CSS challenges to help them brush up on some basics. Through a minimalist design and an interactive coding environment in the browser, users can learn CSS with fun and ease. 

How I built it

To keep with the spirit of the project, I used zero front-end frameworks. Actually, I lied - I used VanillaJS and VanillaCSS. I truly believe these powerful frameworks can outdo all others in terms of preformance, footprint, and ease of installation/setup. But don't just take my word for it, check out these informative sources: VanillaCSS, VanillaJS.

Challenges I ran into

Setting up and using the CodeMirror web text editor in conjunction with WebComponents. Some of the content on the UI is dynamically generated, which caused some problems with particular features down the line. 

Accomplishments that I'm proud of

I tried going for a simple, stylish, minimalist UI - I think I've accomplished that. 

What I learned

Sleep is something I shouldn't take for granted. 

What's next for CSShool

More challenges, more sophisticated CSS code parsing, more customization features for the UI, more 'gamification.' I heavily considered adding a point system but I felt it arbitrary for the few number of challenges there. Additionally being able to save specific challenges that you find more interesting/challenging would be useful; also I considered a leaderboard, etc. 
",,,,"Best Domain Name Registered in PennApps XIV, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App",education,VukPetrovic
Homie,http://pennapps-xiv.devpost.com/submissions/56782-homie,"Inspiration

Home buying is often a difficult process and a lack of information is one of its greatest hurdles. Home buyers are busy enough, so we wanted to make their lives a bit easier.

What it does

Homie consolidates information from many different sources into a simple interface. Users can find information both about the property and about the surrounding area, including parks, emergency services, schools, and much more.

How we built it

We used Flask and MongoDB for the back-end and Materialize for the front-end. We also focused on responsiveness on all devices.

Challenges we ran into

We gathered data from Zillow, Google Maps, and datasets provided by OpenPhilly and other organizations. This required parsing through a lot of information.

Accomplishments that we're proud of

We are proud of creating a useful project that is sleek and easy to use.

What we learned

We learned about UI/UX design as well as how to parse through large amounts of data.

What's next for Homie

We hope to continue adding data to Homie as well as to better filter this data.
",,http://homiefor.me,,"Best Domain Name Registered in PennApps XIV, Best Use of MongoDB, Best User Experience",social + civic hacking,czhao,ezwang,ctrlshiftq
At Helse,http://pennapps-xiv.devpost.com/submissions/56783-at-helse,"Inspiration

A friend who has seasonal affect disorder. 

What it does

It takes into account many different factors, including real-time weather data, to give an accurate analysis and recommended treatments/mitigation strategies.

How I built it

We wrote the backend in node.js and the frontend in ejs, a view engine that we used to inject dynamic content. The materialized css templates were used.

Challenges I ran into

Asynchronous programming is new to both of us. We used a lot of new APIs as well.

Accomplishments that I'm proud of

The frontend programming with the incorporation of material design was new and looked pretty good.

What I learned

I learned some css and some more in-depth functions of certain attributes.

What's next for At Helse

Implementation of machine learning to give more personalized predictions/recommendations.
",,https://github.com/c-er/mental-calendar-research,,"Best Use of Linode Services, Best Use of MongoDB, Best Progressive Web App",health,fifteentooths
Voir,http://pennapps-xiv.devpost.com/submissions/56784-voir,"Inspiration Difficulty for people without vision to locomote

What it does

Vibrates an app according to the distance a blind person is using ultrasonic sensors

How we built it

Challenges we ran into Combining Hardware and Software is always a challange

Accomplishments that we're proud of Making it work!!

What we learned A lot!

What's next for Voir Making it look nicer!
",,https://github.com/caiorox10,,"Lutron IoT Prize, Best User Experience",hardware,caiomucchiani
WootLoot,http://pennapps-xiv.devpost.com/submissions/56785-wootloot,"Inspiration

Certain products did not live up to our expectations and the general careless attitudes towards receipts, forms, and warranties made it almost impossible to accurately identify which products could be returned, refunded, or replaced. 

What it does

We created an app that allows ordinary people to easily keep track of their warranties and receipts so they can track and locate which items are eligible to be returned or replaced based on their uploads. 

How we built it

Since our team members were fairly comfortable with java, we felt that using Android Studio Projects to create an android app was the best way to go. 

Challenges we ran into

Most of the team members were unfamiliar with the Android Studio setup and app development in general. However, with the help of our mentor, we were able to combine our knowledge and create an app of which we are very proud. 

Accomplishments that we're proud of

Creating an app that can help its users and save them money.

What we learned

For most of the team members, this was one of the first real exposures to app development. We enjoyed this new field of which we might have never dabbled in in the first place. 

What's next for WootLoot

Although we were hindered by our lack of app development skills, we have many ideas for furthering the capabilities of this app. We thought of adding photo-recognition that would allow the user to use only a photo so that the app can automatically input details for them. The app could be partnered with credit card companies to provide e-receipts, reducing paper waste and offering better financial management.
",,,,Most Entrepreneurial Hack - Blackstone,education,HanaXu,kalchu,Melleio
Laser Tag,http://pennapps-xiv.devpost.com/submissions/56786-laser-tag,"Inspiration

We were both interested in building a mobile app and we decided on the train over to build a laser tag application. We started off going back and forth pitching game ideas to each other until we found one that seemed right.

What it does

The game takes the color of the enemy shirt for both players, then creates a color coded value for the other player defined area as expressed through the location picked. The eliminated player will be notified via if they are hit (both players must enter each other's phone numbers prior to starting).

How we built it

We used the Android Studio and open-source code to create an AR experience, then added in our own graphics and logic.

Challenges we ran into

Despite our knowledge of Java, we are both high school students with no previous experience in Android, which required a lot of learning on-the-spot. We also had to exclude a few of the more complicated features we hoped to add in order to enhance the core foundations of the game. We planned on implementing a discriminatively trained part based model, multiplayer matches, and different lens for the scope, but we focused on improving the core foundations of the game, given a relatively short time to build a complex app. 

Accomplishments that we're proud of

We're proud to have developed an app as complicated as we did without prior experience with Android.

What we learned

We learned how to incorporate our classroom Java learning into a complicated mobile app with many features very quickly. 

What's next for Laser Tag

Laser Tag has a lot of potential for development of new features, such as a team mode, a better identification model, and shields that are cast by turning the phone sideway
",,https://github.com/JamesRaub/LaserTag,,Most Entrepreneurial Hack - Blackstone,vr/ar,StockUp,JasonKulp
MediBot,http://pennapps-xiv.devpost.com/submissions/56787-medibot,"Inspiration

Hospitals have drastically cut nursing staff over the past decade with millions more patients flowing into the healthcare system. Around 300,000  patients die from medical mistakes, many of which are caused by chronic overwork & staffing shortages. Robots have the potential to minimize busywork for nurses and re-focus hospital attention on patient care. We made MediBot to provide a cost-effective automation solution for hospitals and clinics.

What it does

We provide a fully automated intra-hospital delivery & patient care service to patients in a seamless fashion. The first part of our system is the Amazon Echo Dot + Alexa. Patients (that may be physically disabled) can call for MediBot deliveries. These delivery requests (intents) include:
*Deliver Food
*Deliver Water
*Deliver a Tissue Sample
*Deliver the Blood Sample
*Call for Help
*Bring Nurse
*Call Nurse
*Get the Doctor
*What is my prescription schedule
*When do I take my pills

The tasks assigned are instantly uploaded to our express server and processed to be viewed on our mobile application. In our app, we just list out the tasks that have been assigned to the bot. 

After the task has been assigned (via bluetooth), the Bot begins to move along a solid colored path. In hospitals, MediBot would have a great impact. Due to the fact that colored lines are used as navigation in hospitals, MediBot would have an easy time navigating around the turns using it's photo sensor to follow hospital lines. 

How we built it

The automated intra-hospital delivery & patient care service has three parts: The Alexa commands, a mobile app, and a complimentary delivery bot. The Alexa leverages the Echo SDK and Express to allow for communication within the mobile app based on vocal commands and broadcasting. Once the command has been said, we iterate multiple updating processes to allow for the commands to be shown on the  mobile app created entirely in Ionic. Commands then trigger the Bot to begin to move on a solid colored path. This type of sensing and movement is possible using a BLE receiver and transceiver along with 2 360 servos, Arduino Uno, 4 light sensors, a battery pack, and a 3d lasercut caster/wheels. When all 3 parts eventually combined with live data from commands, they truly form a fully automated Voice assisted intra-hospital delivery & patient care service.

Challenges we ran into

We initially wanted to create a dashboard of multiple Bots in a hospital environment, however this was troublesome due to the limited capability React had with animations. This made us move to a mobile Ionic app. We also had to look into getting Light Sensors to work with because IR sensors were really finicky to work with. We had to learn how to use Light Sensors because change in light for IR Sensors did not change significantly when turned on. We attempted to account for these problems as much as possible, allowing us to bring all 3 parts together with a seamless easy experience. 

Accomplishments that we're proud of

We were excited when we knew we had a cohesive system, from requests to visualization of these requests, to LINE SENSING with our bot, instead of just one part. It was exciting to even see UPenn Medicine professors excited about our product! We were also proud of creating something that all of us will actually see in practice (we solved a common problem and we hope to change lives significantly with this product).

What we learned

We learned how to use Light Sensors. We also learned how to efficiently sync data across Alexa, our Mobile App, and the Bot. This project was filled challenges that we had to google extensively and debug constantly to get right.

What's next for MediBot

We want to implement a machine learning layer on the bot and application to allow for robust sharing of requests across all users. With NLP features, such as keywords and common requests, we can provide patients with even more commands. We can develop ML models to factor in faster routes within a hospital to reach patients. We also want to provide this product ready out of the box to other hospitals or our own in the near future. MediBot can improve in a number of ways -- we will tackle these tasks generally in order of importance as we see fit. 
",,https://github.com/samkho10/mediBotApp,,Best Amazon Alexa Hack,health,AdilVirani,sameerkhoja,gl367,alicepham
Who'sThere,http://pennapps-xiv.devpost.com/submissions/56788-who-sthere,"PennApps Project: Who'sThere

Purpose:

This project implements a modern intelligent door bell that automatically reminds hosts to open the door and give instant feedback to guests who rang the bell. When someone rings the bell, it will immediately send a notification to the user and let him or her to choose whether to open the door or not. After the user gives feedback, this message will be directly forwarded to the bell as shown in LED lights. If the user does not give a clear response in 30 seconds, it will notify the guests as the door will not open. 

Methods:

The project mainly consists of three parts:
Part I: An Arduino circuit with three LEDs that simulates the behavior of the bell and use LEDs as feedback to guests.
Part II: A server developed in Python that connects both the bell and the host. It forwards the ""bell-rang"" message to the host when guests come and then sends the user's to response to the bell and displays it on the LED to let the guests know.
Part III: A front end iOS application developed in Swift that receives ""bell-rang"" message and lets the users leave their feedback. It also records the door bell activities when the host is not at home.
",,https://github.com/haohanshi/PennApps,,"",hardware,shulij,yusihao51,StevenYang0924
iDropbox,http://pennapps-xiv.devpost.com/submissions/56789-idropbox,"Inspiration

As students from Korea, finals was not the end of our stresses as we were searching for places to store our belongings. With not much option available, we were forced to pay hundred of dollars to store our things over the summer. We do not want other international students or students who live away from Philadelphia to go through the same experience.

So, we made iDropbox. In just a few clicks, iDropbox connects students in need of storage to those who can offer storage space. YES. You drop boxes, and I DROP BOXES too. we hope to improve your campus-life quality with iDropbox. We even connect to Lyft. So no more bicep workouts moving Boxes and dropping boxes.
",,http://idropbox.herokuapp.com,,"Best Use of Lyft API, Best Domain Name Registered in PennApps XIV, Lutron IoT Prize, Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App, Best Use of Data Visualization",social + civic hacking,uhrambuhroo,soyoungpark,leebc,david920123
LLVZen,http://pennapps-xiv.devpost.com/submissions/56790-llvzen,"Inspiration

For the last 2^6 years, compilers have struggled with business models and monetization... but no more! With LLVZen you get the dual benefits of an added revenue stream, and an incentive for customers to simplify their code.

How does this magical mystery tool work? I'm so glad you asked. LLVZen hooks into the LLVM compiler toolchain and detects when one of your idiots clients tries to compile a program with more than 5 functions. It pops up with a friendly error message and nulls out the results of all future function calls. Clients are then sternly requested to place money in an LLVM tip jar to continue compilation.

What kinds of problems would this solve?

THIS!

How do I install this gem?

Well, good question... ~watch this space~
",,,,"Best User Experience, Most Entrepreneurial Hack - Blackstone","",tmhrtly
UrbanHelpProject,http://pennapps-xiv.devpost.com/submissions/56791-urbanhelpproject,"UrbanHelpProject

Urban Help Project is an effort to consolidate a network of community members that could potentially use support from local non-profit organizations. The concept of the project is simple: users (you!) feed location and appearence data of community members into our servers, and we parse the data into a graphical user interface available for use by our partnered non-profit organizations. Ideally with enough active users on any given day, the Urban Help Project can significantly streamline relief, rescue, and support efforts within the urban community.

As students at the Johns Hopkins University in the city of Baltimore, it was impossible not to acknowledge that our community was broken. It came to a point where even at our school campus we were able to recognize the same faces over and over asking for spare change outside the nearby Chipotle, or just wandering around the decrepit alleyways they were forced to call home. Having worked with local non-profits that worked to support victims of human trafficking in Baltimore city, such as The Safe House of Hope, we found that a big problem in operations for organziations such as these was simply not knowing where the people they needed to help were located! We developed the Urban Help Project to not only logistically help these local non-profit organizations, but to also promote greater community outreach, especially within our own school community. The Urban Help Project more or less requires person-to-person interaction with those that we want to help, and the hope of this is that we can work towards real community cohesian.
",,https://github.com/jkim502/UrbanHelpProject,,"Best Use of MongoDB, Best Progressive Web App",social + civic hacking,jkim502,matthewjlee,timkim321
Make Slack Great Again,http://pennapps-xiv.devpost.com/submissions/56792-make-slack-great-again,"Make Slack Great Again

A responsive chatbot which mimics political rhetoric and teaches American history

Inspiration: This election season has seen the most absurd political comments since since the creation of ENIAC. To demonstrate the rhetoric’s simplistic patterns - especially from Republican nominee Donald Trump - we set out to make a chatbot which could pass as modern politician. With chat support for Franklin D. Roosevelt, Bernie Sanders, Donald Trump, and Sonia Sotomayor we hoped to demonstrate the full gamut of American politics. What began as a tool to mimic political rhetoric quickly expanded when we realized the uses that our technology has in a classroom.

Our Bots are perfect for teaching tech savvy students about the policies and personalities which govern the country. Sometimes funny, sometimes inspirational, and always a blast to talk to - we are having a hard time picking a candidate! We are able to quickly add new personalities and expand the knowledge of existing characters. With our fine tuned machine learning chatbot, this system is easily scaleable.

The project is primarily a SlackBot framework, with support for voice through Amazon Echo. Users choose a bot in Slack and can ask questions in plain English which are quickly answered from a library of relevant quotations or from newly generated phrases in the style of the character. Trump and Sanders are trained for political discourse and can engage in moderated debate with each other while Roosevelt and Sotomayor are examples of the educational uses for the software and are focused on The New Deal and The US Constitution respectively. 

Educational application: The Roosevelt and Sotomayor characters may have less mass appeal, but to us they are the more exciting test cases. The best application for our chatbots are in the elementary school classroom as a gateway to new worlds. Rather than read a history textbook, or memorize the names or supreme court justices, students could engage with the interactive personalities and ask questions for themselves. A few chats with Sotomayor will inspire any student in ways that textbooks can not. On top of that, Sotomayor acts as a shell for our legal encyclopedia. She answers questions about the law and about her childhood. This combination brings an entire interactive legal experience.

What we learned: This project crystallized for us the difference in how our politicians speak. From FDR’s motivational quotes, to Trump’s inflammatory criticisms, this taught us a great deal about modern political science.
On the technical side we implemented Latent Semantic Indexing (LSI) to read and “understand” the questions that we provide. The concepts generated in that process are then matched to our human profile characteristics.When we are unable to find an appropriate response, our algorithm uses Markov chaining along with a probability equation and a check on grammar in order to weed out the worst answers and the ungrammatical ones. The sum of our Natural Language Processing (NLP) algorithms gives us a powerful and reactive personality. 
",,https://github.com/rangat/realDonaldTrump,,"Best Amazon Alexa Hack, Best Use of Data Visualization",education,rangat,Sail338,aalllxx,kevintse328
LIT,http://pennapps-xiv.devpost.com/submissions/56793-lit,"Inspiration

Lighting sets an appropriate atmosphere and heightens an audience's understanding of a theatrical performance. However, rehearsing and experimenting with lighting is costly and time consuming. That is why we created a virtual lighting lab that is universally accessible, affordable, and fun! :) 

What it does

There are two major components for LIT. 

1) The LIT lighting lab provides a real-time rendering of the stage to the user in response to any change made to light's color, intensity, and the camera perspective.The user can toggle light helpers. The spotlight palette is pre-populated with LEE's filter colors link for users to experiment with, and they may also save the cue configuration and download the scene's images for future reference. The user can zoom by scrolling, rotate by dragging, and pan by holding middle mouse button and dragging.

2) The LIT VR Viewer puts users in the stages they designed. It uses device orientation so users can use a cardboard to look around the stage. By saying ""cue,"" users can transition to a scene with different lighting and perspective. Demonstration of cue: link.

How we built it

We used three.js to set up our 3d environment. Using jQuery, we linked our Material Design Lite (MDL) UI components to the spotlight objects so users could modify the color, intensity, and light helpers. We used Python's Beautiful Soup to scrape LEE's color filters. We used JavaScript to enable cues that can be saved and allow the user to download photos of the stage.We also created a VR mode by hyperlinking to the stage fullscreen and using device orientation to respond to the user's movements. Anyanng's voice recognition API allowed the user to cue through scenes in VR Mode.

Challenges we ran into

Threejs had many deprecated methods. 

It was my first time programming in JavaScript, and I didn't realize the ordering of scripts and function calls mattered. That led to unexpected challenges because the logic in the code was correct but it still didn't compile. We weren't sure if the renderer couldn't handle too many lights, the virtual model was too big, or there was an issue with the code. We started over using Threejs's WebVR scripts. However, the user had to enable the site's WebVR, which would have made LIT less accessible. We were able to configure VR using device orientation and stereo effect by editing and reorganizing the code.

Accomplishments that we're proud of

Intuitive Design
VR mode / Voice Cues

What we learned

This was my first time programming in Javascript, Python, and HTML. I learned how to program a 3d environment and interactive GUI, scrape information, and enable VR and voice recognition using these programs.

What's next for LIT

This prototype is based on Penn's PAC shop: link. This is a demonstration of LightWeb, the light simulation we based our program on: link. We look forward to adding more features such as different types of spotlights and allowing the user to change the location of the cameras.
",https://youtu.be/Um4zF7RQJGU,https://kongsally.github.io/LIT/,,"Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Use of Data Visualization, Best Use of VR/AR for Content Discovery",vr/ar,nickjang,kongsally
Soundscape,http://pennapps-xiv.devpost.com/submissions/56794-soundscape,"Why Soundscape

Hacking for the hack of it. It is a great mantra, and one that we often take to heart. While there is significant value in hackathon projects that offer aid in difficult and demanding tasks, sometimes the most interesting hacks are those that exist for their own sake. Soundscape takes a novel approach to an activity that many of us love — discovering music. Instead of letting the user simply respond ""yea"" or ""nay"" to an ever increasing list of songs, Soundscape places you in midst of the action and shows you a world of music right under your feet. Users can then pursue avenues they find interesting, search for new or exciting pieces, or merely wander through a selection of dynamically curated music. With Soundscape, you have a hack-of-a-lot of power.

Functional Overview

Soundscape is a Virtual Reality application based on the Google Daydream platform. It curates data by crawling Soundcloud and building a relationship model of songs in their repository. From there, it uses advanced graph search techniques to identify songs that are similar to each other, so that users can start with one set a long, and shift the genre and style until they find something new that they enjoy. 

Technical Overview

Soundscape is built on top of Google's yet unreleased platform for high quality mobile virtual reality—Daydream. Developing most of the application’s front end in Unity, we make use of this framework in conjunction with the existing Google Cardboard technology to help power a virtual experience that has high fidelity, low stutter, and intuitive input. The application itself is built in Unity, with custom hooks built into the Daydream infrastructure to allow for a high quality user interface. 

The core functionality of Soundscape lies in our backend aggregation server which runs a node, mongodb, and express.js stack on top of Linode. This server fetches song, user, and playlist data through the SoundCloud API  to generate similarity scores between songs — calculated through user comments and track favorites. This conglomerated data is then queried by the Unity application, alongside the standard SoundCloud data and audio stream. Search functionality within the app is also enabled through voice recognition powered by IBM’s Watson Developer Cloud service for speech to text. All of this works seamlessly together to power one versatile and unique music visualization and exploration app.

Looking Forward

We are excited about Soundscape, and look forward to perfecting this for the final release of Google Daydream. Until then, we have exciting ideas about better search, and ways to incorporate other APIs
",,http://69.164.214.207:1337/,,"Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Best Use of Data Visualization, Best Use of VR/AR for Content Discovery",vr/ar,AzurNova,zwade,bluepichu,el1t
MeChrome,http://pennapps-xiv.devpost.com/submissions/56795-mechrome,"Inspiration

A main driving part of the inspiration was to create a History reading app which would display history in the form of a tree to make it easy for a person to understand his browser history. Apart from this we thought that it would be very convenient if we can create a one stop shop for all my accounts be it Google, Facebook, Twitter or any other, it should be linked to a central point from where I can access all of it easily.

What it does

The application is a Google chrome extrension. It basically displays the chrome history in the form of a tree. Apart from this it helps you access various accounts you have from point itself - gmail, facebook, LinkedIn etc. 

How we built it

We built it using web technologies like HTML 5, Twitter Bootstrap, javascript, treant.js.

Challenges we ran into

We ran into problems of creating a tree structure out of the history api of gooogle chrome.

Accomplishments that we're proud of

We were able to successfully divide the work and then create separate UI and a backend modules as small teams within the team.

What we learned

As this was our first Hackathon we learnt a lot including - Generation of practical ideas, Team dynamics, time management.

What's next for Personal Dash for Chrome

Syncing the extension with other APIs like Twitter etc. to get personal notifications. 
Interfacing the tree UI with the Tree creation algorithm.
",,https://github.com/MihirPattani14/MeChrome,,Best Progressive Web App,"",MihirPattani14,rudrakshashah,rms13aug,NidhiAngle
Rouse,http://pennapps-xiv.devpost.com/submissions/56796-rouse,"Inspiration

A month ago, one of our teammates' friend got depressed. He asked her for help.
He complained about his ex-girlfriend. She asked him questions and he kept complaining. She realized that she couldn’t help him by just focusing on his problem. By calling him every morning and shouting out his dream, she helped him with goal commitment and regular exercises of self-regulation. He felt less depressed by self-reporting. The phone call has made a positive change, but not just for him. She was having trouble waking up on time herself. By calling him, she felt responsible to him and got up on time as well. If a phone call can help she and her friend, it can help you and your friends. We designed a website, Rouse, where everyone can pair to call anyone and make their dreams come true.

What it does

Rouse pairs people with a shared goal - waking up on time. The process is, first, Akshay signs up, inputs when he wants to get up tomorrow, whether he want to be Rouser or a Rousee. Rousers call and shout out Rousees’ dreams. If Akshay chooses Rousee, he will input what goal he wants to be waken up, like “make a positive impact to the world.” Then, all set. Before 9pm the same day, Akshay will be matched with his Rouser, let’s say Annie. Annie has two kinds of information, Akshay’s getting up time and his goal. “Hey, Aksahy, what can you do to make a positive impact to the world?” “Programming!” “How?” “Oh, I have a PennApps to go today.” “What should you do now?” “Get up and go to the airport.” “Just do it.”

How we built it

Some hackathons are devoted to exploring the cutting edge of technology.  Some hackathons are devoted to using proven systems to teach newbies how to do awesome stuff and built something impactful.  PennApps XIV has been the latter: we collaboratively developed a classic PHP web app that taught our entire team the fundamentals of effective web apps and allowed us to most easily create an effective product.

Challenges we ran into

The same challenges as usual: not knowing what to do for the longest time, getting stuck at every point in the road, team disagreement on design choices, etc.

Accomplishments that we're proud of

Finishing

What we learned

Make a positive impact together as a Hackers' team.

What's next for Rouse

Rouse can help people get connected and pursue their dream. By reporting what matters to them, people can use Rouse as a reflection space and a platform to build meaningful connections.
",https://www.youtube.com/watch?v=vN45AOZsD6U,https://github.com/ac2zoom/rouse,,"",social + civic hacking,Mingflow,monmon_2007,Ac2zoom,kuannie1
AirPnP,http://pennapps-xiv.devpost.com/submissions/56797-airpnp,"AirPnP


Have you went on a trip in high spirits only to be stuck at finding a place to park at the crowded beach, wasting your previous time. Or are you trying to get to an urgent meeting in a busy city but has constantly failed to find that plot of land to leave your car on? 


Why?

Following the success of so many crowdsourcing platforms. We decided to tackle the aging issue of parking space, in a new way.

It's all about Economics :)

Down to the core, it is a simple concept: people owning private parking spaces do not utilize their space the whole time, while at other congested areas travelers are finding it increasingly difficult to find parking. Thus with a potential supply and demand, we have a market. Given the increasing strain on infrastructure, it is sensible to tap on hidden potential sources.

Design

We designed our app with users at heart. We give careful attention given to each detail; we followed material design and stripped elements to the bare minimum while maintaining coherence and readability throughout every screen, thus delivering a beautiful product. 

We hope you would enjoy using our app, stay tuned for more updates!
",,https://github.com/Mete0/AirPnP,,"Best Use of the Capital One API - Nessie, Best Use of Linode Services, Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API",social + civic hacking,Cadenjiang,Meteo,sinhadevang
Experia,http://pennapps-xiv.devpost.com/submissions/56798-experia,"Inspiration

VR is an awesome new technology, but is limited in its capabilities of transforming reality. While it's great to experience visual feedback, true experiences involve both visual and physical feedback. We wanted to improve existing VR to include those capabilities.

What it does

Experia provides a completely immersive VR experience, with visual AND physical feedback. As you explore scenes on the Google Cardboard, heat pads, fans, vibration motors, and more allow you to engage with physical sensations, making each scene come alive.

How I built it

We built a Google Cardboard app for the iPhone using Unity and Xcode that communicates via WiFi to an Intel Edison on a headpiece attached to the Google Cardboard, and XBees on two arm pieces containing heat pads, fans, and vibration motors. 

Challenges I ran into

Communication -- It was difficult having all the separate pieces talk to each other.

Accomplishments that I'm proud of

Executing all the individual parts of our idea. Attempting a difficult hardware hack.

What I learned

How device communication works. VR Development.

What's next for Experia

Applications! This technology can be used in training for military personnel, firefighters, or other high risk careers, exposure therapy, autism therapy, etc. 
",,https://github.com/sidhartv/Experia,,"Lutron IoT Prize, Best Use of Rapid Prototyping, sposnored by AddLab, Most Entrepreneurial Hack - Blackstone, Best Use of VR/AR for Content Discovery",vr/ar,namritamurali,cyrustabrizi,tushita,sidhartv
Fire at Will,http://pennapps-xiv.devpost.com/submissions/56800-fire-at-will,"Inspiration

We were inspired to build Fire at Will by some of the problems we have faced as developers in our own rights, trying to test our own sites. We found that the amount of resources that effectively completed this task were extremely limited, and for the most part, far out of the price range of any student/developer. By building this application, we not only came up with an excellent solution to a commonplace problem, but also identified a way in which our system could be utilized to reduce the cost to the tester, making it easier for a company to effectively test their products.

What it does

Fire at Will is a multifunctional application that works to observe and fix potential issues with client-server interactions. By monitoring data transfer rates along with several other metrics, Fire at Will enables a tester to not only test load handling capabilities of their systems, but additionally provides the ability to allow said user to test their systems against brutal waves of requests, similar to that which occur during many attempted penetrations of secure corporate networks.

How we built it

We build Fire at Will using Erlang (due to its benefit of being able to control an infinite number of nodes with minimal latency), Node.js, MongoDB, AWS, Grafana, Linode, Prometheus, and an extremely large amount of caffiene.

Challenges we ran into

Some of the challenges we ran into were related to the feasibility (Or lack thereof) of writing/testing software for

Accomplishments that we're proud of

What we learned

What's next for Fire at Will
",,https://github.com/kunalpatel446/pennapps2016f,,"Best Use of Linode Services, Best Use of MongoDB, Most Entrepreneurial Hack - Blackstone, Best Use of Data Visualization",cybersecurity,kunalpatel446,dshah94,Flak
Find Missing People,http://pennapps-xiv.devpost.com/submissions/56801-find-missing-people,"Inspiration

Each year in China alone, 20,000 to 200,000 children went missing, kidnapped or trafficked. They are usually forced to labor, auctioned illegally for adoption, or even entrapped in gangs. People witness them on the street, but the reporting and identification process is frustratingly time-consuming. Convenient and timely identification check is essential to further actions. We aspire to seek solutions in the light of face recognition technology.

What it does

Our web-based app allows users to match their cloud photo repositories with a missing person database. Upon confident matches, the app will prompt identifications to the user for further investigation. In detail, the app does:


Get photo access autherization from repositories (Google photos for the submission)
Identify distinct faces in each photo
Match each face with identities in the databases
Return photo pairs of 2 similar faces (and mark them)
Render paired results on the website and allow users to decide if we have a match


How we built it

The app was written in Node.js with Express framework. Face recognition is achieved with Microsoft Face API for face detection and similarity matching. We feed source user photos from Google Picasa API. Since there is yet a publicly available missing person database, we created a sample MySQL database on AWS. The Front-End is written in HTML, CSS and javascript with Bootstrap.

Challenges we ran into


Comparing different Face API and work around their conditions, which has been a lot
Node.js - a new web tool for the entire team.


Accomplishments that we're proud of

We are proud to have come up with this powerful idea and made a preliminary product within such short time with new tools.

What we learned

All components of a web application on node; in particular we learned to write async programs.
App development in team.

What's next for Find Missing People


More repositories: Facebook, iCloud, or video searching/ cameras?
Not as an app but as an extension? In fact, we see the app to be 'hidden' behind the scene and only message the user or relevant authorities when a match happens. It might a recursive program on the photo storage platforms.
Better people database. The application is not limited to missing people, but also criminals at large.
Distributed systems and map-reduce for faster calculations

",,https://github.com/westmes/PennAppsXIV.git,,"",social + civic hacking,yibangchen,westmes,dnalom,akahd
AllCommunication,http://pennapps-xiv.devpost.com/submissions/56802-allcommunication,"Inspiration

We wanted to build that application that would help hard-of-hearing people communicate without the need of an interpreter by showing ASL video translations in real time. We felt that this is a very important problem, as it will allow people who are hard of hearing to speak without the need of an interpreter, and thus, have a more personal connectoin with the people they are talking with.

What it does

The app is composed of three parts - a dictionary, speech translation, and live video chat. The dictionary takes in any word and shows the user a sign language representation of the word. This is extremely useful for deaf people who want to learn new English words, as well as others trying to learn ASL. Next, the speech translation takes in speech input from the user, and converts it into a video of the ASL translation. This mode is primarily useful for someone who wants to communicate with a hard-of-hearing person. Finally, we implemented a live video chat as well, which will dynamically send the second user ASL translation videos of what the first person is saying. 

How we built it

We built our server using Node.js and Express, and we webscraped sign language videos using Python's Beautiful Soup. We also used EasyRTC to enable the video chat.

Challenges we ran into

We had trouble integrating EasyRTC into our application - it took us time to get the video chat to work. Additionally, we had a little trouble integrating Node.js and Python as well, as the Python subprocess's return value would not exist in the global scope. Eventually, though, we found a solution, and thus we were able to integrate webscraping into our project.

Accomplishments that we're proud of

Integrating EasyRTC was one of the most difficult parts of the project, so we're definitely proud of that. Also, we were glad that we got the Python and bash subprocesses set up.

What we learned

We learned many useful things about js, web development, and backend development

What's next for AllCommunication

In the future, we want to focus on using motion-detection software to detect the movements of a hard of hearing person, and then convert that into text. 
",,,,"Best Use of MongoDB, Best User Experience, Best Progressive Web App",social + civic hacking,abhiramgogate,vanshady
RecoGrader,http://pennapps-xiv.devpost.com/submissions/56803-recograder,"RecoGrader

Version 1.1.0

Author: Haocheng Fang, Zecheng He, Zesheng Li, Zhengyuan (Zach) Pan

Last updated: Sept 4, 2015
",,https://github.com/zeshengli/pennAppsF16,,"","",JasonLi,zechengh09
Sign Speak,http://pennapps-xiv.devpost.com/submissions/56804-sign-speak,"Inspiration

I was inspired to make Sign Speak when I read about a barista at Star Bucks who learned sign language to make a frequent customer feel even more welcome at Star Bucks. While I may not have the determination to learn ASL, I can sure as hell engineer a way for people who use sign language to be heard!

What it does

Sign Speak is comprised of two main components. 1: The Arduino based glove that feeds data through bluetooth low energy while a person is signing. 2: The iPhone app is collecting the data send over bluetooth and proceeds to feed it into a special machine learning algorithm (SVM) and helps to determine the sign that the person is making. The iPhone then uses it's speaker to speak up what the person signed.

How I built it

I first breadboarded my entire circuit to ensure that everything was in tip top shape. From there, I went on to establish the connection between my Arduino and iPhone application, and limit the flex sensors to a readable value of 0-100. After that I learned how to solder and created the circuits that I needed. Unfortunately, it got fried, and I had to restart... So I re-soldered everything, fit it into an NES Power Glove, and went on to work on the machine learning algorithm. Unfortunately due to the nature of machine learning, a lot of data was needed, and I did not have the hours to train the glove to learn every ASL letter. So I decided on 4 important letters to me. H-A-C-K.

Challenges I ran into

Machine learning was definitely a challenge for me having used it only a few times before. I was given help by a friend 
not attending PennApps, Nathan Flurry aka McFlurry. Another challenge was losing all of my hardware mid way through. Luckily a PennApps volunteer/organizer found out that they had more Arduino Micros, that had not been listed before and saved my project.

Accomplishments that I'm proud of

I am very proud to have made Sign Speak over the past 36 hours on a 1 man team. It was a daunting task but I think I was able to execute a successful hack!

What I learned

I learned a great deal about Arduino hardware and how to solder, as well as Arduino and iOS interfacing. 

What's next for Sign Speak

I hope to create a more refined version of Sign Speak, one with an accelerometer/gyroscope that will allow me to not only identify letters, but also words/gestures.
",,,,"Best Use of Rapid Prototyping, sposnored by AddLab",social + civic hacking,SarwalNeel
Learn My Sites,http://pennapps-xiv.devpost.com/submissions/56805-learn-my-sites,"Learn My Sites is a smart website article aggregator that uses the websites you specify along with advanced machine learning algorithms to show you the best content for you on your favorite websites. 

Our project utilizes word embeddings using Natural Language Processing and scrapes the provided news websites with AJAX in order to obtain these word vector representations of articles.

It then runs unsupervised clustering on these word vectors with K-means to obtain clusters of relevant articles.

The outputs are then a click serve of relevant articles grouped by cluster. In addition we run t-SNE for JS to obtain a 2D embedding of these 4,000+ dimensional word embedding to visually represent the cluster space of articles.
",https://youtu.be/gUrrxVPzapA,https://github.com/isaykatsman/learnmysites,,"Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App, Best Use of Data Visualization",education,isaykatsman,Chilli
Bus Notify,http://pennapps-xiv.devpost.com/submissions/56806-bus-notify,"Inspiration

In all walks of life, getting up in the morning is quite possibly the most tortuous moment of the day. Compounding this painful experience is the stress of worrying about the timing of your school bus. As high school students, the inconsistency of school bus arrival times can force students to wake up early or rush breakfast, losing valuable sleep and nutrients, only to discover that the bus is arriving late. Unfortunately, there is no such app that notifies the high school student about their buses arrival time. Any app that comes close to this tracks the bus and makes that data available to the students, something our team sees as a major security hazard to all of the students and the driver. So we set out to create a service in which students will have a better idea of when their bus is coming, without creating any security risks.

What it does

Bus Notify is a mobile-web coordinated application that notifies students of their bus’ anticipated arrival using crowdsourced bus routes and driver cooperation. Students collaborate to create routes by inputting their individual stops on a website. The driver can obtain the route code from the students and the must allow a background service to run on their phone during their bus route. A custom algorithm determines when the bus is about two minutes away from a stop and sends a text to the students at that stop. Now, a student doesn’t have to worry about whether their bus will be late or early, because the app makes it easy by telling the student how long they have until their bus comes. Currently the app only sends a notification at 2 minutes before the bus reaches a stop, however in the future we plan to have more information about their bus available to the students.

How we built it

We approached this app with a divide and conquer strategy. One person was responsible for the server, while someone else was responsible for the Android app. Our third person can be thought of as the glue between both platforms; he enabled interaction between the server and the Android app. The server was built in NodeJS and used the Twilio API to provide the SMS feature. The Android app was built the classic Android way, in Java, and used the Google Play Services API to collect data about movement and location.

Challenges we ran into

Getting the location of the bus driver and knowing whether they were moving or not was a difficult task to do. On Android, we had to run two different asynchronous services to retrieve this data, however we had to send it within one request to the server, so that was complicated. The algorithm was challenging to visualize at first because it was not a simple mathematical algorithm, but instead a complex algorithm with a series of calculations and API calls that can produce a variety of results, all of which must be accounted for. The last problem we had, and possibly the hardest was finishing the Android app after our teammate responsible for building it had to leave.

Accomplishments that we're proud of

One of our team members had to go home early. This teammate was the responsible for the Android app. Unfortunately, he was unable to finish the app, so we had to pick up where he left off. This involved understanding his code and reading through significant documentation, not to mention having to work on the server in parallel.

What we learned

We learned many hard skills such as using various API’s such as Twilio and Google Maps. But even more than that, we learned a crucial skill for software development, working together with a team. In software development, there may only be one product, however the team behind the product may be 5, 10, or even 100. For the product to work and be successful, every team member's module must “play nice” with the other team member’s modules. Otherwise, all of the code will just be a large blob of code with either missing or no capabilities, a lesson we learned the hard way when we built the server and the Android app our own ways, with our own strategies and assumptions about the product.

What's next for Bus Notify

What’s next? Judging! In all seriousness, the team behind Bus Notify plans to take the app to market of course. However, it's not that simple. Because our product requires crowdsourced data, and involves the tracking of a bus driver, we must ensure that our data collection methods are as easy to use as possible for users and we must guarantee complete security for the students and the bus driver despite having information about the bus driver’s location at any given moment. We are also considering to change the name to “Two minutes” because the only current notifications that a students will get about their bus is that it is two minutes away, or it is here.
",,https://github.com/ShivamHacks/Bus-Notify,,"",social + civic hacking,ShivamAgrawal,luckystars123456789
Thryft,http://pennapps-xiv.devpost.com/submissions/56807-thryft,"Inspiration

We were inspired to make Thryft after noticing that surge pricing for app-based transportation like Lyft was sometimes arbitrary. After doing some research, we learned that sometimes the surge would fluctuate and the price multiplier/price estimate would change drastically when you walk a short distance away from your original location and request a ride. We wanted to make Thryft to save consumers money on Lyft, as well as get more people to use Lyft by decreasing or even negating the extra money they would have to pay for “prime-time”. 

What it does

Thryft is a mobile app that uses the Lyft API and Google Maps API to find the best location for people to request Lyfts in their general vicinity. It capitalizes on price inefficiencies, a common phenomenon in shared economy products. When someone uses the app, they enter in their starting location and their destination, as well as how much they’re willing to walk to save money. Thryft then shows the best location for them to walk to in order to get the cheapest price for their Lyft.

How I built it

We built this using Ionic, a mobile app framework that uses HTML5 and AngularJS. We also integrated Google Maps API and Lyft API to create the map and location points for the user.

Challenges I ran into

Authentication (OAuth2) for Lyft API
Adding Google Maps API to Ionic

Accomplishments that I'm proud of

Learning Angular.JS and Ionic. Making friends!

What I learned

Javascript, Ionic, API Integration

What's next for Thryft

We want to expand Thryft to other transportation companies like Uber.
",,https://github.com/lavkannan/LyftPriceFinder,,Best Use of Lyft API,social + civic hacking,lavkannan,kmurali8,sarahjwang,dkang9
HackerTracker,http://pennapps-xiv.devpost.com/submissions/56808-hackertracker,"Inspiration

My friends and family always ridicule me for my short and easy passwords on my phone and laptop so I wanted to develop a solution to this problem, not by making longer passwords, but though machine learning. After talking to a friend about the vulnerability of passwords in general, we came to this exciting idea and decided to pursue it at this hackathon. 

What it does

Our program protects computers/laptops from hackers by discretely monitoring the unique typing pattern of the user. The program uses machine learning to learn the owner's typing pattern, and once implemented, it can statistically compare the owner's typing pattern to any new typing patterns to detect whether there is an intruder or not using the computer. The user can type anywhere, such as in a word document or in the address bar of a web browser, for our program to detect his or her typing pattern. The best part is that it acts behind the scene without the user or intruder knowing about it. This means it is convenient for the owner but dangerous to the intruder.  

How I built it

We built a desktop application using Java and Maven. We first started off by creating a 27 by 28 matrix for all the different combinations in the alphabet including the space bar. We stored the time it took to travel between each key in the matrix, as well as how long each key was pressed. Then we used a two sample t-Test to determine the z score of the data in order to gauge how likely the user is an intruder or the owner of the computer.

Challenges I ran into

Some of the largest challenges we ran into was getting the background java service to run. We had previously asked a mentor for advice, however, he had trouble believing such a thing was even possible. Fortunately, after much research and troubleshooting, we were able to figure out how to run a java background service using Maven. Furthermore, we had to figure out how to get the program to detect the user's keystrokes in any place on the computer.

Accomplishments that I'm proud of

Accomplishments that we are proud of include sending a text message alert to the user's phone when their computer is being hacked and creating an 26 by 26 matrix authentication system that stores the length each key is pressed and the time between each key. This data is normalized with statistics and compared against data stored locally on the computer/laptop.

What I learned

We learned how to use Maven to run a Java service behind the scenes. This means our program can continue to run in the background and monitor the user even after the application is closed. Finally, we learned how to be a more effective team, in terms of delegating coding tasks and research. 

What's next for HackerTracker

In the future, HackerTracker will implement a GPS feature that will allow users to locate their stolen laptop or computer when it detects that it is being hacked. It will also forcefully shut down or lock the computer to prevent further damage from the hacker/intruder. This technology will have an important role in protecting company as well as consumer laptops and computers in the future. We also look to expand to the mobile phone sector.
",https://youtu.be/d-pkPwykU9U,https://drive.google.com/drive/folders/0B31HQytwTW5NU2xfcXFSLTBuOU0?usp=sharing,,"","",victor8844,jdoman
Virtual biomedical computing system,http://pennapps-xiv.devpost.com/submissions/56809-virtual-biomedical-computing-system,"Inspiration

Biomedical devices like brain epilepsy detectors, heart monitoring system, that are implanted within the body face the major issue of size and power consumption,which should be as small a figure as possible. Hence computationally intesive work like analog processing and digital signal processing, should be routed to virtual processors outside the body, which implement the required logic on-demand, process the sent inputs, and relay the processed output back to requester device.

What it does

This system is not in a fully working condition, but however has immense potential to even reduce the amount of silicon chips that is circulated in the world, with every device querying for processing, allocate resources remotely, compute the inputs and send it to output devices.

How we built it

We conceptualized this using the XILINX pynq board, board itself being under development, having customizable ARM -FPGA fabric that enables one to implement custom logic of choice on the go. We used XBEE modules to send data wirelessly from a requester, to another XBEE module interfaced to PYNQ board

Challenges we ran into

Developing software for the multiprocessor evironment requires working extensively with shared memory concept, needing programming partially in Python, and partially in C , to get the required logic implemented. It took us more time to understand the architecture from scratch ground up, and figure out ways to implement our solution. 

Accomplishments that we're proud of

Stepping outside our comfort zone ! Doing something really challenging, taking the risk of working with a completely new , and potentially amazing hardware board, as opposed to working with our familiarities, that are less capable than this board. 

What we learned

2 days are just not enough to tap the potential of this amazing processing system, now that we know that there is a board like this around in the market, we can continue working further to develop the final working prototype of our project.

What's next for Virtual biomedical computing systems

Ohhh! What is one aspect of this world that is truly unexplored from a computational point of view? It's the human body! . Biomedical implants for brain, heart, muscles etc,. performing wide variety of monitoring and electrical stimulation to terminate any diseases, are the call of hour, and for something to go inside the body, should be as small and least power consuming as possible. This needs virtualization, and fast-tracked development to proceed further in biomedical engineering
",,,,"Best Design Using PYNQ-Z1 Boards, Peripherals, and Tools (3)",health,visheshnp,shreedutch
SpeakAR,http://pennapps-xiv.devpost.com/submissions/56811-speakar,"Inspiration

A week or so ago, Nyle DiMarco, the model/actor/deaf activist, visited my school and enlightened our students about how his experience as a deaf person was in shows like Dancing with the Stars (where he danced without hearing the music) and America's Next Top Model. Many people on those sets and in the cast could not use American Sign Language (ASL) to communicate with him, and so he had no idea what was going on at times. 

What it does

SpeakAR listens to speech around the user and translates it into a language the user can understand. Especially for deaf users, the visual ASL translations are helpful because that is often their native language. 



How we built it

We utilized Unity's built-in dictation recognizer API to convert spoken word into text. Then, using a combination of gifs and 3D-modeled hands, we translated that text into ASL. The scripts were written in C#.

Challenges we ran into

The Microsoft HoloLens requires very specific development tools, and because we had never used the laptops we brought to develop for it before, we had to start setting everything up from scratch. One of our laptops could not ""Build"" properly at all, so all four of us were stuck using only one laptop. Our original idea of implementing facial recognition could not work due to technical challenges, so we actually had to start over with a completely new idea at 5:30PM on Saturday night. The time constraint as well as the technical restrictions on Unity3D reduced the number of features/quality of features we could include in the app.

Accomplishments that we're proud of

This was our first time developing with the HoloLens at a hackathon. We were completely new to using GIFs in Unity and displaying it in the Hololens. We are proud of overcoming our technical challenges and adapting the idea to suit the technology.

What we learned

Make sure you have a laptop (with the proper software installed) that's compatible with the device you want to build for.

What's next for SpeakAR

In the future, we hope to implement reverse communication to enable those fluent in ASL to sign in front of the HoloLens and automatically translate their movements into speech. We also want to include foreign language translation so it provides more value to deaf and hearing users. The overall quality and speed of translations can also be improved greatly.
",,,,"Best Public Safety or Video Processing App, Best Use of Rapid Prototyping, sposnored by AddLab, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Use of VR/AR for Content Discovery",vr/ar,y4smeen,tylercroach,akshaya19,1997mjk
Brognosis,http://pennapps-xiv.devpost.com/submissions/56812-brognosis,"Inspiration

I am a chronic innovator who innovates for the betterment of the society. I look for challenges in the contemporary world and try to solve them using technology. Helping people through technology gives happiness to me. When I see somebody benefiting from my technology, my day is made. During the hackathon, while researching over pressing challenges being faced worldwide, we encountered the issue of breast cancer. ""More women die due to breast cancer in India than anywhere in the world"" As I have roots from India, this statement shocked me and actually inspired me to solve the issue of lack of affordable and accessible diagnostic tool for breast cancer. When I was researching over the topic and understanding the problem of breast cancer, the stories of the people fighting Breast Cancer and the one who actually lost their loved ones inspired me more and more to come up with a solution.

What it does

Working together, our team has come up with an elegant solution,Brognosis. To aid in the early detection and prevention of breast cancer, the Brognosis system uses an ultrasound glove to assist women in performing breast self-examinations. After the scan, an image of the inner parts of the breast appears on a smart phone and is instantly analyzed for cancerous lumps. Brognosis provides painless, radiation-free and delivers accurate results in less than five minutes for less than $1 per exam.It is estimated to be cheaper than any other diagnostic service available. Its easy to use and its more accurate. This is a market opportunity of a billion dollars in India and 8-9 billion dollars globally, annually and our team is going after it.Brognosis analyzes the images of the inner breast, which are obtained through the use of a custom wearable glove, which with the help of ultrasound sensors, does imaging of the inner breast. The images captured by the glove are then sent to the mobile phone application via Bluetooth. These images feed into an algorithm on the Brognosis mobile phone application which uses a convolutional neural network and machine learning to check for lumps and lesions in the Breast. The algorithm then outputs heat maps to highlight problem areas that clinicians can examine to make more-informed diagnoses, providing a more objective assessment than conventional tests. In addition, a remote diagnostic application facilitates sharing of images across to gain consultation from experts right-away via a cloud based tele-oncology platform.

Brognosis also instructs the user on how to preform the self examination by providing them with video tutorials and provides other relevant information towards treatment and healthy lifestyles against breast cancer.

Currently, there exists no portable, non-invasive and easily accessible screening tool for Breast cancer . However there are various diagnosing methods such as Mammography and MRI  which are have limitations such as low specificity and sensitivity , accuracy and radiation respectively. The current system requires a personalized solution for quick and easy screening of theBreast Cancer.  Our innovation includes the features of existing products along with its own advantages to revolutionize Breast cancer screening around the world. The following are the key competitive advantages of the Brognosis system: 
• Simple and easy to use.
• Instantaneous results.
• No additional bulky electronic equipment required.
• Cheap and portable – ideal for rural and remote use.
• Doctor remote consultation and feedback.
• Secure image capture and Lesion detection algorithm.
• Improved Diagnostic Accuracy.
• Multiple Operating System support such as Android and web.

How I built it

We applied Human Centred design while developing Brognosis. After Finalizing on developing a better screening tool for Breast cancer, we started researching on the current scenario and the exisiting solutions and spent around 8 hours researching what the current solutions lack how we could develop something which provides better screening and at a cheaper rate. We came up with 6 ideas ranging from a simple selfie to screen breast cancer using image processing , to, making the use of tactile feedback to detect lumps and lesions in breast , and developing a piezoelectric mechanism based device for screening. We finally narrowed down to Brognosis due to its numerous advantages over the other ideas and because of the number of references we got justifying the technology used. The following are the links to the references: 
Breast cancer early detection methods for low and middle income countries, a review of the evidence:
http://www.ncbi.nlm.nih.gov/pubmed/22289154?dopt=Abstract

Breast tumor detection using piezoelectric fingers: first clinical report.

http://www.ncbi.nlm.nih.gov/pubmed/23623223?dopt=Abstract

We consulted a number of Doctors both from the United states( Developed nation) as well as India ( Developing nation). The product has been Co-created with the help from experts in the field of oncology. Majority of the time was spent on the research part as we wanted the product to be accurate, precise and user friendly. 

Brognosis is a three tier system, the Brognosis glove, the Brognosis mobile phone application and the Brognosis dashboard. For the Brognosis glove, we used ultrasonic sensor running on Arduino nano microcontroller.The glove also makes the use of magnetometer and accelerometer to check for the correct positioning of the glove during the screening time. All the images collected by the glove are sent over to the mobile phone application via bluetooth. This happens with the help of HC-05 bluetooth module. The mobile phone application has been developed for android platform and sends the data to the Microsoft Azure cloud for processing. All the processing takes place on the cloud and the images are fed into our customized algorithm undergo processing and are screened for lumps and lesions. The results obtained are then displayed on the mobile phone application instantly. The mobile phone application has some features like find device, panic button and exercise tutorials. The data collected during the screening time is visualized using the R programming language and automated reports are generated using Azure automation. 

Challenges I ran into

Finding sensors precise enough to detect very small lumps and lesions.
Applying and building everything within 72 hours.
Rapidly designing and prototyping all hardware and electronic parts.
Supplying enough current to electronics.
Simulating the mechanism involved in the lump detection in real time.

Accomplishments that I'm proud of

Coming up with a project which can really save lives and help deal with one of the biggest pressing challenges
Developing something which helps all the sections of the society.
Providing a great user experience in the device. 
Developing the desired product even when we were missing the most crucial hardware components. 
Taking the first step towards eliminating Breast Cancer.

What I learned

PennApps was full of learning. We learned about so many great APIs which the companies have to offer. Also learnt about the machine learning platform and Azure services provided by Microsoft. In total, I learnt about multiple disciplines involved in hacking ranging from Design to hardware development. Another important thing that I learnt was sticking to the deadline and developing the solution within the given span of time.  Product management I would say. We love the fact that as part of PennApps, we got a chance to work on a solution that impacts a million lives. 

What's next for Brognosis

We hope to continue working on this to be able to deploy this on a large-scale for hospitals as well as rural health clinics to save all the lives lost due to breast cancer . We will be applying for an FDA approval for this technology before we launch it. In the meantime we will be partnering with organisations dealing with breast cancer and start a pilot program with them after we fully develop the product. 
",,https://github.com/NeerajSaini7788,,"Best Domain Name Registered in PennApps XIV, Lutron IoT Prize, Best Use of Rapid Prototyping, sposnored by AddLab, Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App, Best Use of Data Visualization",health,asaini7788
Dr0pbox 2.0,http://pennapps-xiv.devpost.com/submissions/56817-dr0pbox-2-0,"Inspiration

Over the past five years, data has not just been a luxury, it has been a need. Each day, hundreds of millions of files are uploaded and shared over the internet, resulting in cloud storage becoming increasingly essential. As students, the need to store and access data at anytime from anywhere has become a necessity and we embarked on this journey to provide free and unlimited storage for everyone. We also needed a robust solution.

Who doesn't want unlimited storage? And who doesn't like free stuff? We combined the two, to bring you the best.

What it does

By accessing the Dropbox API, we synchronously chunk-upload files to multiple DropBox accounts using an automated account creation process. Our backend dynamically allocates free space for the user, all without the need for front end interaction. However, due to time constraints, and lack of access to Google's full OCR platform, we manually implemented this process.

Filled up an account? No problem, we'll move to the next one.

How we built it

DropBox offers a free tier option to users, where they can upload a limited amount of data. Keeping this in mind, we designed our backend in such a way that we could connect multiple DropBox accounts for a single user without the need for them to authenticate or create DropBox accounts. This essentially allows for a single user to have virtually unlimited storage as if one accounts storage capacity is exceeded, a new account is automatically initialized for them on our platform. Users, therefore, access their files through our platform, but we essentially run headless, connecting the two together.

Technical Talk:

Using Node in the backend with MongoDB, we essentially created a virtual cloud, which amalgamates multiple DropBox accounts for each user. This is then relayed to the front end where it can be interacted with in a user-friendly manner through Materialize.css.

Challenges I ran into

One of the biggest challenges for this project was bypassing the need for a Captcha as well as automating the creation of email and DropBox accounts. Since there is no API to accomplish this, we took an elementary approach and conceptualized a macro which would click through the sites for us. Using Google's OCR which we tested through Google Translate's 'Image to Text Feature', we bypassed the DropBox Captcha in near perfect number of attempts.

Once again, due to time constraints, this was not fully implemented and for the sake of completeness, we manually assisted in the process.

Accomplishments that we're proud of

A functioning model by the end of the first night, and working on making it presentable by the next.
Creating a simple hack to overcome a seemingly complex task.

What we learned

I'm sorry DropBox, we've always loved you, and will continue to love you. This was purely a learning experience.

What's next for Dr0pbox


Implementing automated account creation
Breaking down files through bit by bit encoding to increase storage efficiency
Complete CRUD operations on our platform
Switching between grid and list-view and even more user-friendly interface

",https://youtu.be/EhjNBRnPWks,https://github.com/rahul-fakir/pennapps-xiv-dropbox,,"Best Use of MongoDB, Best User Experience, Most Entrepreneurial Hack - Blackstone, Best Progressive Web App",cybersecurity,VirenMohindra,ShantanuPuri,rahulf
wavelength,http://pennapps-xiv.devpost.com/submissions/56818-wavelength,"Inspiration

As students, we've found that sending documents and images can be frustratingly difficult. The multi-step process and multi-platform ways to get files often lead to messy results. Additionally, there are many situations where files need to be transferred but there is no bluetooth or internet connectivity (especially in developing countries). We found that sound could be an underused and better way to transfer information for these initial use cases as well as in a variety of other situations that were revealed to us as we built.

What it does

wavelength uses sound to transmit any kind of file. It does so by assigning frequencies to a system of characters, and using sound frequencies to represent those characters and then turning the sound to characters on a different device. This allows for multiple people to efficiently receive data from the same source/sound, a ""one-to-many"" system that is far more efficient than adding a large group of people to an email chain or google doc. Internet or bluetooth connectivity are not required for wavelength to work. We have also used to Nexmo API to allow users to send texts specifying a file, which our software turns to sound and transmits on a call to the desired target. This allows users who aren't close to each other to send files, with or without an internet connection.

How we built it

Upon starting to build Wavelength, we first had to determine how to encode and decode our files consistently. We chose base-64 encoding as an effective answer to this, since a) there was enough bandwidth in the frequencies we intended to use to match 64 characters and b) files could be easily downloaded as a base-64-encoded string (along with MIME type and file name). Wavelength was completely built as a client-side web application, in vanilla JavaScript. We tapped into a good part of the WebAudio API to create our oscillation frequencies (which varied in increments of 50 from 1900-5200 Hz) as well as to process the microphone input to capture what frequency was being played on the receiving end. We processed our microphone input with a ""highshelf"" BiquadFilter to boost the gain of high-frequency sounds and a Fast Fourier Transform (using AnalyserNode) to deconstruct the frequencies into an array that we used to find the frequency played. On the server side (where we use Node.js to interface with Nexmo Voice API, allowing you to send files through a phone call), the WebAudio spec is not natively supported. We used a couple modules to produce the same oscillation frequencies (although we found differences between implementations of OscillationNode in Node and Chrome), as well as stream buffers to capture the raw PCM audio data and export it to a WAV file. 

Challenges we ran into

A major challenge was accuracy, as we were carrying large, complex sets of data over the low-bandwidth medium of sound. Many times we changed the encoding frequencies we used because we found that many of them were too indistinguishable for an average-quality microphone to detect difference. Another issue we encountered with accuracy was a timing issue, since we had initially planned to begin the transmission with a handshake that synced the sender's and receiver's clients. Although not optimal (since it results in a lengthier transmission), we decided to include a 1950 Hz end-of-character frequency that indicated more explicitly the boundaries between characters and their frequencies. 

Accomplishments that we're proud of

No one on our team has had experience in digital signal processing or using WebAudio, but we were able to successfully create a system for encoding and transmitting files through sound, implement the system in a language that only one of four teammates had experience with (JavaScript), and send actual files using the ridiculous beeps coming from our headphones.

Our team worked very well together, despite only two members working together before the event. We were able to delegate work well and discuss without destructive disagreement.

What's next for wavelength

We're super excited to apply what we've made to industries that would greatly benefit. We see huge potential in being able to broadcast text or images from one to many, especially in scenarios where a teacher or leader wants to easily send information to many people. The ability to transfer files without the internet also makes wavelength very useful to people in developing countries who own computers but have spotty internet connection. In the security industry, wavelength could be used for 2 factor authentication.
",https://www.youtube.com/watch?v=ZgyWEFu8_HQ,,,"Most Entrepreneurial Hack - Blackstone, Best Use of Nexmo API, Best Progressive Web App","",nthnlee97,Jacobanks,ethanlee,justinwei
Mind Palace,http://pennapps-xiv.devpost.com/submissions/56826-mind-palace,"We all experience stress in our lives, especially here at college. Inspired by an increasing awareness of mental health and a desire to bring students innovative coping and anti-anxiety methods, we decided to make MindPalace, a Virtual Reality world that brings together popular ways of de-stressing - immersion into happy thoughts, guided meditation, and gentle, desk-accessible yoga. 

When a user enters the mind palace, they have the option to engage themselves in the three different modules. The first is a positive memory room, where the user can select photos (or videos!) from a file system and deploy them into floating bubbles, which hover around the room and collectively surround the user (reaching out and grabbing any of these orbed memories allows the user to view it in larger detail).

The second module is a meditation module, where the user is brought to a first-person calm environment and can listen to guided meditations on positivity and deep breathing. These sessions are well known for promoting focus in the moment, and provide the user with an immersive getaway into nature.

The last module is a light exercise/yoga module, where users are guided (by their left and right VIVE controllers) into different positions, stretching out their upper body. Stretching, especially for those who have sedentary jobs, is important in keeping an active and fresh mind, and can help clear worries or anxieties.

These features involve all working with VIVE and VIVE controllers to interact with our environment. Challenges we faced included getting the VIVE controllers to work with features we had in unity (such as colliding with buttons in 3D space) and figuring out intuitive and clean UI for a virtual reality app. 
",,https://github.com/davlia-projects/PennApps-Mind-Palace,,"Best Use of Data Visualization, Best Use of VR/AR for Content Discovery",vr/ar,GraceHuang,Danedaworld,Karinnal
